{"id": 5962, "title": "Ticket #5962: Scale Testing the Grid Universe (with Clouds)", "description": "<blockquote>\nThe grid universe consists of three pieces: the schedd, the grid manager, and the GAHP.  When doing scale testing, we generally ignore GRIDMANAGER_SELECTION_EXPRESSION, and just test one user submitting jobs to one schedd, which starts one grid manager, which starts one GAHP.  If a specific desired scale is required, it still desirable to determine the scaling limits (and performance curve, if possible) of a single GAHP, so we can predict how many a given scale will require.\n\n<p>We generally do scale testing with sleep jobs.  (It will be convenient for the duration of the sleep to be in the job arguments.)  This eliminates a lot of awkwardness when getting starting; it also allows us to easily control job duration, which can help isolate different parts of the system for testing.  It also minimizes (or, with a little cleverness, entirely eliminates) file transfer.  Although file transfer may be a bottleneck in production, especially when testing across a WAN, for scale testing we assume that file transfer is a solved problem.\n\n</p><p>At sufficiently large scale (more than 100,000 jobs?), the schedd itself may becomes a bottleneck.  This largely depends on how frequently new jobs enter the queue, and how frequently old jobs leave the queue.  The longer the sleep job, the lower both of these rates need to be obtain a particular scale.\n\n</p><p>If you're testing using the grid universe for provisioning....\n\n</p><p>BEFORE YOU START\n\n</p><p>You're really going to want some way to know if the weird state(s) reported by the job queue are real or a problem brought on by HTCondor getting overloaded.  You're going to want a second source of information about your jobs on the target system, if at all possible.  When you check to make sure things are working, you should also check with the second source of information.  Note that the information in HTCondor's job queue could be several minutes old.\n\n</p><p>The steady state of a job should usually be R(unnning).  An I(dle) job may be in the process of starting up, but shouldn't stay that way for very long.  (Many jobs staying idle for a long time may be a sign that the schedd is overloaded.)  A job on H(old) indicates a problem.  The occasional job may go on hold because of transient problems, but a lot of them going on hold may indicate a more serious (scaling) problem.\n\n</p><p>INITIAL TESTING\n\n</p><p>The first thing to test is a single, short, and basic sleep job, just to confirm that the whole cycle works with your testing set-up.\n\n</p><p>SCALE-OUT TESTING\n\n</p><p>For scale-out testing, set the duration of the sleep job to longer than you care to test for.  We'll test the ability of the system to deal with job termination later.  In this phase of testing, we're seeing how fast the system can start jobs, as well as how many simultaneous jobs it can handle.  HTCondor's internal time-outs are fairly generous, but the schedd can't do anything else while a group of jobs is being submitted, so your queue statements should stay relatively small if you're doing provisioning with the grid universe jobs.  (The negotiator assigns resources to the schedd, but the schedd claims those resources from the startds running on them.)  Try submitting a 'queue 10' and making sure it works, then a 'queue 100', and then a 'queue 1000'; you may need to examine the GAHP or grid manager logs to determine if the GAHP is tripping on any rate-limiting (and if so, if it's responding properly).  You can use DAGMan to manage job submission speed, instead, if you prefer, but 1000 sounds like a nice number of jobs for which to time the job start delay.\n\n</p><p>Also, you can push on the rate-limiting code by submitting multiple large batches of jobs; just be sure to give the schedd \"enough\" time to work between batches.\n\n</p><p>Generally, you don't need to let the system sit for more than about twenty minutes at whatever the desired size is to get a good idea of how it will hold up in the steady state over a longer period of time; I believe all of our timers / leases are shorter than that.\n\n</p><p>UNSTEADY STATES\n\n</p><p>Try removing jobs in small and then larger batches (keep an eye on how long each removal takes, for the same reason you did while submitting).  Some grid universe jobs may take a considerable amount of time to remove (for instances: some services require that we poll the state of a job that's existing until it actually does, because they can go into a partially-exited error state; we therefore may have to wait a long time, considering the polling interval, for that job to exit).\n\n</p><p>From a steady state, kill -9 the gahp; the gridmanager should shut itself it down and then be restarted by the schedd.  Verify that the before and after states are the same, and remain so after the grid manager has exited recovery (check its log).  This could take a while.\n\n</p><p>Repeat this exercise -- although it should have exactly the same result -- for the grid manager itself.\n\n</p><p>If both of those exercises result in an appropriate steady-state, kill -9 the schedd; the master will restart it, and all state should be recovered.\n\n</p><p>DYNAMIC STATES\n\n</p><p>Remove a large batch of jobs and replace it (by submitting again) with a batch of short-duration jobs.  Make sure the system behaves while you're doing this.\n\n</p><p>Once you've replaced all the long-duration jobs with short-duration jobs, the number of running jobs should start decrease naturally on its own.  As it does, feed new short-duration jobs in to replace the lost ones.  This better-represents the steady state of a busy pool.\n\n</p><p>SCALE-IN TESTING\n\n</p><p>Scale-out testing, but with condor_rm instead of condor_submit.\n\n</p><p>PROVISIONING\n\n</p><p>If you're using the grid universe to do provisioning, the concerns about schedd busy-ness become greater, since each job is adding a core (or more) for the schedd to claim.  Also, you're probably concerned with making those jobs' VMs run a startd and report to a central manager.  To address the first issue, you could -- and many sites do for other reasons -- run the provisioning jobs in one schedd and the sleep/test jobs out of another (most easily done on a different machine).  The schedd (on a beefy enough machine with good networking) handle 100,000 jobs without too much trouble, so unless you're doing fairly large-scale testing, one will be enough.\n\n</p><p>When you check the state of the HTCondor system, then, you'll also need to run condor_status.\n\n</p><p>See <a class=\"external\" href=\"https://htcondor-wiki.cs.wisc.edu/index.cgi/wiki?p=ChtcScaleTestSetup\">https://htcondor-wiki.cs.wisc.edu/index.cgi/wiki?p=ChtcScaleTestSetup</a> for wisdom on setting pools for large-scale testing.  (The example happens to have used a lot of EC2 nodes to generate the scale, but the HTCondor configuration of the on-premises is independent of that.)</p></blockquote>", "remarks": "<blockquote>\n</blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "todo", "last_change": "2016-Oct-12 16:44", "status": "new", "created": "2016-Oct-12 16:34", "fixed_version": "2016-Oct-12 16:34", "broken_version": "", "priority": "4", "subsystem": "Grid", "assigned_to": "", "derived_from": "", "creator": "tlmiller", "rust": "", "customer_group": "other", "visibility": "public", "notify": "tlmiller@cs.wisc.edu, jfrey@cs.wisc.edu", "due_date": ""}
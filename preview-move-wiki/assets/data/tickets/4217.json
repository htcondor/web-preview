{"id": 4217, "title": "Ticket #4217: GT2 GAHP fails badly when it runs out of file descriptors", "description": "<blockquote>\nBrian Bockelman and Bockjoo Kim report numerous failures of Condor-G GRAM jobs in GlideinWMS due to the gahp server running out of file descriptors. The errors can occur in many different places in the gahp server (can't open file for transfer, can't accept connection, can't open CA file for authentication). In many cases, no useful error message is propagated anywhere. Eventually, the gahp crashes, taking down the gridmanager and its other gahps with it. This causes a lot of extra recovery work and possibly more failures.\n\n<p>Keeping GRIDMANAGER_MAX_PENDING_REQUESTS small can help prevent running out of FDs, but many FDs are used to handle incoming connections to the gahp (for file transfer and job status notifications), which the gridmanager and gahp don't have direct control over.\n\n</p><p>An email from Brian describing the problem:\n</p><div class=\"code\">\n<pre class=\"code\">It appears that the gahp_server processes on gfactory-1 has a limit of\n1024 open file handles.  This is causing various random errors to happen\non gahp_servers which are servicing many jobs (in particular, UFL and\nPurdue, although there are others).\n\nWhen running out of FDs (or almost run out), it appears that the\ngahp_server has relatively unpredictable behavior.  For example, it can\nsometimes accept a new connections (for example, from globus-url-copy),\nbut not open the file to write the output.  This results in a HTTP 400\nerror, which you are seeing (but the error is not logged anywhere).\n\nAnother way this manifests is being unable to open the CA cert in\n/etc/grid-security, causing what appears to be \"network issues\" when\ncommunicating with the remote CE.  The appropriate error messages do\nnot appear to be logged.\n\nWatching a few, it does not appear to be a leaked FD; rather, it seems\nthe pending queries eventually are returned.  It's possible the factory's\nsetting of 500 pending queries per gahp does not help here.\n\nEventually, the gahp_server segfaults and dies (segfaults appear to be\nhappening at least once a minute) -- but causes various issues until\nthen. It's difficult to see exactly what's happening because the logs\nrotate once every 5 minutes or so.  The gahp server dying causes the\ngrid_manager to EXCEPT, causing all other gahp servers serviced by that\ngrid_manager to get nuked -- meaning that other sites are going to have\nGT5 stdout/err issues if one does.\n\nSo, recommendations for the factory:\n\n1) Decrease the number of pending queries from the grid manager and\nset the status query limit rate for the gahp servers.\n2) Increase the maximum file descriptors per gahp server.\n3) Keep logs for more than a few minutes.\n4) Keep core files and report them to the HTCondor team.\n\nThere are some obvious improvements for the HTCondor team too (log\nfailures, take appropriate action when FDs run low in the gahp_server);\nI've CC'd them.\n\nIt's quite possible (but not guaranteed) that all the stdout/err issues -\nand many of the status issues - are caused by this.\n</pre></div>\n</blockquote>", "remarks": "<blockquote>\n<em>2014-Feb-18 21:09:52 by bbockelm:</em> <br/>\n\nRandom Thoughts:\n\n<p></p><ul>\n<li>Does GT5 have job status notifications?  I thought condor-g polls (or, at least, that's the impression I got from strace).  If this is true, we have a mechanism to throttle.\n</li><li>my_ez.cpp limits the number of accepted connections to 20.  After that, connections are backlogged because accept() isn't called.  This has always bugged me because we basically have no clue how many stageouts are being silently failed.  It would be much better to connect, then immediately deny, attempts over the limit (and log appropriately).  The limit probably ought to be configurable too.\n</li><li>If open() fails for writing or reading in my_ez, it's not logged.  Knowing that we were looking for an EMFILE would have saved considerable time.\n</li><li>The GAHP needs a logging mechanism to provide feedback to the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=GridManager\" title=\"Grid Manager\">GridManager</a></span>.  I suspect that Globus has a very nice error message when CA certs fail to open that is getting swallowed.\n</li><li>File descriptors increase slowly - we could periodically record this and tell the grid manager to back off.  Even giving a \"fake result\" back to the grid manager may be helpful.\n</li><li>We probably need to make the FD limit configurable throughout the grid_manager / gahp stack.\n</li><li>As a correction, in the case of the factory, it was an unrelated gahp (cream_gahp) which was pulling down the grid_manager.  Didn't exactly do wonders for GT5 (uploads of file transfers from the CE failed while things were down).\n</li></ul>\n\n<p></p><hr/>\n<em>2014-Feb-19 10:59:57 by jfrey:</em> <br/>\n\nRandom Responses:\n\n<p></p><ul>\n<li>GT5 has job notifications from the jobmanager to the client, which is why they don't feel the need to have a bulk query option. There's no way for the client to tell if it has missed a notification, so Condor-G does polling of all of the individual jobs as a backup.\n</li><li>It's been a while since I've looked at my_ez.cpp, but my understanding is that it allows 20 connections to be authenticated in parallel, but there can be more than that during data transfer.\n</li></ul>\n\n<p></p><hr/>\n<em>2014-Feb-19 11:18:48 by bbockelm:</em> <br/>\n\nHi Jaime:\n\n<p></p><ul>\n<li>Ah, so this probably was exacerbated by the fact that gridmanager was restarting once a minute?  Constant restarts forcing condor-g to do individual job queries?  Actually, yesterday, I did see the Purdue CE hit the new 4096 FD limit after about 2 hours of running ... so it may not be exactly as we understand it.\n</li><li>I passed through the code when doing the original bug report... did seem to limit the number of accepts.\n</li></ul>\n\n<p>Hope this helps!\n\n</p><p></p><hr/>\n<em>2015-Feb-11 16:02:57 by bbockelm:</em> <br/>\n\nAbandoning this ticket from the OSG POV; OSG encourages sites hitting scale issues to convert to HTCondor-CE.</blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "incident", "last_change": "2015-Feb-11 16:03", "status": "abandoned", "created": "2014-Feb-18 16:24", "fixed_version": "2014-Feb-18 16:24", "broken_version": "", "priority": "2", "subsystem": "Grid", "assigned_to": "", "derived_from": "", "creator": "jfrey", "rust": "", "customer_group": "osg", "visibility": "public", "notify": "jfrey@cs.wisc.edu,tannenba@cs.wisc.edu,bbockelm@cse.unl.edu,sfiligoi@fnal.gov", "due_date": ""}
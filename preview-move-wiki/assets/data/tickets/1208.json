{"id": 1208, "title": "Ticket #1208: Dataflow with Files using Dagman", "description": "<blockquote>\nThis ticket sketches a description of a possible enhancement to the\nDAGMan functionality.\n\n<p>The enhancement is that if you have two nodes in this relationship A-&gt;B,\nwhen node A produces, and completes, a file Af that B solely requires\nto run, then B may immediately start, even though A is still running\npossibly producing other files for other nodes.  This mimics much\nmore of a dataflow dependency instead of a job completion dependency.\nAdditionally, when Af is noticed and moved to where ever it needs to go\nso B can run, an event is written into the event log associated with the\nnode such that we don't try to move the file again in other circumstances.\n\n</p><p><span class=\"section\"></span></p><h2>Milestones </h2>\nThe branch holding this work is <strong>V7_5-dataflow-2-branch</strong>.\n\n<p></p><ul>\n<li><strong>[condor_starter change]</strong>: Periodically bring closed output files back to spool <strong>[DONE]</strong>\n<ol>\n<li>Implement a means to get the open files of a running process. <strong>[DONE]</strong>\n</li><li>Implement: user turns on dataflow semantics with jobad attr, timer in starter to perform checks. <code>transfer-dataflow-output-files</code> <strong>[DONE enough]</strong>\n</li><li>Modify the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=FileTransfer\" title=\"File Transfer\">FileTransfer</a></span> object to:\n<ol>\n<li>Get all pids for job's process tree and get list of open files. <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=1515\" onclick=\"get_ticket_and_populate_wrapper('1515'); return false;\" title=\"Add open files to ProcAPI::getProcInfo &amp; condor_procd\">#1515</a></span> <strong>[DONE]</strong>\n</li><li>Using the filetransfer api, get the list: <div class=\"code\">\n<pre class=\"code\">(filter (closed-file-in-pids-p job-pids)\n (filter output-file-p\n  (enumerate-sandbox-files job-pids)))\n</pre></div>\n\n</li><li>Transfer that list back as if ON_EVICT_OR_EXIT. <strong>[DONE]</strong>\n</li></ol>\n</li></ol>\n\n<p></p></li><li><strong>[file-transfer object change]</strong>: Make it so closed dataflow files\nare only brought back ONCE. Currently, even though the dataflow file is\nclosed, it is brought back multiple times. If the file is repoened and\nclosed again, maybe it can be brought back again (which would actually\nviolate our assumptions so it needs to be seriously considered), but\nalso only ONCE until the next time. This should be implemented by augmenting\nthe file atalog to allow dynamic insertion of files into it after it has been\ncreated. I also need to check at final upload time that if there are dataflow\nfiles in the file catalog to NOT transfer them.\n</li></ul>\n\n<p><span class=\"section\"></span></p><h2>Assumptions </h2>\n<ul>\n<li>The files that are to be created by an executing node and used in the\ndataflow <strong>must</strong> have these semantics. This is more restrictive than how the\nfiletransfer object decides a file is considered an output file or an intermediate file.\n<ol>\n<li>The created file must not exist in the sandbox manifest.\n</li><li>The file is <strong>atomically</strong> created and opened for use, <strong>once</strong>.\n</li><li>Once the file is closed, it <strong>stays closed</strong>.\n</li></ol>\n</li><li>There is a way on Linux/Unix to determine the open files a process is using.\n</li><li>Condor knows a manifest of the sandbox, to determine when these files are\npresent.\n</li><li>There is a means by which the files can be moved asynchronously out of the\njob execution directory while the job is still running.\n</li></ul>\n\n<p><span class=\"section\"></span></p><h2>Feasibility </h2>\n<ul>\n<li>Determination of open files by a process on Linux.\n<ul>\n<li>If one looks in the directory /proc/&lt;pid&gt;/fd you will find a list of fds\nopened by the process and the files to which they point. Using the command\n<code>stat</code> we can see that the read/write status of the fd is available for that\nfile. This information is thusly available.\n</li><li>If one knows the manifest of the sandbox, then any file added to the\nsandbox that is not referenced by an open fd would be considered a candidate\nfor execution interleaving of a future node.\n</li><li>Feasible to find files in the correct state on Linux: <strong>YES</strong>\n</li></ul>\n</li><li>Condor today knows the sandbox manifest.\n</li><li>Condor would need functionality to implement asynchronous file transfer\nwhile job is still running.\n</li></ul>\n\n<p><span class=\"section\"></span></p><h2>Problems </h2>\n<ul>\n<li>Idempotency of output files under retry/eviction/crash conditions.\nIn this scenario:\n<ol>\n<li>job A starts up\n</li><li>file Af is produced.\n</li><li>file Af is asynchronously transfered back and committed.\n</li><li>execute node crashes/evicts/etc/fails.\n</li><li>Job A restarts (also could with <strong>RETRY</strong>) on potentially different node.\n</li></ol>\n\n<p>What will happen is file Af will appear as an input file in the sandbox\nwhen the job restarts. If the job is unware of the previous state it had\nwritten, it will regenerate Af, as which point the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=FileTransfer\" title=\"File Transfer\">FileTransfer</a></span> object\nwill see it as an output file, and transfer it back again--overwriting\nthe first transfer, causing a duplicate dataflow message, and making\nthe entire dataflow possibly inconsistent unless care is taken. If the\njob is aware of the previous state and does not regenerate Af, then Af\nwill never be transferred back and all is well.\n\n</p><p></p></li><li>Dataflow connections between nodes do not have infinite capacity to store\ntokens. Example: if in the dataflow <code>A-&gt;B</code>, B consumes data tokens at a slower rate than A produces them, we would run out of disk space. So, when we're getting close to our allotment of space denoted for that specific stream, how do we signal A to stop? Suspend the process? other?\n\n<p></p></li><li>If a job creates large temporary files, closes them, and reads them as for a table lookup or something similar, then they could be incorrectly flagged as a dataflow file (if the program opens and closes it now and then, but only reads it) and moved back, potentially causing space issues on the submit node or network bandwidth issues. <strong>NOTE:</strong> This can be fixed by allowing the user to specify\nvia regex a list of files that should be considered to have dataflow\nsemantics.\n</li></ul>\n\n<p><span class=\"section\"></span></p><h2>Real world Jobs</h2>\n<ul>\n<li>Some real world jobs don't fall under workflow model.\n<ul>\n<li>Jobs which do things like:\n<ol>\n<li>compute result\n</li><li>open file with append\n</li><li>write result\n</li><li>close file\n</li><li>repeat\n</li></ol>\n</li><li>Jobs which overwrite their previous output files on restart.\n</li></ul>\n\n<p></p></li><li>Real world examples of jobs that could benefit\n<ul>\n<li>Craig Struble had a workflow where an image generation phase of\nN processes generated N images (where N is 200,000 or so) and then\nan accumulation phase summed the image into a single image. His main\nproblem was that if you rendered workflow this as a DAG, you'd need\nstorage space for 200,000 images--which were individually large and\nhe didn't have that much space. Since DAGMan can't run a job until all\nof their parents complete, we are <strong>forced</strong> to store all 200,000 images\nbefore the accumulator process is run.\n\n<p>The solution we came up with was there is a DAG producing the\nN images and the space needed for returning images is controlled by\nDAG throttling. Then, the post scripts on the image nodes copy the\nresultant image into a directory using unix rename()--which is atomic,\nwhere a seperate scheduler universe process is polling every X seconds\nand accumulating the partial results into the final image. When the DAG\nfinishes, the final node writes a 'done' messge into the directory which\ncompletes the dataflow.\n\n</p><p>This real world job is the exact type of dataflow job that the\nfeature of this ticket could solve.\n</p></li></ul>\n</li></ul>\n\n<p><span class=\"section\"></span></p><h2>References </h2>\n<ul>\n<li><a class=\"external\" href=\"http://www.jpaulmorrison.com/fbp/index.shtml\">Flow Based Programming</a>\n<ul>\n<li><a class=\"external\" href=\"http://www.jpaulmorrison.com/fbp/book.pdf\">Flow Based Programming - Book</a>\n</li></ul>\n</li><li><a class=\"external\" href=\"http://portal.acm.org/ft_gateway.cfm?id=1013209&amp;type=pdf&amp;coll=GUIDE&amp;dl=GUIDE&amp;CFID=79152560&amp;CFTOKEN=19491160\">Advances in Dataflow Programming Languages</a> - This is particularly good.\n</li><li><a class=\"external\" href=\"http://rainbow.polytech.unice.fr/publis/montagnat-isnard-etal:2009.pdf\">A data-driven workflow language for grids based on array programming principles</a> - Interesting.</li></ul>\n</blockquote>", "remarks": "<blockquote>\n<em>2010-Feb-17 11:16:30 by psilord:</em> <br/>\n\nThis ticket has no dated milestones yet since I am in discussion with Miron about the scope of the initial milestone. However, since I'm doing some research in the code and general thinking about it, it is marked as active.\n\n<p></p><hr/>\n<em>2010-Mar-17 14:03:34 by psilord:</em> <br/>\n\nImplementing.\n\n<p></p><hr/>\n<em>2010-Mar-23 12:00:59 by psilord:</em> <br/>\n\nI've written the function which will retrieve the set of files currently held open by the process. This works on Linux only.\n\n<p></p><hr/>\n<em>2010-Apr-07 15:28:35 by psilord:</em> <br/>\n\nmissed deadline due to 7.5.2 wrangling. Moving forward another two weeks due to condor week.\n\n<p></p><hr/>\n<em>2010-May-06 16:47:13 by psilord:</em> <br/>\n\nAbout to start on this project. Have to finish <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=1195\" onclick=\"get_ticket_and_populate_wrapper('1195'); return false;\" title=\"Break recursive dependency between nightly.pl and condor_nmi_submit\">#1195</a></span> first. Then I'll set a milestone date.\n\n<p></p><hr/>\n<em>2010-Sep-28 14:47:22 by psilord:</em> <br/>\n\nCreated a new branch <strong>V7_5-dataflow-2-branch</strong> from the master and merged <strong>V7_5-dataflow-branch</strong> to it. I'm abandoning the <strong>V7_5-dataflow-branch</strong>.\n\n<p>I did this because danb made the filetransfer code directory aware on the master\nand I thought it'd be a good idea to have that knowledge present when changing the filetransfer object to periodically move the dataflow files back since now they can reside in directories.\n\n</p><p></p><hr/>\n<em>2010-Sep-30 16:31:05 by psilord:</em> <br/>\n\nOk the merge was done. Hacking code again.\n\n<p>After more testing, in the dataflow timer handler, once I get the set of opened\nfiles I know I shouldn't transfer, pass that information to <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=UploadFiles\" title=\"Upload Files\">UploadFiles</a></span> which\nwon't pass those files on. If I leave the other <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=UploadFiles\" title=\"Upload Files\">UploadFiles</a></span> code alone, then\nit should correctly see to transfer the files again if they've been updated.\n\n</p><p></p><hr/>\n<em>2010-Oct-20 10:41:25 by psilord:</em> <br/>\n\nI had been making good progress. I've changed <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=UploadFiles\" title=\"Upload Files\">UploadFiles</a></span> to accept two more arguments: a \"doing dataflow\" flag, and an STL set which contains the set of open files from the job family under the starter. It is intended that when doing dataflow, only the dataflow files which have been closed and not already sent, will be sent.\n\n<p>I'm in the middle of having ComputeFilesToSend() pick out the right files in the dataflow to send back to the spool.\n\n</p><p>However, I just got preempted by ticket <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=1722\" onclick=\"get_ticket_and_populate_wrapper('1722'); return false;\" title=\"Fix condor_nmi_submit to handle unknown platforms\">#1722</a></span> and <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=1723\" onclick=\"get_ticket_and_populate_wrapper('1723'); return false;\" title=\"make nightly NMI submit script submit Condor to build on EVERYTHING\">#1723</a></span>.\n</p><hr/>\n<em>2010-Oct-20 16:03:30 by jfrey:</em> <br/>\n\nBulk change of target version from v070504 to v070505 using ./ticket-target-mover.\n<hr/>\n<em>2011-Jan-27 14:46:04 by danb:</em> <br/>\n\nBulk change of target version from v070505 to v070506 using ./ticket-target-mover.\n\n<p></p><hr/>\n<em>2011-Jan-28 16:43:29 by psilord:</em> <br/>\n\nOk, I finally got it working so that only closed files which are specifically dataflow files are brought back to the spool directory.\n\n<p>Now I have to make it so that this happens only ONCE for a closed dataflow file.\n\n</p><p>I think I will add the feature of: If a closed dataflow file which has already been transferred is open/closed again, it will be transferred again once.\n\n</p><p>This allows people who write checkpoint files to write to the same file and use dataflow to store the checkpoint file into the spool at periodic times.\n\n</p><p>However, it does go against the assumptions initially laid out. But...there is a user who'd use (some guy in ChemE maybe? gthain knows) it if it had that feature. :)\n\n</p><p></p><hr/>\n<em>2011-Jan-28 16:44:21 by psilord:</em> <br/>\n\nI still need to clean up what I have a little bit and commit it to the branch... I'll do that monday.\n<hr/>\n<em>2011-Feb-01 16:01:00 by tannenba:</em> <br/>\n\nBulk change of target version from v070506 to NULL using ./ticket-target-mover.\n\n<p></p><hr/>\n<em>2011-Feb-07 16:08:12 by psilord:</em> <br/>\n\nFixed behavior so that the dataflow files are transferred ONCE after being closed. I did this by inserting the transferred dataflow files into the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=FileCatalog\" title=\"File Catalog\">FileCatalog</a></span>.\n\n<p></p><hr/>\n<em>2011-Mar-11 11:48:38 by psilord:</em> <br/>\n\nI made a new branch from master called <strong>V7_7-dataflow-branch</strong> to which I am merging <strong>V7_5-dataflow-2-branch</strong>. I'm in the middle of merging and testing what\nI merged. When I'm done, the dataflow stuff will exist on the <strong>V7_7-dataflow-branch</strong>.\n\n<p></p><hr/>\n<em>2011-Mar-30 11:35:34 by psilord:</em> <br/>\n\nThe next thing I need to do with this is merge it into the master.\n\n<p></p><hr/>\n<em>2011-Apr-01 15:39:23 by psilord:</em> <br/>\n\nStalled due to preemption.\n\n<p></p><hr/>\n<em>2011-Oct-28 10:33:01 by psilord:</em> <br/>\n\nI'm getting rid of languishing stalled tickets and putting this back to new.</blockquote>", "derived_tickets": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=1515\" onclick=\"get_ticket_and_populate_wrapper('1515'); return false;\" title=\"Add open files to ProcAPI::getProcInfo &amp; condor_procd\">#1515</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nAdd open files to ProcAPI::getProcInfo &amp; condor_procd</td></tr>\n</tbody></table>", "attachments": "<html><head></head><body></body></html>", "check_ins": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">2011-Mar-17 12:05</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/e2543f888226159dbb1f781c8e3cfe90c8e52141\">[21125]</a></span>: ===GT=== <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=1208\" onclick=\"get_ticket_and_populate_wrapper('1208'); return false;\" title=\"Dataflow with Files using Dagman\">#1208</a></span> Added a new file for compilation.  (By Peter Keller )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2011-Mar-10 15:58</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/83fdf34d3d289e1a703068eb0e2784442e08b7c3\">[21124]</a></span>: Merged <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/ebc0cf96d54d4760d1629fe0b41437f449ab6c77\">[19081]</a></span>, <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/2cdd997a9eab479e054ab098d2ba606150bc2fe7\">[20883]</a></span>, <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/eb8ae6683503de04c8e2ed387f9ae85c0af807d8\">[20884]</a></span>, Merged V7_5-dataflow-2-branch into V7_7-dataflow-branch <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=1208\" onclick=\"get_ticket_and_populate_wrapper('1208'); return false;\" title=\"Dataflow with Files using Dagman\">#1208</a></span>  (By Peter Keller )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2011-Mar-03 15:16</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/86c5d1c17068b6d5ba1963978c9dfe0410895f2d\">[20906]</a></span>: ===GT=== <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=1208\" onclick=\"get_ticket_and_populate_wrapper('1208'); return false;\" title=\"Dataflow with Files using Dagman\">#1208</a></span> This is the first checking of the new V7_7-dataflow-branch which will be where the dataflow work continues from the V7_5-dataflow-2-branch (which will be merged here shortly). This is because the V7_7-dataflow-branch is made from a recent master so it contains the cmake feature, file transfer\u00a0[...]\n (By Peter Keller )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2011-Mar-02 15:34</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/eb8ae6683503de04c8e2ed387f9ae85c0af807d8\">[20884]</a></span>: ===GT=== <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=1208\" onclick=\"get_ticket_and_populate_wrapper('1208'); return false;\" title=\"Dataflow with Files using Dagman\">#1208</a></span> The dataflow files are correctly identified (meaning that they are actually dataflow files and closed) and periodically brought back to the submit side. Hacked the file transfer object to understand that sometiems when it is transferring files it is doing so on behalf of dataflow.  (By Peter Keller )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2010-Oct-18 13:45</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/2cdd997a9eab479e054ab098d2ba606150bc2fe7\">[20883]</a></span>: A temporary checkin related to solidifying the timer path to call the dataflow timer handler. <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=1208\" onclick=\"get_ticket_and_populate_wrapper('1208'); return false;\" title=\"Dataflow with Files using Dagman\">#1208</a></span>  (By Peter Keller )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2010-Aug-17 14:19</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/913c1725abe2f6e91b50d99e613a9411fa380f1a\">[18816]</a></span>: Detect that dataflow is needed from the job ad, init timers from jobad. We also check the data path from procapi to the dataflow timer wrt knowing the openfile set of the job in question for which we are about to perform the dataflow transfer. <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=1208\" onclick=\"get_ticket_and_populate_wrapper('1208'); return false;\" title=\"Dataflow with Files using Dagman\">#1208</a></span>  (By Peter Keller )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2010-Jul-26 16:32</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/d857140ec53fd02c79389408e2e0bec737bdac1e\">[18632]</a></span>: The ProcAPI::getProcInfo call fills in the open file set of the process on linux into the procInfo structure. That information is now propogated through the procd and into the kill family codebase too. The JIC Shadow in the vanilla starter was modified so it knows the job pid, and can ask the procd for\u00a0[...]\n (By Peter Keller )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2010-Jul-20 16:40</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/bb7e02447ee7164233ef143e605c822921396669\">[18602]</a></span>: + Finish converting the procInfo and procInfoRaw structures into Objects with a method or two on them to make it easy to deal with them. + Clean up some stupid uses of inipi() and got rid of initProcInfo(), which is replaced by calls of Clear() on the new objects. This gets rid of a nasty use of memset()\u00a0[...]\n (By Peter Keller )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2010-Jul-19 15:30</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/9e9e04e2a7c301ac9e432753328d85dfc777b2f8\">[18590]</a></span>: Rewrote open_files_in_pid() to not use Condor's Directory object or c++ util becuase ultimately this code is in the ProcAPI library and that has to be linked against the condor_procd, which uses a very minimal footprint of Condor code because it runs as root and needs to be auditable. Also made it so\u00a0[...]\n (By Peter Keller )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2010-Jul-19 13:00</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/de7776305557900f63a0853d0cc04361704c87fa\">[18589]</a></span>: Moved function for discovery of open files from condor_c++_util to condor_procapi because because it needs to work with the procd and the procd has a limited set of things to which it links. Changed some structs to classes in preparation of adding more functionality to them. <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=1208\" onclick=\"get_ticket_and_populate_wrapper('1208'); return false;\" title=\"Dataflow with Files using Dagman\">#1208</a></span>  (By Peter Keller )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2010-Jul-19 10:30</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/cce624ba726ee6cf953a82b8429cf3878435df5d\">[18588]</a></span>: Add timer stubs and a default initial value for the timers which will ultimately call into the filetransfer object to transfer dataflow files around. <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=1208\" onclick=\"get_ticket_and_populate_wrapper('1208'); return false;\" title=\"Dataflow with Files using Dagman\">#1208</a></span>  (By Peter Keller )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2010-May-12 15:44</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/7fd17c756b1935e8a7e46c9e71c7d1117df3666a\">[17980]</a></span>: Changing the version string for V7_5-dataflow-branch branch to denote what it is in case I use it at a site. <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=1208\" onclick=\"get_ticket_and_populate_wrapper('1208'); return false;\" title=\"Dataflow with Files using Dagman\">#1208</a></span>  (By Peter Keller )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2010-Mar-23 16:01</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/252ec456f1dd184e44b66fbd060851fcbe0b90a7\">[17684]</a></span>: Added a function open_files_in_pid() to get the set of open files of a process. This is currently only implemented for Linux and there is a test program called test_open_files in condor_c++_util which will do a simple test of the functionality. This is for ticket <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=1208\" onclick=\"get_ticket_and_populate_wrapper('1208'); return false;\" title=\"Dataflow with Files using Dagman\">#1208</a></span>.  (By Peter Keller )</td></tr>\n</tbody></table>", "type": "enhance", "last_change": "2012-Oct-17 12:25", "status": "new", "created": "2010-Feb-11 14:02", "fixed_version": "2010-Feb-11 14:02", "broken_version": "", "priority": "4", "subsystem": "Dag", "assigned_to": "", "derived_from": "", "creator": "psilord", "rust": "", "customer_group": "other", "visibility": "public", "notify": "psilord@cs.wisc.edu, wenger@cs.wisc.edu", "due_date": ""}
{"id": 3135, "title": "Ticket #3135: Specify dependencies in the schedd", "description": "<blockquote>\nWe would like the schedd to support simple job dependency workflows.</blockquote>", "remarks": "<blockquote>\n<em>2012-Sep-27 15:01:27 by wenger:</em> <br/>\n\nMy first question with this is, \"What is the use case?\". The main case where I can see that this would really provide an advantage over what we already have is when a user submits a job, and then later wants to submit another job that depends on the first one.  I've had users request something similar to this -- basically, \"Can I make a DAG where the first job is already running?\"\n\n<p>I'm kind of skeptical whether this would allow us to simplify the DAGMan implementation -- all of the existing dependency code would have to be there in case jobs have scripts, retries, etc., but then we'd have some extra code that would take advantage of the schedd dependencies.  Also, if DAGMan did the \"submit everything and exit\" mode of operation, you'd lose the capability to delete an entire workflow by condor_rm'ing the DAGMan job.  And I don't think it would work for nested DAGs, either.  So if DAGMan did have this capability, we'd have to have a flag to disable it.\n\n</p><p>Another thing to think about is error handling -- a big part of the DAGMan code is for dealing with errors, so if we're going to implement this I think we should get a pretty good handle on the error conditions we'd have to deal with.  For example, what if a job specifies a parent ID that doesn't exist?  What if a job is submitted with dependencies but then manually released?\n\n</p><p>I guess one possibility is that we'd implement this functionality, but not necessarily have DAGMan take advantage of it.\n\n</p><p>My thought at this point, though, is that I'd like some more specifics about what we think we'll gain from doing is, and also at least an idea of how elaborate the error handling will have to get.\n\n</p><p></p><hr/>\n<em>2012-Oct-20 05:26:43 by matt:</em> <br/>\n\nFour use cases where this request appears -\n\n<p>First, desire for full materialization of a DAG within the Schedd. The benefit here is all jobs in a workflow are readily available within the queue for inspection. All jobs can be managed and monitored with standard tools without having an extra step of processing the dag file to figure out what is yet to come. For instance, how close is a dag to completion using standard tools?\n\n</p><p>Second, DAG editing, with full materialization of a dag within the schedd, the dependency relationships can be modified. Right now that modification requires stopping dagman, changing the dag, restarting dagman.\n\n</p><p>Third, team collaboration, when one user has a workflow that needs to depend on another user's workflow, or a specific step in another user's workflow.\n\n</p><p>Fourth, scheduler migration, simply users who are used to this functionality within other schedulers and do not want to move to dagman when moving to condor.\n\n</p><p></p><hr/>\n<em>2012-Oct-20 05:56:28 by matt:</em> <br/>\n\nFirst blush comments on design -\n\n<p>Avoid using the HOLD state. Using hold will introduce noise into a state the provides valuable signal about jobs that cannot run, often because of errors, and require manual intervention. Instead, add a new policy that defines if a job is runnable. The job would remain Idle, but not be considered for execution until the policy expression evaluated to true. Note: this functionality could eventually be used for cron jobs as well.\n\n</p><p>Another reason not to piggyback on Hold and PERIODIC_RELEASE is the rate at which PERIODIC_RELEASE is evaluated. Job runnability wrt dependencies should be edge triggered, i.e. when dependencies completes a job should immediately be runnable. The process of updating child jobs can also reevaluate their runnability without having to process the entire potential PERIODIC_RELEASE set, which is everything based on how periodic policies are implemented.\n\n</p><p>Consider what will happen if a big dag user, O(10K+) nodes, tries to use this functionality instead of DAGMan. For instance, <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=ChildJobs\" title=\"Child Jobs\">ChildJobs</a></span> w/ O(1000) entries. If you don't want people to replace DAGMan with this functionality, provide information/guidance what a workflow looks like that should use DAGMan instead, e.g. if you want pre/post, if you have more than 100 children per node, etc.\n\n</p><p>For users, jobs ids are only ClusterId.ProcId. Technically there's a third component that DAGMan can see, but it does not have to be exposed to users. It will also be .0 in all cases.\n\n</p><p>A simplification would be to have ParentJob_110_0_JobStatus instead of Exit* attributes, but I can see how Exit* may be easier to manage (attr mirroring happens less frequently) and provide more information.\n\n</p><p>Some conditions to consider: parent not in queue when child submitted (error or assume completed?), parent removed from queue without running (remove child? hold child as never runnable, an error?), dependency modification (multiple qedits) won't be atomic, dependency modifications on a running job (add parent when child is starting)\n\n</p><p>condor_q -analyze will need to learn about jobs dependencies. Teaching it about a runnable policy will be cleaner than teasing the information out of Hold.\n\n</p><p></p><hr/>\n<em>2012-Oct-22 13:15:11 by tstclair:</em> <br/>\n\nI think you could be opening pandoras box.  Why not just use <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=JobPrio\" title=\"Job Prio\">JobPrio</a></span>, because you still can't get cross schedd deps without a massive snafu, and <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=JobPrio\" title=\"Job Prio\">JobPrio</a></span> should ensure \"relative\" ordering within a single schedd.\n<hr/>\n<em>2012-Dec-11 16:38:39 by johnkn:</em> <br/>\n\nBulk change of target version from v070902 to v070903 using ./ticket-target-mover.</blockquote>", "derived_tickets": "", "attachments": "<blockquote>\n<ul>\n<li><a href=\"attach_get/716/Incorporating+dependencies+in+the+schedd.docx\">Incorporating dependencies in the schedd.docx</a>\n19263 bytes added by nwp on 2013-Apr-02 19:17:56 UTC.\n<br/>\nfourth edition<br/>\n</li></ul>\n</blockquote>", "check_ins": "", "type": "enhance", "last_change": "2013-Jan-29 14:14", "status": "reviewdesign", "created": "2012-Jul-17 16:03", "fixed_version": "2012-Jul-17 16:03", "broken_version": "v070900", "priority": "3", "subsystem": "Daemons", "assigned_to": "tannenba", "derived_from": "", "creator": "nwp", "rust": "", "customer_group": "other", "visibility": "public", "notify": "tannenba@cs.wisc.edu wenger@cs.wisc.edu matt@cs.wisc.edu tstclair@redhat.com", "due_date": ""}
{"id": 3411, "title": "Ticket #3411: What does the chtcjobwrapper do?", "description": "<blockquote>\nIdentify exactly what functionality the chtcjobwrapper is providing, with the goal of identifying features that Condor currently provides and features Condor should provide.\n\n<p><span class=\"section\"></span></p><h2>Summary </h2>\n\n<p></p><ul>\n<li><strong>Updates itself</strong> - Users never update things; can be good for admin to do for them.\n</li><li><strong>Varies download source by execute node</strong> - User may know better sources for downloads.\n</li><li><strong>Logs progress</strong> - Useful for debugging, analysis\n</li><li><strong>Logs local environment</strong> - Useful for debugging\n</li><li><strong>Detects OS/Distribution version</strong> - For platform specific binaries\n</li><li><strong>Identify HTTP cache</strong> - For download speed, bandwidth savings\n</li><li><strong>Puts . in PATH</strong> - Simplify user's life\n</li><li><strong>Puts current working directory in $HOME</strong> - Simplifies running R and Matlab\n</li><li><strong>Downloads files</strong> - So we don't crush the submit point with input files\n</li><li><strong>Sets up prerequisites (R/Matlab/DMTCP)</strong> - Fiddly!\n</li><li><strong>Runs program/script correctly</strong> - R jobs need non-obvious command line\n</li><li><strong>Checks checksums for prerequisites</strong> - Can we trust network/disk?\n</li><li><strong>Selects appropriate build of prerequisites for the platform</strong> - Second part of detecting OS/Distro version\n</li><li><strong>Delete/don't send back unneeded files</strong> - Save bandwidth, disk space\n</li><li><strong>Sets requested environment variables</strong> - Absolute paths to scratch dir and appending to PATH are hard now\n</li><li><strong>Runs a pre-program and post-program</strong> - Execute side setup and teardown prior to real job\n</li><li><strong>Changes to another directory</strong> - ? benefit unknown\n</li><li><strong>Ensures the \"real\" job is executable</strong> - Wrappers means easy to get permissions wrong.\n</li><li><strong>Captures job standard output to another file</strong> - To separate from wrapper's noise\n</li><li><strong>Acts as a simple scripting language</strong> - Essentially execute-side DAGs for fast chains of programs\n</li><li><strong>Removes standard output and error</strong> - ? benefit unknown\n</li></ul>\n\n<p><span class=\"section\"></span></p><h2>Details </h2>\n\n<p><span class=\"subsection\"></span></p><h3>Updates itself</h3>\n\n<p>Specifics: the job wrapper is actually a relatively thin script that exists to download the \"real\" script (chtcinnerwrapper) from our web site.  This was a user can have a single static copy that they submit as part of their jobs and never have to update, but they get the benefits of bugfixes and changes made by CHTC.\n\n</p><p>Broader applicability: A user might want to make a broad statement (\"I need R version 2.15.X\") and want to be never change their submit file again.  They may want to delegate worrying about new builds for new operating systems, or bug-fixes, to someone else, presumably a system administrator.  Actual experience supports this, if you ask users to change their functioning systems, they resist.\n\n</p><p><span class=\"subsection\"></span></p><h3>Varies download source by execute node</h3>\n\n<p>Specifics: The URLs used to download additional files changes depending on where the script is running.  Specifically, if it detects it's running on a machine with <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=IsEuclid\" title=\"Is Euclid\">IsEuclid</a></span> set, it downloads from a local mirror instead of the public site.\n\n</p><p>Broader applicability: A clueful user (or possibly administrator) may know that jobs running at site X have access to a fast local mirror of desired shared files.  The local mirror's location varies from site to site.  I could certainly see this being true from high energy physics data, or gene sequence databases.  This might be useful functionality to put in the users hands, or in the administrators hands.  But, it raises the possibility of being wrong, \"Oops, the local mirror is missing this file, or has an out of date file,\" in which case you either need to download from the public mirror, or you need to declare failure.\n\n</p><p><span class=\"subsection\"></span></p><h3>Logs progress</h3>\n\n<p>Specifics: A log of downloads and runtime is maintained and returned to the user.  The file is designed to be easy to parse.\n\n</p><p>Broader applicability: What was downloaded, that it was or wasn't successful, and how long it took should certainly be logged.  (We use download durations as a crude tool for measuring effectiveness of local caches.)   But where?  Especially if setting up a job environment is more user facing, this shouldn't be hiding in a <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=StarterLog\" title=\"Starter Log\">StarterLog</a></span> on a machine they may not be able to log into.  Currently it ends up in a file called AuditLog.[something].  The information could certainly be dumped into supplemental messages in the user log.\n\n</p><p>The runtime is an interesting question.  Time spent downloading and setting up an environment, and time spend actually running the job are both potentially useful to someone managing the system.  Notably, if setup time is dwarfing productive time, it's a clue there is a problem.  Simply lumping the two together is probably the wrong answer, and making download and setup \"free\" is similarly wrong.\n\n</p><p>If we consider keeping an installed environment on an execute node and sharing it between jobs, the idea of the cost of setting up a job is even weirder.  Should later jobs hear what it originally cost, if only so they can easily see how much they saved?\n\n</p><p><span class=\"subsection\"></span></p><h3>Logs local environment</h3>\n\n<p>Specifics: The script logs information about the executing system: hostname, current environment, free disk space, current working directory, Linux distribution version, contents of scratch directory and potentially other directories, total size of the scratch directory.  The contents and size of the scratch directory may be reported multiple times, before and after various steps run.  This information has been found to be useful when diagnosing problems.\n\n</p><p>Broader applicability: When a user is debugging a failed job, there is a pile of potentially useful information about the executing system they might like to have.  Some is available in the machine's classad (<span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=OpSys\" title=\"Op Sys\">OpSys</a></span> and Arch), although if the machine is down the information may be gone, some information (Disk) is probably no longer accurate, and nearly anything may change if hardware or software is upgraded.  Other information was never present in the machine's classad (environment, current working directory, distribution version).  Either way, once the job has failed, your chance to collect useful information is gone.  Maybe you can instrument your program and rerun it and see the same or a similar failure, but if failures are unusual this can be difficult.  Perhaps Condor should identify a list of \"common causes of failure for a program\" (not enough disk, incorrect environment, etc), collect data about those, and store it somewhere convenient for the user.  The user log is the most obvious spot, but a user might specify condor_diagnostic_file=diagnostics.txt in their submit file.\n\n</p><p>The contents of the scratch directory at various stages (at the start, after unpacking any binaries or data, at the end) can also be useful.\n\n</p><p><span class=\"subsection\"></span></p><h3>Detects OS/Distribution version</h3>\n\n<p>Specifics: The script crudely determines if it's on Scientific Linux 5 or 6. It needs this information to make decisions about what binaries to download.\n\n</p><p>Broader applicability: Condor is much better about providing this information (<span class=\"quote\">OpSysAndVer, OpSysVer, OpSysShortName, OpSysLongName, OpSysMajorVer, OpSysName</span>).  We might be done here.  Potential future areas for consideration:\n\n</p><p></p><ul>\n<li>More robust detection - Currently the Linux solution assumes /etc/issue's first line is something reasonable and easy to parse. Appears to work well in practice.\n</li><li>Identification of equivalent versions - Currently there is no clue that SL6 and CentOS6 are nearly equivalent to RedHat6\n</li><li>Identification of other relevant aspects - Major library versions, in particular.\n</li></ul>\n\n<p><span class=\"subsection\"></span></p><h3>Identify HTTP cache</h3>\n\n<p>Specifics: OSG sites don't necessarily set HTTP_PROXY or http_proxy; we can only rely on OSG_SQUID_LOCATION.  So the script ensures that HTTP_PROXY and http_proxy, basing them on http_proxy, HTTP_PROXY, or OSG_SQUID_LOCATION as it can be found.  The specific value cannot be known in advance; it's site specific.\n\n</p><p>Broader applicability: We may need up elements in the environment based on what we detect about the local site.  We might be able to get the information from the machine ad (in which case it's a simple matter of \"environment=HTTP_PROXY=$$(SQUID_LOCATION)\"), but we might be stuck with nosing around in the environment as we do here.\n\n</p><p><span class=\"subsection\"></span></p><h3>Puts . in PATH</h3>\n\n<p>Specifics: . is put at the front of the path list.  While a bad idea in the general case for security reasons, for a job running in a sandbox, probably safe?  Probably in place to help people who insecurely put . in their path for their terminal.\n\n</p><p>Broader applicability: Potentially dangerous if the job is prone to chdiring to weird places, but probably generally safe, and it eliminates a common problem in writing a shell script to drive your other programs.\n\n</p><p><span class=\"subsection\"></span></p><h3>Puts current working directory in $HOME</h3>\n\n<p>Specifics: the environment variable HOME is set to the scratch directory.  This is useful because Matlab and R assume they can put dot files in your home directory.  An HTCondor job's user may not have a home directory or that they have access and if they do we probably don't want to put files there.\n\n</p><p>Broader applicability: This seems like pure win for dedicated execute nodes; it helps in a small way with job isolation.  It's bad, however, if the job might really have access to your home directory (NFS?) and you want that.  Perhaps we should automatically do it if slot users are being used (not clear how widely true this is for OSG)\n\n</p><p><span class=\"subsection\"></span></p><h3>Downloads files</h3>\n\n<p>Specifics: Prerequisite packages and data files are downloaded via HTTP.  This is done to minimize load/bandwidth on the submit point.  Through the use of a local HTTP cache (identified earlier), we can eve minimize load/bandwidth on the webserver.\n\n</p><p>Broader applicability: Bandwidth is a challenge for any job with large binaries or data sets.  Offering a ways to offload this is a win.  The obvious answer is file transfer hooks, and that may solve the problem for most people.  One challenge is that file transfer hooks are provided by the administrator, which leaves the user out of luck if the administrator hasn't or won't set up a hook for the desired protocol, be it HTTP, <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=BitTorrent\" title=\"Bit Torrent\">BitTorrent</a></span>, or something else.\n\n</p><p><span class=\"subsection\"></span></p><h3>Sets up prerequisites (R/Matlab/DMTCP)</h3>\n\n<p>Specifics: The script ensures prerequisites are ready to go.  Most of these steps are optional, but they're all available.  Packages include: Matlab, R, DMTCP, \"sharefiles.tar.gz\" (user defined), SLIBS.tar.gz or SLIBS1.tar.gz (user defined libraries, should contain directory \"SS\"), RLIBS.tar.gz (user defined R libraries), SITEPACKS.tar.gz (user defined Python libraries? Unsure.)\n\n</p><p></p><ol>\n<li>Download the package. - Doesn't use Condor's built in file transfer so we can potentially get benefits from local caches and multiple mirrors.  Relies on the HTTP cache having been properly identified and set up properly.  Pretty much guaranteed to be .tar.gz\n</li><li>Unpack the package. - tar xzf filename.tar.gz\n</li><li>Set up the environment - The varies from prerequisite to prerequisite, although setting the PATH and LD_LIBRARY_PATH are common. In addition, R sets RHOME, Matlab sets XAPPLRESDIR, DISPLAY, MATLAB_PREF, and MCR_CACHE_ROOT.  R also rewrites the (official) R wrapper script itself to set a path (probably could be worked around by shipping a modified R wrapper script.)\n</li></ol>\n\n<p>It's possible to use multiple prerequisites.  Notably, a user might have a custom library they need for their R or Matlab program\n\n</p><p>Broader applicability: This is the meat of what we're interested in.\n\n</p><p></p><ol>\n<li>Download the package. - See \"Downloads files\" above.\n</li><li>Unpack the package. - No current support beyond \"teach your job to do it.\"  If the job requires the package to even start, this means in practice \"write a wrapper to do it.\"  Might be part of the download hook (httpandunpack://url/foo.tar.gz).\n</li><li>Set up the environment - If the job requires the package to even start, this means in practice \"write a wrapper to do it.\"  Slightly challenging because many of these settings  should be absolute paths into the scratch directory, so they can't be simply set in the environment setting in the submit file.\n</li></ol>\n\n<p><span class=\"subsection\"></span></p><h3>Runs program/script correctly</h3>\n\n<p>Specifics: As far as HTCondor is concerned, the executable is always the wrapper script. But as far as the user is concerned, the executable is something else, and there are additional arguments to pass in.  For Matlab, it's an actual executable, but for R it's a script.  Instead of running \"./myprogram arguments\", you invoke \"R-install-dir/bin/Rscript --no-save ./myprogram arguments\".\n\n</p><p>Broader applicability: If we're running a script, and we're bringing the interpreter along with us (from a shared location), what is logically our executable (the script) actually isn't, it's the interpreter, and we may need to have a complex command line for the interpreter.  Maybe this should be in the user's hands (allows additional options to the interpreter), or maybe HTCondor should handle everything for simplicity.\n\n</p><p><span class=\"subsection\"></span></p><h3>Checks checksums for prerequisites</h3>\n\n<p>Specifics: In addition to downloading a prerequisite package, the wrapper also downloads a SHA1 checksum file and compares it to the sha1sum of the downloaded package.  If the checksums don't match, it's a failure.  Both the package and the checksum come from the same source, so this is a weak defense against active attack, but it can potentially detect accidental corruption.\n\n</p><p>TODO: Check with bt to see if it's ever actually paid off.  Maybe detected stale copies from cache?\n\n</p><p>Broader applicability: Our file transfer mechanism (HTTP) is basically unsecured, and could conceivably be corrupted in transit, in memory, or on disk.  The actual risk seems likely to be low.  We could teach Condor to checksum transferred files easily, although it's not clear what they would be checked against, especially if the file's identity was decided at runtime (ex. \"executable=my-program.$$(OpSysAndVer)\").\n\n</p><p><span class=\"subsection\"></span></p><h3>Selects appropriate build of prerequisites for the platform</h3>\n\n<p>Specifics: Appears to download both the SL5 and SL6 binaries and unpacks the appropriate one.  Some prerequisites may have both sets of binaries inside of them, with each .so file prefixed with sl5- or sl6-; the appropriate one will be symlinked to.\n\n</p><p>Broader applicability: seems like this is better solved with $$(OpSysAndVer) to download the appropriate version.   But, if you might download Matlab-RedHat6.tar.gz or Matlab-Redhat5.tar.gz or even Matlab-Debian10.tar.gz, we need to handle unpacking correctly.\n\n</p><p><span class=\"subsection\"></span></p><h3>Delete/don't send back unneeded files</h3>\n\n<p>Specifics: To minimize bandwidth and submit-machine disk usage, the wrapper ensures that a list of files is automatically deleted, or at least not sent back.  It's important to catch them when the job finishes or if the job is evicted and when_to_transfer_output=ON_EXIT_OR_REMOVE.\n\n</p><p></p><ol>\n<li>Prerequisite packages - Sometimes added to deletion list and deleted at the end, sometimes deleted as soon as it's unpacked.\n</li><li>Prerequisite installations - We rely on them being in a subdirectory, and that HTCondor ignores subdirectories when transferring files back.  This may not be a safe assumption in the future.\n</li><li>User specified files - Deleted when we finish.  These are (typically) temporary working files that the user's job fails to clean up, but that we don't care about.  Specified in a list in the file REMOVE.\n</li></ol>\n\n<p>Broader applicability: Very relevant, for much the reasons given above.  1 can be avoided by doing downloads into another location (perhaps ./tmp?), or the window reduced by deleting the file as soon as it's unpacked. 2 works for now if everything is in a subdirectory.\n\n</p><p><span class=\"subsection\"></span></p><h3>Sets requested environment variables</h3>\n\n<p>Specifics: The user can provide an ENV file which contains instructions for environment variables to set.  Obviously this can be handled with the environment setting in the submit file, but this file allows for some special cases, all revolving around handling of paths and the current working directory.  These are useful because we might desire absolute paths, but can't predict the name of our scratch directory.\n</p><ol>\n<li>Prepend something to an environment variable, specifically PATH.  It may be useful to keep the system provided path, but to add another directory relative to our scratch directory.\n</li><li>Set an environment variable (especially PATH, but also others) to an absolute path relative to the scratch directory.\n</li></ol>\n\n<p>Broader applicability: The two cases above apply to many other situations.  If you're installing libraries and applications in our scratch directory, there are plenty of possible reasons to set environment variables pointing at subdirectories or the scratch directory itself.  Relative paths aren't what we want; sometimes they won't be accepted, more commonly they'll break if the program changes directories.  We might special case a search and replace for [SCRATCH_DIR] in the environment variables to put in the absolute path to the scratch directory.  Or maybe allow any environment variable (PERL5LIB=[PERL5LIB]:[SCRATCH_DIR]/myperl).  The later also allows for appending/prepending.\n\n</p><p><span class=\"subsection\"></span></p><h3>Runs a pre-program and post-program</h3>\n\n<p>Specifics: Runs a user provided pre-program (\"pre-script\") prior to running the real job.  The same is done for a post-program (\"post-script\")  The program is run blindly and the wrapper continues without regard to the result.  TODO: Who and or what is this for?  Perhaps to allow a bit of simple setup before running a \"real\" Matlab/R program without need to add an entire shell script wrapper?\n\n</p><p>Broader applicability: Unknown.  <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=PreCmd\" title=\"Pre Cmd\">PreCmd</a></span> and <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=PostCmd\" title=\"Post Cmd\">PostCmd</a></span> may fit the bill, although they lack the ability to take command line arguments like the wrapper.  Need to also consider behavior if a given command fails: does a failed <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=PreCmd\" title=\"Pre Cmd\">PreCmd</a></span> mean we shouldn't run the actual job (and what do we return); do we return the job's exit code or <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=PostCmds\" title=\"Post Cmds\">PostCmds</a></span>?\n\n</p><p><span class=\"subsection\"></span></p><h3>Changes to another directory</h3>\n\n<p>Specifics: Prior to running the job (but after downloading and unpacking various packages!), simply chdirs to a new directory specified by the user, presumably a relative path.  Use case is unknown (TODO).\n\n</p><p>Broader applicability: initialdir does the right thing for standard universe, or vanilla universe with a shared filesystem, but not otherwise.  But I'm dubious as to actual need for this.\n\n</p><p><span class=\"subsection\"></span></p><h3>Ensures the \"real\" job is executable</h3>\n\n<p>Specifics: chmod 755 $real_executable.  Since the executable isn't the executable Condor recognizes, and may have come from transfer_input_files, or from a download and unpackage earlier, it's possible that the execute bit isn't set.\n\n</p><p>Broader applicability: If we move toward having Condor do the work of the wrapper script, the executable really will be the executable, largely eliminating the problem.  It can come up in other cases (multiple executables), but the wrapper script doesn't worry about those at all.\n\n</p><p><span class=\"subsection\"></span></p><h3>Captures job standard output to another file</h3>\n\n<p>Specifics: The real executable's standard output can be redirected to a specified file, keeping it distinct from output generated by the wrapper script, programs called by the wrapper script (wget/curl/tar), and the PRE and POST scripts.  Standard error is not similarly captured.\n\n</p><p>Broader applicability: Not relevant.  First, it's arguably backward; the wrapper script should put it's off-topic noise into a separate file and keep the \"real\" standard output and error clean for the real executable.  Second, if we move a bunch of this functionality into Condor, the information will naturally end up elsewhere, not in the job's stdout/stderr.\n\n</p><p><span class=\"subsection\"></span></p><h3>Acts as a simple scripting language</h3>\n\n<p>Specifics: Instead of a simple executable to run, the user can specify, in a file named CONTROL, a list of executables to run.  This essentially forms an incredibly simple scripting language, where each executable is run serially; any step returning non-zero terminates the run.  The benefit of knowing how to re-write a job to run under R is preserved, so this can be used to easily run several R jobs one-after another\n\n</p><p>Broader applicability:   The fundamental problem: I have several programs that chained in a DAG, but there is no need to pull temporary results back (say, because many steps are really fast).  This seems like a real issue.  Solvable today with a shell script (but maybe not so easy to write, especially for R whose command line gets rewritten).  Also solvable with execute-side DAGs, but support for that in Condor is weak.\n\n</p><p><span class=\"subsection\"></span></p><h3>Removes standard output and error</h3>\n\n<p>Specifics: At user request, removes/stops transfer of the standard output or error.  This will include output from the wrapper script and potentially output from the real executable.\n\n</p><p>Broader applicability: No idea. Perhaps saving bandwidth/disk for large scale DAGs?  Kills ones ability to investigate problems. TODO</p></blockquote>", "remarks": "<blockquote>\n<em>2013-Jan-04 13:22:54 by bbockelm:</em> <br/>\n\nCouldn't some of the logging information go into the job's user log?\n\n<p>I've long wanted a few of the \"standard\" wrapper outputs to go into the user log.  For example, we train users to do \"date; env; ls; hostname\" at the beginning of the wrapper.  No reason HTCondor can't do this for us already.\n\n</p><p>Depending on how many files are transferred, per-file stats might be a bit verbose (especially as we can recursively transfer a directory!).  However, per-protocol (HTCondor file transfer, file transfer plugin protocol A, protocol B) statistics would be useful.</p></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "todo", "last_change": "2013-May-29 16:01", "status": "resolved", "created": "2013-Jan-04 11:43", "fixed_version": "2013-Jan-04 11:43", "broken_version": "", "priority": "4", "subsystem": "", "assigned_to": "adesmet", "derived_from": "", "creator": "adesmet", "rust": "", "customer_group": "other", "visibility": "public", "notify": "", "due_date": ""}
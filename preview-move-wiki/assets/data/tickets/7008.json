{"id": 7008, "title": "Ticket #7008: Terminate event missing from DAGMan job event log", "description": "<blockquote>\nStefano Belforte from CMS reports an incident where a job that's part of a DAG completes and leaves the queue, but there's no terminate event in the DAGMan event log. There is a terminate event in the corresponding user job event log.\n\n<p>Ticket in CRABServer issue tracker: <a class=\"external\" href=\"https://github.com/dmwm/CRABServer/issues/5876\">https://github.com/dmwm/CRABServer/issues/5876</a></p></blockquote>", "remarks": "<blockquote>\n<em>2019-Apr-18 14:13:29 by jfrey:</em> <br/>\n\nStefano Belforte's original email:\n\n<p></p><div class=\"verbatim\">\n<pre>Hi Jaime\nsorry for direct mail, I do not seem to have better ways to report this.\nI have found a couple occasions where it looked like DAGMAN was loosing\ntrack of some node, there may be more of course, so this time I thought\nto report it.\nSince it is very rare, it is not a real problem for us, but in case\nyou want to find out what's happening... you may want to open a tkt\n\nI have this DAG with 5508 nodes, each of them is a simple\nprejob/vanillajob/postjob (the CMS CRAB standard)\nFor node 5044 I find in DAGMAN status file (which we call node_state)\n[\n Type = \"NodeStatus\";\n Node = \"Job5044\";\n NodeStatus = 3; /* \"STATUS_SUBMITTED\" */\n StatusDetails = \"not_idle\";\n RetryCount = 0;\n JobProcsQueued = 1;\n JobProcsHeld = 0;\n]\n\nand in dagman log\n[cms1627@vocms0106 cluster28708598.proc0.subproc0]$ grep 28720855 RunJobs.dag.nodes.log\n000 (28720855.000.000) 04/17 02:25:58 Job submitted from host: &lt;188.184.85.27:4080?addrs=188.184.85.27-4080&amp;noUDP&amp;sock=1426426_7bc8_3&gt;\n001 (28720855.000.000) 04/17 02:45:57 Job executing on host: &lt;131.225.191.224:33998?CCBID=137.138.148.95:9618%3faddrs%3d137.138.148.95-9618%26noUDP%26sock%3dcollector9671#600617%20131.225.205.232:9639%3faddrs%3d131.225.205.232-9639#904863&amp;addrs=131.225.191.224-33998+[--1]-33998&amp;noUDP&gt;\n[cms1627@vocms0106 cluster28708598.proc0.subproc0]$\n\nand in DAGMAN stdout:\n[cms1627@vocms0106 cluster28708598.proc0.subproc0]$ grep 28720855 RunJobs.dag.dagman.out |tail -2\n04/18/19 19:01:06   Node Job5044, HTCondor ID 28720855, status STATUS_SUBMITTED\n04/18/19 19:11:09   Node Job5044, HTCondor ID 28720855, status STATUS_SUBMITTED\n[cms1627@vocms0106 cluster28708598.proc0.subproc0]$\n\n\nBut that job completed one day ago:\n[cms1627@vocms0106 cluster28708598.proc0.subproc0]$ condor_history -userlog job_log 28720855\nID     OWNER          SUBMITTED   RUN_TIME     ST COMPLETED   CMD\n28720855.0   cms1627         4/17 02:25   0+00:07:05 C   4/17 02:53 ???\n\nsame if I query global logs instead of the userlog\n[cms1627@vocms0106 cluster28708598.proc0.subproc0]$ condor_history --match 1 28720855\nID     OWNER          SUBMITTED   RUN_TIME     ST COMPLETED   CMD\n28720855.0   cms1627         4/17 02:25   0+00:07:16 C   4/17 02:53 /data/srv/glidecondor/condor_lo\n[cms1627@vocms0106 cluster28708598.proc0.subproc0]$\n[cms1627@vocms0106 cluster28708598.proc0.subproc0]$ condor_q 28720855\n\n\n-- Schedd: crab3@vocms0106.cern.ch : &lt;188.184.85.27:4080?... @ 04/18/19 19:31:33\nID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD\n\n0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended\n[cms1627@vocms0106 cluster28708598.proc0.subproc0]$\n\n\nWhy does DAGMAN still think that it is running ?\n\nI believe you have access to vocms0106 for investigating, the directory is\n/data/srv/glidecondor/condor_local/spool/8598/0/cluster28708598.proc0.subproc0\n\nand I am certainly and always available to help if useful.\n\nThanks\nStefano\n</pre></div>\n\n\n<p>And a followup:\n</p><div class=\"verbatim\">\n<pre>clarification:\nwe need the PostJob script to run for all CRAB jobs, but\nhere DAGMAN is not running it. Which is how I noticed it and\nwhy it is a problem).\n\nCurrent workaround is for user to kill and resubmit in CRAB\nwhich forces the DAGMAN to rerun this node via its rescue mode.\nBut I do not need this DAG to be 100% complete now, so am\nleaving it as is in case there's some useful info in logs.\nThat directory should stay around for 4 more weeks.\n\nStefano\n</pre></div>\n\n\n<p></p><hr/>\n<em>2019-Apr-18 15:00:40 by jfrey:</em> <br/>\n\nBasic information:\n<ul>\n<li>Condor Version 8.6.13\n</li><li>Machine vocms0106.cern.ch\n</li><li>user cms1627\n</li><li>DAG job id 28708598.0\n</li><li>node job id 28720855.0\n</li><li>DAG node name Job5044\n</li></ul>\n\n<p>The user and dagman event log files are on local disk, in the same directory. No other events in the dagman event log have the same timestamp. The shadow daemon log has no errors related to the event log, and the shadow process continues normal operation after writing the terminate event.\n\n</p><p>Here are the events for the job:\n</p><ul>\n<li>04/17 02:25:58 submit event\n</li><li>04/17 02:45:57 execute event\n</li><li>04/17 02:46:06 image size event\n</li><li>04/17 02:51:07 image size event\n</li><li>04/17 02:53:00 image size event\n</li><li>04/17 02:53:02 terminate event (not in dagman log)\n</li></ul>\n\n<p></p><hr/>\n<em>2019-Apr-26 14:22:30 by jfrey:</em> <br/>\n\nAfter examining the code, I see two explanations for the missing event:\n<ul>\n<li>Sometime between the writing of the execute and terminate events, something modified the memory location in the shadow holding the fd of the dagman log with a negative value. I see no legitimate code that would do this.\n</li><li>The write() syscall for writing to the dagman log returned a short write of 0 bytes. Documentation I've found online says this shouldn't happen (if the call is interrupted before any bytes are written, it should return -1), but some I/O drivers in the linux kernel might do it none-the-less.\n</li></ul>\n\n<p>The current log writing code flags an error if the write() call returns -1, but not if write() returns any non-negative value. We should change it so that any value less than the full size of the event indicates an error.\n\n</p><p></p><hr/>\n<em>2019-Jul-03 11:58:59 by coatsworth:</em> <br/>\n\n<strong>CODE REVIEW</strong>\n\n<p>All looks good.</p></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">2019-Jun-20 13:30</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/958b645e09ee77542eee46916f89ffc74cf4bd6e\">[57198]</a></span>: Docs for improved detection of event log write errors. <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=7008\" onclick=\"get_ticket_and_populate_wrapper('7008'); return false;\" title=\"Terminate event missing from DAGMan job event log\">#7008</a></span>  (By Jaime Frey )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2019-Jun-11 10:27</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/0a6c0edc7d3fbad2b027961da212c2b894b13bce\">[57136]</a></span>: Detect short writes to the job event logs. <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=7008\" onclick=\"get_ticket_and_populate_wrapper('7008'); return false;\" title=\"Terminate event missing from DAGMan job event log\">#7008</a></span>  (By Jaime Frey )</td></tr>\n</tbody></table>", "type": "defect", "last_change": "2019-Jul-09 15:16", "status": "resolved", "created": "2019-Apr-18 14:10", "fixed_version": "2019-Apr-18 14:10", "broken_version": "v080613", "priority": "3", "subsystem": "DaemonsSubmitNode", "assigned_to": "jfrey", "derived_from": "", "creator": "jfrey", "rust": "", "customer_group": "cms", "visibility": "public", "notify": "", "due_date": ""}
{"id": 1308, "title": "Ticket #1308: Do-it-yourself meta-scheduler toolkit", "description": "<blockquote>\nDAGMan is a great metascheduler, but its quite clear in its goal.  If you need to do metascheduling based on disk space management or you need cyclic workflows, DAGMan becomes a poor match.   We can keep extending DAGMan, but these tasks seem out of scope.  It would be nice to provide a simple toolkit that makes writing DAGMan-like systems easier.  I'm envisioning a sort of message pump API.  Something like\n\n<p></p><div class=\"code\">\n<pre class=\"code\">void initial_jobs(CondorMetaScheduler &amp; m) {\n    m.submit_job(step_1);\n}\n\nvoid job_done(CondorMetaScheduler &amp; m, unsigned int cluster, unsigned int process, int result) {\n    if(result == 0 &amp;&amp; job_is_step_1(cluster)) {\n        m.submit_job(step_2);\n    }\n}\nint main() {\n    CondorMetaScheduler m;\n    m.job_done_callback(job_done);\n    m.initialize(initial_jobs);\n    // If all you need is the simple pump, you can just call \"m.pump()\", but\n    // the expanded form is shown for completeness.\n    // \"done\" is defined as \"No jobs in queue, no misc work to do\"\n    while( ! m.done()) {\n        m.process();\n    }\n}\n</pre></div>\n\n\n<p>It might also be implemented by inheriting from <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=CondorMetaScheduler\" title=\"Condor Meta Scheduler\">CondorMetaScheduler</a></span> and overriding the default no-op implementations of CondorMetaScheduler::job_done or similar.\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">class MyScheduler : public CondorMetaScheduler {\npublic:\n    virtual void initialize() {\n        submit_job(step_1); /* Provided by CondorMetaScheduler */\n    }\n\n    virtual void job_done(unsigned int cluster, unsigned int process, int result) {\n        if(result == 0 &amp;&amp; job_is_step_1(cluster)) {\n            submit_job(step_2); /* Provided by CondorMetaScheduler */\n        }\n    }\n};\n\nint main() {\n    MyScheduler m; // Automatically calls m.initialize()\n    while( ! m.done()) {\n        m.process();\n    }\n}\n</pre></div>\n\n\n<p>No matter the API, it would include functionality to:\n\n</p><p></p><ul>\n<li>Automatically watch log files\n</li><li>Submit jobs\n</li><li>Call back as jobs exit the queue\n</li><li>Attach simple data to a job that I can query (so I can, say, flag a job with a useful human friendly name, or a CATEGORY ala DAGMan.)\n</li></ul>\n\n<p>Ideally this shouldn't be implemented alongside DAGMan.  Instead, the core logic of DAGMan would become this toolkit, and DAGMan would be implemented in terms of it.  That would ensure that the toolkit receives real testing.\n\n</p><p><span class=\"section\"></span></p><h2>Use case</h2>\n\n<p>This is based on a real world system I worked on:\n\n</p><p>There are 500 disconnected backup tasks to do.  Each task consists of: tar (create a tar file), scp (copy it to the archive server), rm (delete the local copy), wait 4 hours (yes, really), validate (ensure the archive server copied it to tape)\n\n</p><p>We faced several limits:\n</p><ul>\n<li>Don't run more than 2 tar commands simultaneously to avoid choking the system in I/O.\n</li><li>Don't run more than 2 scp commands simultaneously; that's plenty to saturate the bandwidth.\n</li><li>It at all possibly run a tar and an scp simutaneously; we're pressed for time and need to keep the entire pipeline full.\n</li><li>Between the tar and rm steps, we're using up local disk.  We have enough local disk to have anywhere from 1 to 5 tar files on disk simultaneously.  We <em>can</em> predict how big a given tar file will be in advance, but completely filling the disk is not an option.\n</li></ul>\n\n<p>There are further challenges:\n</p><ul>\n<li>The archive service sometimes is down, sometimes for days.  We'll need to retry the scp and validate steps; if the error is unable to connect, we should retry on a slow basis (say, every 5 minutes?) for days. If the error is something else, we should probably retry a few times quickly, then give up.\n</li><li>If any step fails, and retries don't help, we want to retry the entire chain, going all the way back to tar.\n</li><li>Once a filename is claimed on the archive server, it is claimed <em>forever</em>.  It is an error to try and rewrite it.  As a result if the scp fails, we might need to rewrite the names of the tar files. For example, \"-attempt-01.tar\" becomes \"-attempt-02.tar\".  If validate fails, when we retry the entire chain we need to signal that tar and later steps need to rewrite the names.\n</li></ul>\n\n<p>All of this is relatively straightforward, but proved extremely complex to implement using just DAGMan and Condor.  The implementation in production has a number of crude limitations.  I seriously considered writing a new metascheduler from scratch, but the overhead of worrying about following user logs, checking the output from condor_submit, and the like seemed infeasible given the very short deadline.  With a library like I describe above, I believe writing a new metascheduler would have been faster to create and better fitted to the problem than the Frankenstein creation I stitched together.</p></blockquote>", "remarks": "<blockquote>\n<hr/>\n<em>2010-Oct-20 16:03:30 by jfrey:</em> <br/>\n\nBulk change of target version from v070504 to v070505 using ./ticket-target-mover.\n<hr/>\n<em>2011-Jan-27 14:46:04 by danb:</em> <br/>\n\nBulk change of target version from v070505 to v070506 using ./ticket-target-mover.\n<hr/>\n<em>2011-Feb-01 16:01:00 by tannenba:</em> <br/>\n\nBulk change of target version from v070506 to NULL using ./ticket-target-mover.</blockquote>", "derived_tickets": "", "attachments": "", "check_ins": "", "type": "enhance", "last_change": "2011-Feb-01 16:01", "status": "new", "created": "2010-Mar-25 16:59", "fixed_version": "2010-Mar-25 16:59", "broken_version": "v070500", "priority": "3", "subsystem": "Dag", "assigned_to": "", "derived_from": "", "creator": "adesmet", "rust": "", "customer_group": "other", "visibility": "public", "notify": "psilord@cs.wisc.edu wenger@cs.wisc.edu matt@cs.wisc.edu", "due_date": ""}
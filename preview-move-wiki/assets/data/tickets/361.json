{"id": 361, "title": "Ticket #361: Export failover functionality in HDFS datanode as plugin", "description": "<blockquote>\nThis has two parts (a) To let data-nodes of Hadoop fail-over if they loose connection with the name server (b) We want to have a plug-in that is invoked and kind of tell data-node what it should do if it looses connection with a name server.\n\n<p>We would be able to use this plug-in to query our collector to know the network address of redundant name server (if first one fails) and have data-node reconnect. The same mechanism then can be used in non-Condor context where someone what to use their own intelligent methods.</p></blockquote>", "remarks": "<blockquote>\n<em>2009-Apr-02 19:53:13 by faisal:</em> <br/>\n\nHere is how I created a prototype implementation and would appreciate Dhruba's comments on it.\n\n<p>For my first version I used the plug-in mechanism used by HADOOP-5257-v8.patch. I do have a small comment on this that is coming in a moment. What I noticed in <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=DataNode\" title=\"Data Node\">DataNode</a></span> code is that it uses a RPC proxy for all communication with the name node. So, once the connection via this proxy is broken, we probably need to create a new proxy object as these objects are tied to network address.  Once I get this object based on new network location of name node, 'handshake' and 'register' can take care of some initial bootstrap.\n\n</p><p>The plug-in code is invoked once the 'sendHeartbeat' fails and this plug-in updates the local configuration object (passed to it as argument when notifying it about lost connection). The fail-over code within data-node will use this new information to create the proxy object and proceed as explained above.\n\n</p><p>One comment about using HADOOP-5257's plug-in modules is that those are specific to starting and stopping daemons along with name/data node startup/shutdown. Although we can add a generic changeState(state_info, Object) method to surf other purposes like ours but still in our case we probably need at one time only one plug-in to handle fail-over and only that plug-in should be notified when a connection is broken.\n\n</p><p>I will try to attach a patch by tomorrow after doing some cleanup in the code. I still need to test the full effects of these changes on Hadoop. Any suggestion or comments will be helpful especially about the proper way to do the re-handshake and re-register with a new name-server.\n\n</p><p></p><hr/>\n<em>2009-Apr-05 03:32:07 by dhruba:</em> <br/>\n\nCalling doFailOver from inside the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=DataNode\" title=\"Data Node\">DataNode</a></span> code works fine, but probably won't be accepted by the community to be committed to the apache hadoop source svn. The reason being that it is not part of any pluggable API.\n\n<p>what u really need is <a class=\"external\" href=\"http://issues.apache.org/jira/browse/HADOOP-3628\">http://issues.apache.org/jira/browse/HADOOP-3628</a>. But this JIRA will take a longer time to get committed.\n\n</p><p>For ur experiments, you can always distribute ur own modified version of hadoop code.\n\n</p><p></p><hr/>\n<em>2009-Apr-16 12:49:15 by dhruba:</em> <br/>\n\nAnother one that might came handy for you: <a class=\"external\" href=\"http://issues.apache.org/jira/browse/HADOOP-5640\">http://issues.apache.org/jira/browse/HADOOP-5640</a>\n\n<p></p><hr/>\n<em>2009-Apr-16 12:51:11 by dhruba:</em> <br/>\n\nThis might be handy for you: <a class=\"external\" href=\"http://issues.apache.org/jira/browse/HADOOP-5640\">http://issues.apache.org/jira/browse/HADOOP-5640</a>\n\n<p></p><hr/>\n<em>2009-Apr-22 10:59:05 by dhruba:</em> <br/>\n\nLike we discussed, if we restart datanode(s) then none of the above JIRAs are required. However, we do need the client to not-hang-indefinitely when datanode(s) dies. Details in <a class=\"external\" href=\"http://issues.apache.org/jira/browse/HADOOP-2757\">http://issues.apache.org/jira/browse/HADOOP-2757</a>.</blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body><blockquote>\n<ul>\n<li><a href=\"../files/44/HADOOP_failover.patch\">HADOOP_failover.patch</a>\n2566 bytes added by faisal on 2009-Apr-05 07:38:49 UTC.\n<br/>\nInitial proof of concept for doing fail over of data nodes upon losing connection with name server. <br/>\n</li></ul>\n</blockquote></body></html>", "check_ins": "", "type": "todo", "last_change": "2009-Dec-26 22:00", "status": "active", "created": "2009-Apr-02 16:43", "fixed_version": "2009-Apr-02 16:43", "broken_version": "", "priority": "3", "subsystem": "", "assigned_to": "dhruba", "derived_from": "#245", "creator": "faisal", "rust": "", "customer_group": "other", "visibility": "public", "notify": "tannenba@cs.wisc.edu", "due_date": ""}
{"id": 4401, "title": "Ticket #4401: HTCondor CGROUP use might be causing OS lockup", "description": "<blockquote>\nHTCondor's use of CGROUPs seems to be causing operating system lockups and crashes.  CGROUPs and PID namespaces were introduced to the CHTC pool on Thursday, June 5th. Since then they have cause HTCondor to cease functioning. At first we thought PID namespaces were to blame because on all the machines that were dysfunctional, Zombie processes like these could be seen in abundance.\n\n<p>From e074.chtc.wisc.edu:\n</p><div class=\"code\">\n<pre class=\"code\">20652    30026  0.0  0.0      0     0 ?        Zs   Jun12   0:00 [condor_pid_ns_i] &lt;defunct&gt;\n</pre></div>\n\n\n<p>Also, the \"ps aux\" command would lock up, refusing to return control to the terminal.\n\n</p><p>On Thursday, June 12th Infrastructure began disabling PID namespaces in the CHTC pool.  However, the lockups continued.  Defunct processes again showed up in abundance on the locked up systems.\n\n</p><p>From e075.chtc.wisc.edu:\n</p><div class=\"code\">\n<pre class=\"code\">slot7     8014  1.6  0.0      0     0 ?        Zs   Jun12  15:42 [condor_exec.exe] &lt;defunct&gt;\ncondor   13554  0.0  0.0  90288  6884 ?        Ss   Jun12   0:00 condor_starter -f -a slot1_5 login02.hep.wisc.edu\ncondor   13559  0.0  0.0  90276  6892 ?        Ss   Jun12   0:00 condor_starter -f -a slot1_9 login02.hep.wisc.edu\nslot5    13614  1.6  0.0      0     0 ?        Zs   Jun12  15:36 [condor_exec.exe] &lt;defunct&gt;\nslot9    13615  1.6  0.0      0     0 ?        Zs   Jun12  15:36 [condor_exec.exe] &lt;defunct&gt;\ncondor   17482  0.0  0.0  90276  6860 ?        Ss   Jun12   0:00 condor_starter -f -a slot1_10 submit-3.chtc.wisc.edu\n20652    17488  0.0  0.0      0     0 ?        Zs   Jun12   0:00 [condor_exec.exe] &lt;defunct&gt;\n</pre></div>\n\n\n<p>I didn't see anything in dmesg that looked related.\n\n</p><p>The end of the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=MasterLog\" title=\"Master Log\">MasterLog</a></span> on e075.chtc.wisc.edu looked like:\n</p><div class=\"code\">\n<pre class=\"code\">06/12/14 14:40:07 ******************************************************\n06/12/14 14:40:07 ** condor_master (CONDOR_MASTER) STARTING UP\n06/12/14 14:40:07 ** /usr/sbin/condor_master\n06/12/14 14:40:07 ** SubsystemInfo: name=MASTER type=MASTER(2) class=DAEMON(1)\n06/12/14 14:40:07 ** Configuration: subsystem:MASTER local:&lt;NONE&gt; class:DAEMON\n06/12/14 14:40:07 ** $CondorVersion: 8.2.0 May 22 2014 BuildID: 249330 PRE-RELEASE-UWCS $\n06/12/14 14:40:07 ** $CondorPlatform: x86_64_RedHat6 $\n06/12/14 14:40:07 ** PID = 6709\n06/12/14 14:40:07 ** Log last touched 6/11 19:18:22\n06/12/14 14:40:07 ******************************************************\n06/12/14 14:40:07 Using config source: /etc/condor/condor_config\n06/12/14 14:40:07 Using local config sources:\n06/12/14 14:40:07    /etc/condor/config.d/00chtc-global.conf\n06/12/14 14:40:07    /etc/condor/config.d/01chtc-exec.conf\n06/12/14 14:40:07    /etc/condor/config.d/02chtc-flocking.conf\n06/12/14 14:40:07    /etc/condor/config.d/03chtc-slot_dirs.conf\n06/12/14 14:40:07    /etc/condor/config.d/04chtc-slots.conf\n06/12/14 14:40:07    /etc/condor/config.d/05chtc-slot_users.conf\n06/12/14 14:40:07    /etc/condor/config.d/06chtc-blacklist.conf\n06/12/14 14:40:07    /etc/condor/config.d/07chtc-startdcron.conf\n06/12/14 14:40:07    /etc/condor/config.d/08chtc-kvm.conf\n06/12/14 14:40:07    /etc/condor/config.d/08chtc-vmgahp.conf\n06/12/14 14:40:07    /etc/condor/config.d/09chtc-ckpt.conf\n06/12/14 14:40:07    /etc/condor/config.d/11chtc-isdedicated.conf\n06/12/14 14:40:07    /etc/condor/config.d/12chtc-cgroups.conf\n06/12/14 14:40:07    /etc/condor/config.d/13chtc-run-as-owner.conf\n06/12/14 14:40:07    /etc/condor/config.d/rhel6-exec.conf\n06/12/14 14:40:07    /etc/condor/condor_config.local\n06/12/14 14:40:07 config Macros = 470, Sorted = 470, StringBytes = 24840, TablesBytes = 17088\n06/12/14 14:40:07 CLASSAD_CACHING is OFF\n06/12/14 14:40:07 Daemon Log is logging: D_ALWAYS D_ERROR\n06/12/14 14:40:07 DaemonCore: command socket at &lt;128.104.58.85:52819&gt;\n06/12/14 14:40:07 DaemonCore: private command socket at &lt;128.104.58.85:52819&gt;\n06/12/14 14:40:07 Master restart (GRACEFUL) is watching /usr/sbin/condor_master (mtime:1400797216)\n06/12/14 14:40:07 Started DaemonCore process \"/usr/sbin/condor_startd\", pid and pgroup = 6714\n06/12/14 14:40:07 Started process \"/usr/sbin/condor_ckpt_server\", pid and pgroup = 6715\n06/12/14 15:40:07 Preen pid is 3274\n06/12/14 15:40:07 DefaultReaper unexpectedly called on pid 3274, status 0.\n06/12/14 20:52:27 ERROR: Child pid 6714 appears hung! Killing it hard.\n06/12/14 20:52:27 DefaultReaper unexpectedly called on pid 6714, status 6.\n06/12/14 20:52:27 The STARTD (pid 6714) was killed because it was no longer responding\n</pre></div>\n\n\n<p>End of the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=StartLog\" title=\"Start Log\">StartLog</a></span> on e075.chtc.wisc.edu looked like:\n</p><div class=\"code\">\n<pre class=\"code\">06/12/14 19:56:04 (D_ALWAYS) Starter pid 17458 exited with status 0\n06/12/14 19:56:04 (D_ALWAYS) slot1_18: State change: starter exited\n06/12/14 19:56:04 (D_ALWAYS) slot1_18: Changing activity: Busy -&gt; Idle\n06/12/14 19:56:04 (D_ALWAYS) slot1_19: Called deactivate_claim_forcibly()\n06/12/14 19:56:04 (D_ALWAYS) slot1_18: Got activate_claim request from shadow (128.104.100.44)\n06/12/14 19:56:04 (D_ALWAYS) slot1_18: Remote job ID is 4013936.0\n06/12/14 19:56:04 (D_ALWAYS) slot1_18: Setting affinity env to 17\nStack dump for process 6714 at timestamp 1402624347 (24 frames)\n/usr/lib64/condor/libcondor_utils_8_2_0.so(dprintf_dump_stack+0x12d)[0x7fd933dd5f4d]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_Z18linux_sig_coredumpi+0x40)[0x7fd933ebecb0]\n/lib64/libpthread.so.0[0x355820f500]\n/lib64/libpthread.so.0(read+0x10)[0x355820e530]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_condor_full_read+0x2e)[0x7fd933dd140e]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN10DaemonCore14Create_ProcessEPKcRK7ArgList10priv_stateiiPK3EnvS1_P10FamilyInfoPP6StreamPiSE_iP10__sigset_tiPmSE_S1_P8MyStringP15FilesystemRemapl+0x1b68)[0x7fd933eb6bd8]\ncondor_startd(_ZN7Starter13execDCStarterERK7ArgListPK3EnvPiP6Stream+0x497)[0x450f47]\ncondor_startd(_ZN7Starter13execDCStarterEP6Stream+0x125)[0x4513c5]\ncondor_startd(_ZN7Starter5spawnElP6Stream+0x83)[0x451913]\ncondor_startd(_ZN5Claim12spawnStarterEP6Stream+0x39)[0x455699]\ncondor_startd(_Z14activate_claimP8ResourceP6Stream+0x76e)[0x441b5e]\ncondor_startd(_Z22command_activate_claimP7ServiceiP6Stream+0x1bb)[0x441e9b]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN10DaemonCore18CallCommandHandlerEiP6Streambbff+0x389)[0x7fd933eb0919]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN21DaemonCommandProtocol11ExecCommandEv+0x687)[0x7fd933e93637]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN21DaemonCommandProtocol10doProtocolEv+0xf8)[0x7fd933e93d88]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN10DaemonCore9HandleReqEP6StreamS1_+0x72)[0x7fd933ea8d02]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN10DaemonCore24CallSocketHandler_workerEibP6Stream+0x705)[0x7fd933eb1805]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN10DaemonCore35CallSocketHandler_worker_demarshallEPv+0x1d)[0x7fd933eb183d]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN13CondorThreads8pool_addEPFvPvES0_PiPKc+0x40)[0x7fd933d67230]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN10DaemonCore17CallSocketHandlerERib+0x147)[0x7fd933ea9427]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN10DaemonCore6DriverEv+0x36c2)[0x7fd933ead022]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_Z7dc_mainiPPc+0x1799)[0x7fd933ec0e39]\n/lib64/libc.so.6(__libc_start_main+0xfd)[0x3557e1ecdd]\ncondor_startd[0x421a79]\n</pre></div>\n\n\n<p>Infrastructure is working now to disable HTCondor CGROUP usage pool-wide until this issue can be resolved.</p></blockquote>", "remarks": "<blockquote>\n<em>2014-Jun-13 15:37:42 by tannenba:</em> <br/>\n\nLooks like both cgroup and cpu affinity were enabled, i.e the nodes have\n\n<p></p><pre>   ENFORCE_CPU_AFFINITY = True\n   SLOT1_CPU_AFFINITY = 0,1,2,3,4,5,6,7,8\n   SLOT2_CPU_AFFINITY = 0\n   SLOT3_CPU_AFFINITY = 1\n   SLOT4_CPU_AFFINITY = 2\n   ...\n</pre>\n\n<p>It seems like a weird config to have both cpu affinity and cgroup support enabled.... no need for cpu affinity if cgroups is on.\n\n</p><p>Perhaps these two options are incompatible?  If so, HTCondor should definitely ignore all cpu affinity configs if cgroups is in effect. I could envision the kernel getting a bit confused over the conflicting instructions...\n\n</p><p></p><hr/>\n<em>2014-Jun-13 15:38:12 by tannenba:</em> <br/>\n\nFor the record, the nodes having the issues were running Scientific Linux 6.3.\n\n<p></p><hr/>\n<em>2014-Jun-17 10:14:31 by gthain:</em> <br/>\n\nI can't reproduce this at all on my Fedora laptop, so I'm thinking this may be a system issue.\n\n<p>There are core dumps from the starter and startd on e099.  The starter core dump seems unrelated to the current problem, but the startd stack trace looks like:\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">Core was generated by `condor_startd -f'.\n#0  0x00007f5f412da018 in WriteCoreDump () from /usr/lib64/condor/libcondor_utils_8_2_0.so\nMissing separate debuginfos, use: debuginfo-install condor-8.2.0-249330.x86_64\n(gdb) where\n#0  0x00007f5f412da018 in WriteCoreDump () from /usr/lib64/condor/libcondor_utils_8_2_0.so\n#1  0x00007f5f412bed13 in linux_sig_coredump(int) () from /usr/lib64/condor/libcondor_utils_8_2_0.so\n#2  &lt;signal handler called&gt;\n#3  0x0000003a888e0ce3 in __select_nocancel () from /lib64/libc.so.6\n#4  0x00007f5f411c2b06 in Selector::execute() () from /usr/lib64/condor/libcondor_utils_8_2_0.so\n#5  0x00007f5f412cbb5d in NamedPipeReader::read_data(void*, int) ()\n   from /usr/lib64/condor/libcondor_utils_8_2_0.so\n#6  0x00007f5f412c92b9 in ProcFamilyClient::get_usage(int, ProcFamilyUsage&amp;, bool&amp;) ()\n   from /usr/lib64/condor/libcondor_utils_8_2_0.so\n#7  0x00007f5f411def0a in ProcFamilyProxy::get_usage(int, ProcFamilyUsage&amp;, bool) ()\n   from /usr/lib64/condor/libcondor_utils_8_2_0.so\n#8  0x00007f5f4129b6a9 in DaemonCore::Get_Family_Usage(int, ProcFamilyUsage&amp;, bool) ()\n   from /usr/lib64/condor/libcondor_utils_8_2_0.so\n#9  0x000000000044f694 in Starter::percentCpuUsage() ()\n#10 0x00000000004241d9 in Resource::compute_condor_load() ()\n#11 0x0000000000436ca8 in CpuAttributes::compute(int) ()\n#12 0x0000000000426169 in Resource::compute(int) ()\n#13 0x000000000042dd57 in ResMgr::walk(void (Resource::*)(int), int) ()\n#14 0x0000000000430c1e in ResMgr::compute(int) ()\n#15 0x0000000000430e14 in ResMgr::eval_all() ()\n#16 0x00007f5f4129a6f1 in TimerManager::Timeout(int*, double*) () from /usr/lib64/condor/libcondor_utils_8_2_0.so\n#17 0x00007f5f412aa557 in DaemonCore::Driver() () from /usr/lib64/condor/libcondor_utils_8_2_0.so\n#18 0x00007f5f412c0e39 in dc_main(int, char**) () from /usr/lib64/condor/libcondor_utils_8_2_0.so\n#19 0x0000003a8881ecdd in __libc_start_main () from /lib64/libc.so.6\n#20 0x0000000000421a79 in _start ()\n</pre></div>\n\n\n<p>Note that this is because the Master is killing the startd:\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">06/13/14 16:57:35 DefaultReaper unexpectedly called on pid 8605, status 0.\n06/14/14 08:48:25 ERROR: Child pid 8158 appears hung! Killing it hard.\n06/14/14 08:48:33 DefaultReaper unexpectedly called on pid 8158, status 6.\n06/14/14 08:48:33 The STARTD (pid 8158) was killed because it was no longer responding\n06/14/14 08:48:34 Sending obituary for \"/usr/sbin/condor_startd\"\n06/14/14 08:48:36 restarting /usr/sbin/condor_startd in 10 seconds\n06/14/14 08:48:46 Started DaemonCore process \"/usr/sbin/condor_startd\", pid and pgroup = 25475\n06/14/14 16:57:33 Preen pid is 8379\n\n</pre></div>\n\n\n<p></p><hr/>\n<em>2014-Jun-17 10:22:27 by gthain:</em> <br/>\n\nAnd the startd is hanging after starting a VM Universe job:\n\n<p></p><div class=\"code\">\n<pre class=\"code\">06/14/14 06:38:58 (D_ALWAYS) slot1_12: Request accepted.\n06/14/14 06:38:58 (D_ALWAYS) slot1_12: Remote owner is osgtest@osghost.chtc.wisc.edu\n06/14/14 06:38:58 (D_ALWAYS) slot1_12: State change: claiming protocol successful\n06/14/14 06:38:58 (D_ALWAYS) slot1_12: Changing state: Owner -&gt; Claimed\n06/14/14 06:38:58 (D_ALWAYS) slot1_12: Got activate_claim request from shadow (128.105.244.224)\n06/14/14 06:38:58 (D_ALWAYS) slot1_12: Remote job ID is 91075.0\n06/14/14 06:38:58 (D_ALWAYS) slot1_12: Setting affinity env to 11\n06/14/14 06:38:58 (D_ALWAYS) slot1_12: Got universe \"VM\" (13) from request classad\n06/14/14 06:38:58 (D_ALWAYS) slot1_12: State change: claim-activation protocol successful\n06/14/14 06:38:58 (D_ALWAYS) slot1_12: Changing activity: Idle -&gt; Busy\n06/14/14 06:40:57 (D_ALWAYS) DaemonCommandProtocol: deadline for security handshake with &lt;128.105.244.14:54327&gt; ha\ns expired.\n06/14/14 06:59:27 (D_ALWAYS) Unable to calculate keyboard/mouse idle time due to them both being USB or not presen\nt, assuming infinite idle time for these devices.\nStack dump for process 8158 at timestamp 1402753705 (21 frames)\n/usr/lib64/condor/libcondor_utils_8_2_0.so(dprintf_dump_stack+0x12d)[0x7f5f411d5f4d]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_Z18linux_sig_coredumpi+0x40)[0x7f5f412becb0]\n/lib64/libpthread.so.0[0x3a88c0f500]\n/lib64/libc.so.6(__select+0x13)[0x3a888e0ce3]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN8Selector7executeEv+0xa6)[0x7f5f411c2b06]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN15NamedPipeReader9read_dataEPvi+0x7d)[0x7f5f412cbb5d]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN16ProcFamilyClient9get_usageEiR15ProcFamilyUsageRb+0x99)[0x7f5f412c9\n2b9]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN15ProcFamilyProxy9get_usageEiR15ProcFamilyUsageb+0x4a)[0x7f5f411def0\na]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN10DaemonCore16Get_Family_UsageEiR15ProcFamilyUsageb+0x19)[0x7f5f4129\nb6a9]\ncondor_startd(_ZN7Starter15percentCpuUsageEv+0x24)[0x44f694]\ncondor_startd(_ZN8Resource19compute_condor_loadEv+0xf9)[0x4241d9]\ncondor_startd(_ZN13CpuAttributes7computeEi+0xa8)[0x436ca8]\ncondor_startd(_ZN8Resource7computeEi+0x59)[0x426169]\ncondor_startd(_ZN6ResMgr4walkEM8ResourceFviEi+0x67)[0x42dd57]\ncondor_startd(_ZN6ResMgr7computeEi+0x7e)[0x430c1e]\ncondor_startd(_ZN6ResMgr8eval_allEv+0x34)[0x430e14]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN12TimerManager7TimeoutEPiPd+0x1a1)[0x7f5f4129a6f1]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_ZN10DaemonCore6DriverEv+0xbf7)[0x7f5f412aa557]\n/usr/lib64/condor/libcondor_utils_8_2_0.so(_Z7dc_mainiPPc+0x1799)[0x7f5f412c0e39]\n/lib64/libc.so.6(__libc_start_main+0xfd)[0x3a8881ecdd]\ncondor_startd[0x421a79]\n06/14/14 08:48:47 (D_ALWAYS) ******************************************************\n06/14/14 08:48:47 (D_ALWAYS) ** condor_startd (CONDOR_STARTD) STARTING UP\n\n</pre></div>\n\n\n<p></p><hr/>\n<em>2014-Jun-17 12:58:05 by tannenba:</em> <br/>\n\nSo we are running SL6.3; note that there have been many kernel bugs related to cgroups fixed in SL6.5.  For example -\n\n<p><strong>BZ#1081908</strong> The kernel task scheduler could trigger a race condition while migrating tasks over CPU cgroups. The race could result in accessing a task that pointed to an incorrect parent task group, causing the system to behave unpredictably, for example to appear being unresponsive. This problem has been resolved by ensuring that the correct task group information is properly stored during the task's migration.\n\n</p><p><strong>BZ#1054072</strong> Due to the locking mechanism that the kernel used while handling Out of Memory (OOM) situations in memory control groups (cgroups), the OOM killer did not work as intended in case that many processes triggered an OOM. As a consequence, the entire system could become or appear to be unresponsive. A series of patches has been applied to improve this locking mechanism so that the OOM killer now works as expected in memory cgroups under heavy OOM load.\n\n</p><p><strong>BZ#811255</strong> The Out of Memory (OOM) killer killed processes outside of a memory cgroup when one or more processes inside that memory cgroup exceeded the \"memory.limit_in_bytes\" value. This was because when a copy-on-write fault happened on a Transparent Huge Page (THP), the 2 MB THP caused the cgroup to exceed the memory.limit_in_bytes value but the individual 4 KB page was not exceeded. With this update, the 2 MB THP is correctly split into 4 KB pages when the memory.limit_in_bytes value is exceeded. The OOM kill is delivered within the memory cgroup; tasks outside the memory cgroups are no longer killed by the OOM killer.\n\n</p><p></p><hr/>\n<em>2014-Jul-28 14:31:40 by tim:</em> <br/>\n\nClosing this due to the fact that an older kernel is most likely to blame.</blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "incident", "last_change": "2014-Jul-28 14:31", "status": "abandoned", "created": "2014-Jun-13 11:30", "fixed_version": "2014-Jun-13 11:30", "broken_version": "v080200", "priority": "2", "subsystem": "DaemonsExecNode", "assigned_to": "gthain", "derived_from": "", "creator": "moate", "rust": "", "customer_group": "chtc", "visibility": "public", "notify": "nyehle@cs.wisc.edu, bgietzel@cs.wisc.edu, van-lyse@cs.wisc.edu, tannenba@cs.wisc.edu, bbockelm@cse.unl.edu, moate@cs.wisc.edu", "due_date": ""}
{"id": 5119, "title": "Ticket #5119: Intermittent proxy rename errors", "description": "<blockquote>\nOne of our customers has seen issues with their HTCondor CE once every few days from crondor submissions (when the crondor service is restarted, the issue disappears). We're seeing interesting failures when trying to move the job's proxy from a temp file in the spool directory to another:\n\n<p></p><div class=\"verbatim\">\n<pre>05/21/15 16:48:09 rename(/fdata/spool/2055/0/cluster22055.proc0.subproc0.tmp/rsvproxy, /fdata/spool/2055/0/cluster22055.proc0.subproc0/rsvproxy) failed with errno 2\n05/21/15 16:48:09 ERROR \"FileTransfer CommitFiles Failed -- What Now?!?!\" at line 2564 in file /builddir/build/BUILD/condor-8.2.8/src/condor_utils/file_transfer.cpp\n05/21/15 16:48:10 Transaction::Commit(): fflush_with_status() took 595 seconds to run\n</pre></div>\n</blockquote>", "remarks": "<blockquote>\n<em>2015-Jun-23 17:10:18 by blin:</em> <br/>\n\nI can provide the full <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=SchedLog\" title=\"Sched Log\">SchedLog</a></span> where we saw this error but it's too big to attach. Trey could also provide a more recent <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=SchedLog\" title=\"Sched Log\">SchedLog</a></span> whenever he sees this happen again.\n\n<p></p><hr/>\n<em>2015-Sep-23 15:01:15 by zmiller:</em> <br/>\n\nHere's my best guess:\n\n<p></p><div class=\"verbatim\">\n<pre>...\n05/21/15 16:38:13 get_file(): going to write to filename /fdata/spool/2055/0/cluster22055.proc0.subproc0.tmp/org.osg.gratia.metric\n05/21/15 16:38:13 get_file: Receiving 26200 bytes\n05/21/15 16:38:13 get_file: wrote 26200 bytes to file\n05/21/15 16:38:13 ReliSock::get_file_with_permissions(): going to set permissions 755\n05/21/15 16:38:13 Initializing Directory: curr_dir = /fdata/spool/2055/0/cluster22055.proc0.subproc0.tmp\n05/21/15 16:38:14 OwnerCheck retval 1 (success),no ad\n05/21/15 16:38:14 schedd: NewCluster rval 22059 errno 0\n05/21/15 16:38:14 OwnerCheck retval 1 (success),no ad\n05/21/15 16:38:14 schedd: NewProc rval 0 errno 0\n05/21/15 16:48:09 rename(/fdata/spool/2055/0/cluster22055.proc0.subproc0.tmp/rsvproxy, /fdata/spool/2055/0/cluster22055.proc0.subproc0/rsvproxy) failed with errno 2\n05/21/15 16:48:09 ERROR \"FileTransfer CommitFiles Failed -- What Now?!?!\" at line 2564 in file /builddir/build/BUILD/condor-8.2.8/src/condor_utils/file_transfer.cpp\n05/21/15 16:48:10 Transaction::Commit(): fflush_with_status() took 595 seconds to run\n...\n</pre></div>\n\n\n<p>Something blocked for 10 entire minutes while trying to do a rename() and ultimately it failed with \"No such file or directory\".\n\n</p><p>Possbily the mount /fdata is some kind of shared filesystem that had a hiccup?  There's no other useful debug messages in there so unless I can get a more verbose log (and something actual DID happen) that's as much as I have to go on for now.\n\n</p><p></p><div class=\"verbatim\">\n<pre>Also, Brian Lin said:\n&gt; Last I spoke with Trey, this\n&gt; issue hasn't popped up in a while so I'm not sure how easy it will be to\n&gt; provide more recent or verbose logs.\n&gt;\n&gt; - Brian\n</pre></div>\n\n\n<p></p><hr/>\n<em>2015-Sep-23 15:42:00 by treydock:</em> <br/>\n\nCorrect, /fdata is a shared filesystem running FhGFS (BeeGFS).  It does occasionally have periods where operations will hang if metadata or storage nodes are overloaded.  A rename likely would be impacted by the metadata system.  My monitoring for \"busyness\" shows some load on metadata around the timestamps in the logs, but nothing that I would expect to cause a 10 minute hang.  However there have been times when metadata system will become unstable and usually the first host to notice this is our CE as SPOOL is on /fdata.\n\n<p></p><hr/>\n<em>2015-Sep-28 14:54:14 by blin:</em> <br/>\n\nFrom Trey:\n\n<p></p><div class=\"blockquote\">\n<table width=\"95%\"><tbody><tr><td>\u00a0</td><td>\n  \nWe're actually on 8.2.9, but a version with some patches from BrianB to allow our odd way of supporting GlideIns.  So far we've seen fewer issues.  Only issue at this time is occasional corruption of job history and sometimes job queue stored in SPOOL.  Seems to occur when our shared filesystem has issues which causes IO to simply hang.  May move to /home which is a simpler setup (NFS) and has fewer issues historically.\n</td></tr></tbody></table></div>\n\n\n<p>So I think this ticket can be closed as appropriate.</p></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "incident", "last_change": "2015-Oct-01 11:16", "status": "resolved", "created": "2015-Jun-23 17:06", "fixed_version": "2015-Jun-23 17:06", "broken_version": "v080208", "priority": "2", "subsystem": "DaemonsSubmitNode", "assigned_to": "blin", "derived_from": "#4555", "creator": "blin", "rust": "", "customer_group": "osg", "visibility": "public", "notify": "blin@cs.wisc.edu treydock@tamu.edu", "due_date": ""}
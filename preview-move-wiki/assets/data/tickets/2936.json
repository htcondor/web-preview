{"id": 2936, "title": "Ticket #2936: CLAIM_PARTITIONABLE_LEFTOVERS=True segfaults", "description": "<blockquote>\nWe enabled a schedd that understands leftover claims and immediately started to segfault startd's left and right.\n\n<p>The stack dump looks like this:\n\n</p><p></p><div class=\"verbatim\">\n<pre>\ncondor_startd(_ZN5Claim2idEv+0xb)[0x44102b]\ncondor_startd(_Z20accept_request_claimP8ResourceP5Claim+0x9d)[0x4356cd]\ncondor_startd(_Z13request_claimP8ResourceP5ClaimPcP6Stream+0xbeb)[0x43a35b]\ncondor_startd(_Z21command_request_claimP7ServiceiP6Stream+0x122)[0x43a9e2]\n/usr/lib64/libcondor_utils_7_9_0.so(_ZN10DaemonCore18CallCommandHandlerEiP6Streambbff+0x17e)[0x7f1268332a1e]\n/usr/lib64/libcondor_utils_7_9_0.so(_ZN21DaemonCommandProtocol11ExecCommandEv+0x50f)[0x7f12683386af]\n/usr/lib64/libcondor_utils_7_9_0.so(_ZN21DaemonCommandProtocol10doProtocolEv+0xe8)[0x7f1268339d28]\n/usr/lib64/libcondor_utils_7_9_0.so(_ZN21DaemonCommandProtocol14SocketCallbackEP6Stream+0x80)[0x7f1268339e40]\n/usr/lib64/libcondor_utils_7_9_0.so(_ZN10DaemonCore24CallSocketHandler_workerEibP6Stream+0x665)[0x7f1268333b95]\n/usr/lib64/libcondor_utils_7_9_0.so(_ZN10DaemonCore35CallSocketHandler_worker_demarshallEPv+0x1d)[0x7f1268333c6d]\n/usr/lib64/libcondor_utils_7_9_0.so(_ZN13CondorThreads8pool_addEPFvPvES0_PiPKc+0x40)[0x7f1268261670]\n/usr/lib64/libcondor_utils_7_9_0.so(_ZN10DaemonCore17CallSocketHandlerERib+0x147)[0x7f1268328e07]\n/usr/lib64/libcondor_utils_7_9_0.so(_ZN10DaemonCore6DriverEv+0x2828)[0x7f126832bd08]\n/usr/lib64/libcondor_utils_7_9_0.so(_Z7dc_mainiPPc+0x1312)[0x7f126833e4d2]\n</pre></div>\n\n\n<p>I was able to get this behavior to go away by setting:\n\n</p><p>CLAIM_PARTITIONABLE_LEFTOVERS=False\n\n</p><p>I can't get this version of Condor to dump a core, otherwise I'd be more useful.  However, by the stack trace, it appears that the leftover_claim object is improperly initialized in request_claim - no other explanation as to why leftover_claim-&gt;id() would segfault.</p></blockquote>", "remarks": "<blockquote>\n<em>2012-Apr-11 10:49:23 by matt:</em> <br/>\n\n<div class=\"verbatim\">\n<pre>\ncondor_startd(Claim::id()+0xb)[0x44102b]\ncondor_startd(accept_request_claim(Resource*, Claim*)+0x9d)[0x4356cd]\ncondor_startd(request_claim(Resource*, Claim*, char*, Stream*)+0xbeb)[0x43a35b]\ncondor_startd(command_request_claim(Service*, int, Stream*)+0x122)[0x43a9e2]\n/usr/lib64/libcondor_utils_7_9_0.so(DaemonCore::CallCommandHandler(int, Stream*, bool, bool, float, float)+0x17e)[0x7f1268332a1e]\n/usr/lib64/libcondor_utils_7_9_0.so(DaemonCommandProtocol::ExecCommand()+0x50f)[0x7f12683386af]\n/usr/lib64/libcondor_utils_7_9_0.so(DaemonCommandProtocol::doProtocol()+0xe8)[0x7f1268339d28]\n/usr/lib64/libcondor_utils_7_9_0.so(DaemonCommandProtocol::SocketCallback(Stream*)+0x80)[0x7f1268339e40]\n/usr/lib64/libcondor_utils_7_9_0.so(DaemonCore::CallSocketHandler_worker(int, bool, Stream*)+0x665)[0x7f1268333b95]\n/usr/lib64/libcondor_utils_7_9_0.so(DaemonCore::CallSocketHandler_worker_demarshall(void*)+0x1d)[0x7f1268333c6d]\n/usr/lib64/libcondor_utils_7_9_0.so(CondorThreads::pool_add(void (*)(void*), void*, int*, char const*)+0x40)[0x7f1268261670]\n/usr/lib64/libcondor_utils_7_9_0.so(DaemonCore::CallSocketHandler(int&amp;, bool)+0x147)[0x7f1268328e07]\n/usr/lib64/libcondor_utils_7_9_0.so(DaemonCore::Driver()+0x2828)[0x7f126832bd08]\n/usr/lib64/libcondor_utils_7_9_0.so(dc_main(int, char**)+0x1312)[0x7f126833e4d2]\n</pre></div>\n\n\n<p></p><hr/>\n<em>2012-Apr-11 11:01:29 by bbockelm:</em> <br/>\n\nReviewing the Claim::Id code, it looks like it would be very difficult to make it segfault by itself.  Thus, it's possible there's other heap corruption issues going on elsewhere in the code.  Maybe valgrind would reveal the culprit.\n\n<p>After 12 hours of CLAIM_PARTITIONABLE_LEFTOVERS=False, we've been running fairly stably.  So, I think the issue is related to these code paths.\n\n</p><p></p><hr/>\n<em>2012-May-03 14:37:27 by bbockelm:</em> <br/>\n\nMore info\n\n<p></p><div class=\"verbatim\">\n<pre>\n(gdb) bt\n#0  id (this=0x108a590) at /usr/src/debug/condor-7.9.0/src/condor_startd.V6/claim.h:66\n#1  Claim::id (this=0x108a590) at /usr/src/debug/condor-7.9.0/src/condor_startd.V6/claim.cpp:1291\n#2  0x000000000043a50d in accept_request_claim (rip=0x108a2d0, leftover_claim=0x108a590) at /usr/src/debug/condor-7.9.0/src/condor_startd.V6/command.cpp:1601\n#3  0x000000000043f2bb in request_claim (rip=0x108a2d0, claim=&lt;value optimized out&gt;,\n    id=0x107cd50 \"&lt;172.16.1.114:36383&gt;#1336073429#1#[Integrity=\\\"YES\\\";Encryption=\\\"NO\\\";CryptoMethods=\\\"3DES\\\";CurrentTime=time();]11e7ba0663248b81ebefe4d4a03f2a746d0fbc4d\", stream=0x10862c0)\n    at /usr/src/debug/condor-7.9.0/src/condor_startd.V6/command.cpp:1541\n#4  0x000000000043f992 in command_request_claim (cmd=&lt;value optimized out&gt;, stream=0x10862c0) at /usr/src/debug/condor-7.9.0/src/condor_startd.V6/command.cpp:342\n#5  0x00007ff8cbfee7be in DaemonCore::CallCommandHandler (this=0xfdfd80, req=442, stream=0x10862c0, delete_stream=false, check_payload=&lt;value optimized out&gt;, time_spent_on_sec=0.00706911087, time_spent_waiting_for_payload=0)\n    at /usr/src/debug/condor-7.9.0/src/condor_daemon_core.V6/daemon_core.cpp:4018\n#6  0x00007ff8cbff449f in DaemonCommandProtocol::ExecCommand (this=0x1072550) at /usr/src/debug/condor-7.9.0/src/condor_daemon_core.V6/daemon_command.cpp:1442\n#7  0x00007ff8cbff5b78 in DaemonCommandProtocol::doProtocol (this=0x1072550) at /usr/src/debug/condor-7.9.0/src/condor_daemon_core.V6/daemon_command.cpp:153\n#8  0x00007ff8cbff5c90 in DaemonCommandProtocol::SocketCallback (this=0x1072550, stream=0x10862c0) at /usr/src/debug/condor-7.9.0/src/condor_daemon_core.V6/daemon_command.cpp:213\n#9  0x00007ff8cbfef935 in DaemonCore::CallSocketHandler_worker (this=0xfdfd80, i=3, default_to_HandleCommand=&lt;value optimized out&gt;, asock=&lt;value optimized out&gt;)\n    at /usr/src/debug/condor-7.9.0/src/condor_daemon_core.V6/daemon_core.cpp:3806\n#10 0x00007ff8cbfefa0d in DaemonCore::CallSocketHandler_worker_demarshall (arg=0x1050ec0) at /usr/src/debug/condor-7.9.0/src/condor_daemon_core.V6/daemon_core.cpp:3768\n#11 0x00007ff8cbf1d290 in CondorThreads::pool_add (routine=&lt;value optimized out&gt;, arg=&lt;value optimized out&gt;, tid=&lt;value optimized out&gt;, descrip=&lt;value optimized out&gt;)\n    at /usr/src/debug/condor-7.9.0/src/condor_utils/condor_threads.cpp:1109\n#12 0x00007ff8cbfe4ba7 in DaemonCore::CallSocketHandler (this=0xfdfd80, i=@0x7fff8346838c, default_to_HandleCommand=true) at /usr/src/debug/condor-7.9.0/src/condor_daemon_core.V6/daemon_core.cpp:3756\n#13 0x00007ff8cbfe7aa8 in DaemonCore::Driver (this=0xfdfd80) at /usr/src/debug/condor-7.9.0/src/condor_daemon_core.V6/daemon_core.cpp:3629\n#14 0x00007ff8cbffa352 in dc_main (argc=1, argv=0x7fff83468aa0) at /usr/src/debug/condor-7.9.0/src/condor_daemon_core.V6/daemon_core_main.cpp:2410\n#15 0x0000003f3201ecdd in ?? ()\n#16 0x0000000000000000 in ?? ()\n(gdb) up 2\n#2  0x000000000043a50d in accept_request_claim (rip=0x108a2d0, leftover_claim=0x108a590) at /usr/src/debug/condor-7.9.0/src/condor_startd.V6/command.cpp:1601\n1601\t\tif ( leftover_claim &amp;&amp; leftover_claim-&gt;id() &amp;&amp;\n(gdb) p *leftover_claim\n$1 = {&lt;Service&gt; = {_vptr.Service = 0x10bec20}, c_rip = 0x0, c_client = 0x10bd8b0, c_id = 0x940108a600, c_type = 148, c_ad = 0x0, c_starter = 0x0, c_rank = 0, c_oldrank = 0, c_universe = -256, c_proc = 0, c_cluster = 0,\n  c_global_job_id = 0x0, c_job_start = 0, c_last_pckpt = 0, c_claim_started = 0, c_entered_state = 1336073216, c_job_total_run_time = 0, c_job_total_suspend_time = 0, c_claim_total_run_time = 0,\n  c_claim_total_suspend_time = 0, c_activation_count = 17245312, c_request_stream = 0x2100000000, c_match_tid = 33, c_lease_tid = -1, c_sendalive_tid = 0, c_alive_inprogress_sock = 0x0, c_lease_duration = 0, c_aliveint = 0,\n  c_cod_keyword = 0x0, c_has_job_ad = 0, c_state = CLAIM_UNCLAIMED, c_last_state = CLAIM_UNCLAIMED, c_pending_cmd = 0, c_wants_remove = false, c_may_unretire = false, c_retire_peacefully = false, c_preempt_was_true = false,\n  c_badput_caused_by_draining = false, c_schedd_closed_claim = false, c_pledged_machine_max_vacate_time = 17302000}\n(gdb) p leftover_claim-&gt;c_id\n$2 = (ClaimId *) 0x940108a600\n(gdb) p *leftover_claim-&gt;c_id\nCannot access memory at address 0x940108a600\n</pre></div>\n\n\n<p>Core file incoming.\n\n</p><p></p><hr/>\n<em>2012-May-03 14:42:19 by bbockelm:</em> <br/>\n\nCore file was too big to attach.\n\n<p>Here's a link: <a class=\"external\" href=\"http://pages.cs.wisc.edu/~bbockelm/condor_startd_core\">http://pages.cs.wisc.edu/~bbockelm/condor_startd_core</a>\n\n</p><p>Here are the debug symbols: <a class=\"external\" href=\"http://koji.hep.caltech.edu/public/packages/condor/7.9.0/0.4.257bc70git.osg.el6/x86_64/condor-debuginfo-7.9.0-0.4.257bc70git.osg.el6.x86_64.rpm\">http://koji.hep.caltech.edu/public/packages/condor/7.9.0/0.4.257bc70git.osg.el6/x86_64/condor-debuginfo-7.9.0-0.4.257bc70git.osg.el6.x86_64.rpm</a>\n\n</p><p>Here is the build: <a class=\"external\" href=\"http://koji.hep.caltech.edu/koji/buildinfo?buildID=783\">http://koji.hep.caltech.edu/koji/buildinfo?buildID=783</a>\n\n</p><p></p><hr/>\n<em>2012-May-03 15:46:42 by bbockelm:</em> <br/>\n\nHere's the valgrind output:\n\n<p><a class=\"external\" href=\"http://pages.cs.wisc.edu/~bbockelm/startd_valgrund_2.gz\">http://pages.cs.wisc.edu/~bbockelm/startd_valgrund_2.gz</a>\n\n</p><p>Definitely trying to use a freed object:\n\n</p><p></p><div class=\"verbatim\">\n<pre>\n==1439== Invalid read of size 8\n==1439==    at 0x446150: Claim::id() (claim.cpp:1290)\n==1439==    by 0x43A50C: accept_request_claim(Resource*, Claim*) (command.cpp:1601)\n==1439==    by 0x43F2BA: request_claim(Resource*, Claim*, char*, Stream*) (command.cpp:1541)\n==1439==    by 0x43F991: command_request_claim(Service*, int, Stream*) (command.cpp:342)\n==1439==    by 0x4E637BD: DaemonCore::CallCommandHandler(int, Stream*, bool, bool, float, float) (daemon_core.cpp:4018)\n==1439==    by 0x4E6949E: DaemonCommandProtocol::ExecCommand() (daemon_command.cpp:1442)\n==1439==    by 0x4E6AB77: DaemonCommandProtocol::doProtocol() (daemon_command.cpp:153)\n==1439==    by 0x4E597A1: DaemonCore::HandleReq(Stream*, Stream*) (daemon_core.cpp:4176)\n==1439==    by 0x4E649D4: DaemonCore::CallSocketHandler_worker(int, bool, Stream*) (daemon_core.cpp:3820)\n==1439==    by 0x4E64A0C: DaemonCore::CallSocketHandler_worker_demarshall(void*) (daemon_core.cpp:3768)\n==1439==    by 0x4D9228F: CondorThreads::pool_add(void (*)(void*), void*, int*, char const*) (condor_threads.cpp:1109)\n==1439==    by 0x4E59BA6: DaemonCore::CallSocketHandler(int&amp;, bool) (daemon_core.cpp:3756)\n==1439==  Address 0x5fcefd8 is 24 bytes inside a block of size 232 free'd\n==1439==    at 0x4A0545F: operator delete(void*) (vg_replace_malloc.c:387)\n==1439==    by 0x439831: ResState::enter_action(State, Activity, bool, bool) (ResState.cpp:685)\n==1439==    by 0x439025: ResState::change(State, Activity) (ResState.cpp:148)\n==1439==    by 0x439F2E: ResState::eval() (ResState.cpp:545)\n==1439==    by 0x43F1DD: request_claim(Resource*, Claim*, char*, Stream*) (Resource.h:92)\n==1439==    by 0x43F991: command_request_claim(Service*, int, Stream*) (command.cpp:342)\n==1439==    by 0x4E637BD: DaemonCore::CallCommandHandler(int, Stream*, bool, bool, float, float) (daemon_core.cpp:4018)\n==1439==    by 0x4E6949E: DaemonCommandProtocol::ExecCommand() (daemon_command.cpp:1442)\n==1439==    by 0x4E6AB77: DaemonCommandProtocol::doProtocol() (daemon_command.cpp:153)\n==1439==    by 0x4E597A1: DaemonCore::HandleReq(Stream*, Stream*) (daemon_core.cpp:4176)\n==1439==    by 0x4E649D4: DaemonCore::CallSocketHandler_worker(int, bool, Stream*) (daemon_core.cpp:3820)\n==1439==    by 0x4E64A0C: DaemonCore::CallSocketHandler_worker_demarshall(void*) (daemon_core.cpp:3768)\n</pre></div>\n\n\n<p></p><hr/>\n<em>2012-May-04 14:01:10 by tannenba:</em> <br/>\n\nLooking things over, current theory is if the pslot is in owner state when the claim arrives, a seg fault happens as the transition from owner to unclaimed state invalidates the claim leftover's pointer to the pslot capability.  Gonna test this out...  patch looks like it may be very simple (fingers crossed)...\n\n<p></p><hr/>\n<em>2012-May-04 16:15:12 by tannenba:</em> <br/>\n\nIndeed, the crash can be reproduced by setting\n\n<p></p><pre>    START = Owner =?= \"tannenba\"\n</pre>\n\n<p>and then submitting a job as tannenba that matches to a pslot. The above START expression results in the pslot being in Owner state (because it evals to False until a job is matched), and the crash happens when claiming a pslot that\nis in Owner state.\n\n</p><p>Reproduced crash before patch, verified it no longer crashes after patch.\nPatch pushed into V7_8_0-branch.\n\n</p><p></p><hr/>\n<em>2012-May-06 08:03:32 by matt:</em> <br/>\n\nHead of master for me is,\n\n<p></p><div class=\"verbatim\">\n<pre>commit 1eb5b6ce7a95da7fb279acbc05bf82902e53ac13\nMerge: 3cea184 478f015\nAuthor: Erik Erlandson &lt;eerlands@redhat.com&gt;\nDate:   Wed May 2 10:41:32 2012 -0700\n</pre></div>\n\n\n<p>Config is,\n\n</p><p></p><div class=\"verbatim\">\n<pre>ALL_DEBUG = D_FULLDEBUG\nNUM_SLOTS = 1\nNUM_SLOTS_TYPE_1 = 1\nSLOT_TYPE_1_PARTITIONABLE = TRUE\nSTART = Owner =?= \"matt\"\n</pre></div>\n\n\n<p>Job is,\n\n</p><p></p><div class=\"verbatim\">\n<pre>echo 'cmd=/bin/sleep\\nargs=1d\\nqueue 2' | condor_submit\n</pre></div>\n\n\n<p>I am not seeing a crash.\n\n</p><p>I am seeing something I'm not familiar with in the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=SchedLog\" title=\"Sched Log\">SchedLog</a></span>,\n\n</p><p></p><div class=\"verbatim\">\n<pre>05/06/12 08:56:03 (pid:6908) Finished negotiating for matt in local pool: 1 matched, 1 rejected\n05/06/12 08:56:03 (pid:6908) Completed REQUEST_CLAIM to startd slot1@eeyore.local &lt;192.168.122.1:40428&gt;#13363089\n55#1#... for matt\n05/06/12 08:56:03 (pid:6908) Received match from startd, leftover slot ad slot1@eeyore.local claim &lt;192.168.122.\n1:40428&gt;#1336308955#4#...\n05/06/12 08:56:03 (pid:6908) Received match for job -1.-1: slot1@eeyore.local\n05/06/12 08:56:03 (pid:6908) GetAttributeInt() failed\n05/06/12 08:56:03 (pid:6908) Job -1.-1: not runnable\n05/06/12 08:56:03 (pid:6908) Reusing prioritized runnable job list because nothing has changed.\n05/06/12 08:56:03 (pid:6908) Job 1.0: is runnable\n05/06/12 08:56:03 (pid:6908) record for job -1.-1 skipped until PrioRec rebuild\n05/06/12 08:56:03 (pid:6908) Job 1.1: is runnable\n05/06/12 08:56:03 (pid:6908) Checking consistency running and runnable jobs\n05/06/12 08:56:03 (pid:6908) Tables are consistent\n05/06/12 08:56:03 (pid:6908) Rebuilt prioritized runnable job list in 0.000s.  (Expedited rebuild because no mat\nch was found)\n</pre></div>\n\n\n<p>What's with the -1.-1 job match?\n\n</p><p>What are the specific reproduction instructions?\n\n</p><p></p><hr/>\n<em>2012-May-07 08:48:59 by tannenba:</em> <br/>\n\nI have a couple ideas why you cannot reproduce the problem.\n\n<p>First off, you should be using code from V7_8_0-branch - who knows what else is going on with the master.\n\n</p><p>Second, re why you are not seeing a crash is because in startd ResState.cpp in enter_action(), there is code like:\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">\tcase owner_state:\n\t\t\t// Always want to create new claim objects\n\t\tif( rip-&gt;r_cur ) {\n\t\t\tdelete( rip-&gt;r_cur );\n\t\t}\n\t\trip-&gt;r_cur = new Claim( rip );\n</pre></div>\n\n\n<p>This is the code snippet valgrid complained about. My guess is when you are running on your system, the newly created r_cur pointer is identical to the old r_cur pointer that was just deleted, and thus the crash is avoided!  Perhaps this is one reason this bug was so sporadic. Would be nice if there was a way on debug builds to tell glibc to fill out deallocated memory regions with 0xDEADBEEF.\n\n</p><p>Assuming my guess is correct, repro instructions would be:\n\n</p><p>1. Patch as follows to help ensure a different region of memory is used (do not commit this patch)\n</p><div class=\"code\">\n<pre class=\"code\">diff --git a/src/condor_startd.V6/ResState.cpp b/src/condor_startd.V6/ResState.cindex 8969e70..ef8e557 100644\n--- a/src/condor_startd.V6/ResState.cpp\n+++ b/src/condor_startd.V6/ResState.cpp\n@@ -684,7 +684,15 @@ ResState::enter_action( State s, Activity a,\n                if( rip-&gt;r_cur ) {\n                        delete( rip-&gt;r_cur );\n                }\n+               {\n+                       Claim *qqq1 = new Claim( rip );\n+                       Claim *qqq2 = new Claim( rip );\n+                       Claim *qqq3 = new Claim( rip );\n                rip-&gt;r_cur = new Claim( rip );\n+                       delete qqq1;\n+                       delete qqq2;\n+                       delete qqq3;\n+               }\n                if( rip-&gt;r_pre ) {\n                        rip-&gt;remove_pre();\n                }\n</pre></div>\n\n\n<p>2. Config condor as follows, so you end up with one pslot in owner state\n</p><div class=\"code\">\n<pre class=\"code\">slot_type_1 = cpus=100%\nnum_slots_type_1 = 1\nslot_type_1_partitionable = true\nSTART=Owner=?=\"foo\"\n</pre></div>\n\n\n<p>3. Submit matching job as owner <em>foo</em>, or whatever user you used in the <code>START</code> expression in the step above. Startd should crash during claiming step.\n\n</p><p></p><hr/>\n<em>2012-May-07 11:35:13 by matt:</em> <br/>\n\nI've managed to repro on 0c43ecd (pre-patch). I didn't have SLOT_TYPE_1=cpus=100% in config. I think it's a bug that that is required, default should be cpus=auto and w/ one slot auto should be 100%.\n\n<p>This bug is <strong>NOT</strong> 100% reproducible. I'm getting about a 20% chance over 10 runs of submit + vacate_job + reschedule.\n\n</p><p></p><hr/>\n<em>2012-May-07 11:51:57 by matt:</em> <br/>\n\nFYI, I'm reproducing with,\n\n<p></p><div class=\"verbatim\">\n<pre>$ tail config.d/50mef.config\nALL_DEBUG = D_FULLDEBUG\nSLOT_TYPE_1 = cpus=100%\n#NUM_SLOTS = 1\nNUM_SLOTS_TYPE_1 = 1\nSLOT_TYPE_1_PARTITIONABLE = TRUE\nSTART = Owner =?= \"matt\"\n#START = TRUE\n\n$ echo 'cmd=/bin/sleep\\nargs=1d\\nqueue 2' | condor_submit\n\n$ count=0; while [ true ]; do echo \"run $count\"; condor_vacate_job 1 ; sleep 10 ; condor_reschedule ; sleep 10 ; condor_q; count=$((count+1)); done\n</pre></div>\n\n\n<p>Post-patch I'm up to 20 runs without a crash.\n\n</p><p></p><hr/>\n<em>2012-May-07 12:11:20 by matt:</em> <br/>\n\nI ran the repro instructions to count=75 for a 5bf288a build (post-patch). No crashes.\n\n<p></p><hr/>\n<em>2012-May-07 12:22:31 by matt:</em> <br/>\n\n<strong>CODE REVIEW</strong>\nThis looks good to me. I'm still uneasy about the fragility of the state machine, but that should be addressed outside this bug.</blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">2012-May-04 16:22</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"chngview?cn=31925\">[31925]</a></span>: Add item into version history for gt <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=2936\" onclick=\"get_ticket_and_populate_wrapper('2936'); return false;\" title=\"CLAIM_PARTITIONABLE_LEFTOVERS=True segfaults\">#2936</a></span>  (By Todd Tannenbaum )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2012-May-04 16:10</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"chngview?cn=31924\">[31924]</a></span>: Fix startd crash when claiming a pslot in owner state. <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=2936\" onclick=\"get_ticket_and_populate_wrapper('2936'); return false;\" title=\"CLAIM_PARTITIONABLE_LEFTOVERS=True segfaults\">#2936</a></span>  (By Todd Tannenbaum )</td></tr>\n</tbody></table>", "type": "defect", "last_change": "2012-May-07 13:24", "status": "resolved", "created": "2012-Apr-10 17:40", "fixed_version": "2012-Apr-10 17:40", "broken_version": "v070705", "priority": "2", "subsystem": "Daemons", "assigned_to": "tannenba", "derived_from": "#2790", "creator": "bbockelm", "rust": "", "customer_group": "cms", "visibility": "public", "notify": "bbockelm@cse.unl.edu, tstclair@redhat.com, tannenba@cs.wisc.edu, matt@cs.wisc.edu", "due_date": ""}
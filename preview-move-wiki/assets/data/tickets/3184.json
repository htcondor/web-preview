{"id": 3184, "title": "Ticket #3184: Condor collector deadlock", "description": "<blockquote>\nOur primary collector deadlocked last night and the master was unable to recover it.\n\n<p>When the collector forked, the child process hit the below deadlock:\n\n</p><p></p><div class=\"verbatim\">\n<pre>[root@red-condor ~]# pstack 8377\n#0  0x0000003bea2df01e in __lll_lock_wait_private () from /lib64/libc.so.6\n#1  0x0000003bea28be9d in _L_lock_1685 () from /lib64/libc.so.6\n#2  0x0000003bea28bbe7 in __tz_convert () from /lib64/libc.so.6\n#3  0x00002b00e633c1a4 in _condor_dprintf_va () from /usr/lib64/libcondor_utils_7_9_1.so\n#4  0x00002b00e634ccf8 in dprintf () from /usr/lib64/libcondor_utils_7_9_1.so\n#5  0x00002b00e63af55e in ForkWork::NewJob() () from /usr/lib64/libcondor_utils_7_9_1.so\n#6  0x0000000000420a42 in CollectorDaemon::receive_query_cedar(Service*, int, Stream*) ()\n#7  0x00002b00e64880d9 in DaemonCore::CallCommandHandler(int, Stream*, bool, bool, float, float) () from /usr/lib64/libcondor_utils_7_9_1.so\n#8  0x00002b00e64884e9 in DaemonCore::HandleReqPayloadReady(Stream*) () from /usr/lib64/libcondor_utils_7_9_1.so\n#9  0x00002b00e648918a in DaemonCore::CallSocketHandler_worker(int, bool, Stream*) () from /usr/lib64/libcondor_utils_7_9_1.so\n#10 0x00002b00e648961d in DaemonCore::CallSocketHandler_worker_demarshall(void*) () from /usr/lib64/libcondor_utils_7_9_1.so\n#11 0x00002b00e633fd18 in CondorThreads::pool_add(void (*)(void*), void*, int*, char const*) () from /usr/lib64/libcondor_utils_7_9_1.so\n#12 0x00002b00e6486225 in DaemonCore::CallSocketHandler(int&amp;, bool) () from /usr/lib64/libcondor_utils_7_9_1.so\n#13 0x00002b00e648bd3c in DaemonCore::Driver() () from /usr/lib64/libcondor_utils_7_9_1.so\n#14 0x00002b00e646e1d1 in dc_main(int, char**) () from /usr/lib64/libcondor_utils_7_9_1.so\n#15 0x0000003bea21d994 in __libc_start_main () from /lib64/libc.so.6\n#16 0x000000000040dd49 in _start ()\n</pre></div>\n\n\n<p>Not sure how this one happened.  Perhaps there was a rogue thread active in the parent when the fork occurred?  Globus can launch threads...\n\n</p><p>This caused the child to hang around forever.  Because it lasted for quite some time, the copy-on-write semantics caused the memory to grow quite quickly.  Eventually the OOM-killer selected the parent collector and shot it.\n\n</p><p>[Side note: the code to limit the number of children does not appear to be functioning, as this daemon had up to 7 children, but the maximum should have been set to two.]\n\n</p><p>However, the children still have a port open on 9618.  This means the master cannot start up a new collector, as its port is already taken.\n\n</p><p>Why didn't the procd kill off the children after the collector died?  Because, right before the OOM-killer took out the collector, it killed the procd.\n\n</p><p>The system remained in this state until admin intervention was taken.  It's about the third time in the last two weeks that this occurred.</p></blockquote>", "remarks": "<blockquote>\n<em>2012-Aug-08 08:34:12 by bbockelm:</em> <br/>\n\nScratch the side note about the number of children.  I fat-fingered the condor_config_val query, and failed to notice we had increased this setting locally.\n\n<p></p><hr/>\n<em>2012-Aug-08 09:02:15 by bbockelm:</em> <br/>\n\nAfter some discussion with tstclair in #distcomp, I realized what's happening here.\n\n<p>This occurred after an upgrade.  On upgrade of condor-qmf, as the QMF config file is not marked %config(noreplace), it overwrote our config file, and re-enabled the QMF plugin.  The collector was then restarted.  A few hours later, puppet came by and fixed the config file.  So, when I restarted the condor_collector, there was no evidence of threads.\n\n</p><p>Long story short, the collector was running with threads, and dprintf uses a thread-safe, but non-reentrant call (localtime).  This caused the children to deadlock, and the rest of the behavior.\n\n</p><p>So the lesson here (besides the fact we need to tweak our puppet config) - collector plugins and forked children cannot mix!</p></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "defect", "last_change": "2015-Aug-01 21:04", "status": "abandoned", "created": "2012-Aug-08 07:51", "fixed_version": "2012-Aug-08 07:51", "broken_version": "v070901", "priority": "4", "subsystem": "", "assigned_to": "", "derived_from": "", "creator": "bbockelm", "rust": "", "customer_group": "other", "visibility": "public", "notify": "", "due_date": ""}
{"id": 2681, "title": "Ticket #2681: parallel universe broken on dynamic slots (request_cpus is ignored)", "description": "<blockquote>\n<strong>Resolved ticket as this issue was addressed in #2808</strong>\n\n<p>[This is copied from two emails by Steffen Grunewald &lt;Steffen.Grunewald@aei.mpg.de&gt; to the CondorLIGO list.]\n\n</p><p>Getting closer with Parallel Universe on Dynamic slots... but still no cigar.\n\n</p><p>The setup consists of 5 4-core machines and some more 2-cores machines.\nAll of them have been configured as single, partitionable slots.\nPreemption is forbidden completely.\nThe rank definitions are as follows:\n</p><div class=\"code\">\n<pre class=\"code\">RANK = 0\nNEGOTIATOR_PRE_JOB_RANK = 1000000000 + 1000000000 * (TARGET.JobUniverse =?= 11) * (TotalCpus+TotalSlots) - 1000 * Memory\n</pre></div>\n\n\n<p>I'd expect this to favour big machines over small ones (for Parallel jobs), and partially occupied ones over empty ones.\n\n</p><p>What I see with the following submit file, is quite different:\n</p><div class=\"code\">\n<pre class=\"code\">universe   = parallel\ninitialdir = /home/steffeng/tests/mpi/\nexecutable = /home/steffeng/tests/mpi/mpitest\narguments  =  $(Process) $(NODE)\noutput     = out.$(NODE)\nerror      = err.$(NODE)\nlog        = log\nnotification = Never\non_exit_remove = (ExitBySignal == False) || ((ExitBySignal == True) &amp;&amp; (ExitSignal != 11))\nshould_transfer_files = yes\nwhen_to_transfer_output = on_exit\nRequirements = ( TotalCpus == 4 )\nrequest_memory = 500\nmachine_count = 10\n</pre></div>\n\n(mpitest is the ubiquitous \"MPI hello world\" program trying to get rank and\nsize from MPI_COMM_WORLD)\n\n<p></p><ul>\n<li>if I leave the Requirements out, the 10 MPI nodes will end up on the big 5 machines (one per machine) plus 5 small ones\n</li><li>with the Requirements set as above, each of the big machines will run exactly two nodes instead of 4+4+2+0+0\n</li><li>not all out.* and err.* files get written (the pattern looks semi-random)\n</li><li>all of them identify as \"rank 0\" of \"size 1\"\n</li></ul>\n\n<p>Condor version is 7.6.0 (and should include the fixes of ticket 986 which\nwent into 7.5.6).\n\n</p><p>How can I debug this?\n\n</p><p>===\n\n</p><p>It turns out that request_cpus=n, independent of n, will result in one slot\nclaimed per machine, as I could prove with \"request_cpus=4\" and \"machine_count=4\"\nwhich claimed a single slot on four machines, same as would \"request_cpus=1\" or\n\"request_cpus=2\" would have done.\n\n</p><p>\"machine_count\" obviously gets translated into the number of individual MPI jobs (nodes),\nand \"request_cpus\" would define the number of CPU cores assigned to each of them.\nIt's my problem if the nodes don't know about multi-core on their own.\n\n</p><p>Apparently, dynamic slot provisioning doesn't work well with parallel universe yet.\n\n</p><p>As soon as I return to old-style slot splitting (NUM_SLOTS=4, cpu=1, memory=25%, etc.)\nI get the \"proximity\" I'm looking for - of machine_count=10, the first 4 nodes get\nsent to one node, 4 to the next one, 2 to another.\n\n</p><p>So I either do hard partitioning, and get proper MPI behaviour, or dynamic partitioning,\nand am able to run memory-hungry jobs.\nUnfortunately, the users have been asking for both (and the mix is unpredictable).\n\n</p><p>To add to the inconvenience, for each reconfig Condor has to be stopped completely\non the machines affected.\n\n</p><p>Are there plans to make Condor more flexible?\nUsing up as many dynamic slots as possible on the same machine would help a lot.\nIn the manual, and everywhere else I looked, \"dynamic slots\" and \"parallel universe\"\nseem to be disjoint concepts...\n\n</p><p>BTW:\n<strong>If</strong> there was proper co-existence of dynamic slots and parallel universe, one would\nhave to look for a N_P_J_R expression that yields best results for the parallel job\nwhile harming as little other jobs as possible - perhaps such a thing doesn't even\nexist if preemption is allowed?\nWithout preemption things should be easier:\n- Rank by number of unclaimed CPUs?\n How to do that? have another machine <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=ClassAd\" title=\"Class Ad\">ClassAd</a></span> attribute <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=UnclaimedCpus\" title=\"Unclaimed Cpus\">UnclaimedCpus</a></span>?\n I vaguely remember someone had come up with a huge ifThenElse construction to\n sum up the resources \"bound\" by claimed dynamic slots, but there should be a\n solution that still works for 64 cores...)</p></blockquote>", "remarks": "<blockquote>\n<em>2011-Dec-02 16:36:34 by tstclair:</em> <br/>\n\nCould have sworn this has been fixed.\n\n<p></p><hr/>\n<em>2011-Dec-09 08:56:04 by matt:</em> <br/>\n\nSee also, <a class=\"external\" href=\"https://bugzilla.redhat.com/show_bug.cgi?id=545423\">https://bugzilla.redhat.com/show_bug.cgi?id=545423</a>\n\n<p></p><hr/>\n<em>2011-Dec-15 17:10:53 by eje:</em> <br/>\n\nFixing this requires making some mods to the negotiator.\n<ol>\n<li>Make the computation of available resources (\"pie\") more aware of partitionable slots (a sort of extension to the fix for <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=2440\" onclick=\"get_ticket_and_populate_wrapper('2440'); return false;\" title=\"HGQ improperly handling partitionable slots\">#2440</a></span>).\n</li><li>To achieve the desired \"packing\" of machines, the negotiator loop needs logic to allow matching more than one job to a partitionable slot ad.\n</li></ol>\n\n<p>Currently the negotiation loop cycles through the slot ads, under the (incorrect) assumption that one job can be matched to one ad.  Fixing this would also improve non-parallel matching performance, since a partitionable slot could match against more than one job per negotiation cycle.\n\n</p><p></p><hr/>\n<em>2011-Dec-16 10:57:00 by eje:</em> <br/>\n\nMaking the necessary changes to the negotiation logic seems a bit risky for stable series, especially on short notice and w/out better regression testing.\n\n<p>Changing this partitionable-slot based logic would also be a good opportunity for considering issues of code-paths around weighted/unweighted slots, and also the semantics of weighted slots combined with partitionable slots.\n\n</p><p></p><hr/>\n<em>2012-Feb-10 14:47:41 by pfc:</em> <br/>\n\nUpdated priority to match Todd's new scheme (1=fire, 2=soon, 3=time permitting, 4=not yet prioritized, 5=wishlist/ideas).\n\n<p></p><hr/>\n<em>2012-Apr-14 08:42:45 by tannenba:</em> <br/>\n\nThis development work in this ticket was addressed in <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=2808\" onclick=\"get_ticket_and_populate_wrapper('2808'); return false;\" title=\"Dedicated Scheduler doesn't work with fast dynamic slot partitioning\">#2808</a></span></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "defect", "last_change": "2012-Apr-14 08:45", "status": "defer", "created": "2011-Dec-01 21:44", "fixed_version": "2011-Dec-01 21:44", "broken_version": "v070600", "priority": "5", "subsystem": "Parallel", "assigned_to": "gthain", "derived_from": "#986", "creator": "pfc", "rust": "", "customer_group": "ligo", "visibility": "public", "notify": "pfcouvar@syr.edu, Steffen.Grunewald@aei.mpg.de, tstclair@redhat.com, eje@cs.wisc.edu, tannenba@cs.wisc.edu, gthain@cs.wisc.edu, tstclair@redhat.com,pcouvare@caltech.edu", "due_date": ""}
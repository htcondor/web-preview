{"id": 2420, "title": "Ticket #2420: preen finds stale ickpt files in spool directory --- why?", "description": "<blockquote>\nIgor asks why preen found stale ickpt files in the spool directory:\n\n<p></p><div class=\"verbatim\">\n<pre>The condor_preen process has found the following\nstale condor files on &lt;glidein-1&gt;:\n\n  /opt/glidecondor/condor_local/schedd_glideins3/spool/3474/cluster233474.ickpt.subproc0 - Removed\n  /opt/glidecondor/condor_local/schedd_glideins3/spool/3861/cluster233861.ickpt.subproc0 - Removed\n  /opt/glidecondor/condor_local/schedd_glideins3/spool/3760/cluster233760.ickpt.subproc0 - Removed\n  /opt/glidecondor/condor_local/schedd_glideins3/spool\n&lt;many more cases like the above --- 121 in total&gt;\n</pre></div>\n\n\n<p>One thing that I find quite disturbing is the logic in condor_preen for deciding if a spooled file is still associated with a job that is in the queue.  It queries the schedd to see if the job still exists.  It does not distinguish between failure of the query and non-existence of the job!\n\n</p><p>The connection to the schedd is created with no timeout on the client side, but other things could go wrong: schedd times out, schedd dies in the middle of the query, network interruption.\n\n</p><p>Unfortunately for this theory, I see no evidence in the <code>ScheddLog</code> that any query errors happened when preen was running.  I also confirmed that the cluster ids mentioned in the preen message do not appear in the job queue or in the history, which goes back two days.\n\n</p><p>Therefore, I think these deletions by condor_preen were legitimate garbage files.  The question then is why these files got left behind.  I don't yet have an explanation for that.</p></blockquote>", "remarks": "<blockquote>\n<em>2011-Sep-01 16:49:51 by danb:</em> <br/>\n\nAnother incident of this problem happened.  This time, I noticed that the job was removed forcibly:\n\n<p></p><pre>   LastHoldReason = \"Globus error 131: the user proxy expired (job is still running)\"\n   HoldReason = \"Globus error 121: the job state file doesn't exist\"\n   RemoveReason = \"via condor_rm -forcex (by user gfactory)\"\n</pre>\n\n<p>I experimented in 7.7.2 with submitting both vanilla and gt2 jobs and getting them into a state where they were removed (status appears as X).  When in this state, preen still correctly sees that the file should not be removed.  Then when I remove the job with condor_rm -forcex, the file is correctly removed.  So I failed to reproduce the problem.\n\n</p><p></p><hr/>\n<em>2012-Oct-31 15:46:00 by jfrey:</em> <br/>\n\nIgor found the source of this problem. When <code>copy_to_spool</code> is set to <code>True</code> or the job is standard universe, condor_submit spools the executable to the schedd as part of the queue-management transaction, before setting any of the job's attributes. If the transaction is aborted, the job is never submitted but the file isn't cleaned up by the schedd.\n\n<p>In Igor's case, the submission was aborted because the job's proxy was expired.\n\n</p><p></p><hr/>\n<em>2012-Nov-02 16:47:46 by jfrey:</em> <br/>\n\nA possible solution is to call ClusterCleanup(), or some subset of it, in the schedd for each uncommitted cluster when aborting a transaction.</blockquote>", "derived_tickets": "", "attachments": "", "check_ins": "", "type": "incident", "last_change": "2012-Nov-02 16:47", "status": "new", "created": "2011-Aug-25 11:51", "fixed_version": "2011-Aug-25 11:51", "broken_version": "v070602", "priority": "4", "subsystem": "", "assigned_to": "danb", "derived_from": "", "creator": "danb", "rust": "", "customer_group": "cms", "visibility": "public", "notify": "isfiligoi@ucsd.edu,dan@hep.wisc.edu,jfrey@cs.wisc.edu,tstclair@redhat.com", "due_date": ""}
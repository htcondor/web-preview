{"id": 7882, "title": "Ticket #7882: Spurious out of memory errors with cgroup_memory_policy = soft", "description": "<blockquote>\nNote:  This is a dup of htcondor-17\n\n<p>The HTCondor starter can ask the cgroup memory controller to freeze all processes within a job's cgroup when the memory usage of all the processes in the cgroup hits some limit.  That is, if the startd is runnning as root, and CGROUP_MEMORY_LIMIT_POLICY = hard.  The starter does this by writing the memory size of the slot, in bytes to memory.memory_limit_in_bytes.  When this happens, the starter is notified, and it has a decision to make. It can remove the job from the worker node and put it on hold. Or, it can ignore the message, sleep for a bit, and hope for the best.  Early versions of HTCondor just did the former, but we ran into a problem \u2013 on machines running non-condor services that used significant amounts of memory, the starter could get the out-of-memory signal even when the job was using less than the amount allocated to the slot (because a non-condor process was hogging memory, and cgroups don't support memory reservations, just limits.  It seemed unfair to put the job on hold (and this was something that the Nemo cluster tripped over a lot), so, we changed HTCondor to check how much memory the job was using, and if it was using less than the allocation, we'd continue, and usually the OOM signal eventually made it to the real offender.\n\n</p><p>However, when we poll the procd to ask it how much memory the job is using, there are a lot of categories of memory: let's just consider the main two:  user-space RSS, and the kernel page cache used by the job.  The limit in memory.memory_limit_in_bytes, does not count just the user-space RSS, but it includes at least some amount of kernel page cache (this is, arguably a bug, but what are you going to do?).  So, when the starter gets the OOM signal, if it just looks at the pure user space RSS, it will always be lower than the provisioned memory limit, and it will let the job go, and it will get into a tight loop, making no progress.  CHTC got hit by this a lot.  So, we changed the memory reported by the start to include the page cache.  It seemed inconsistent to have one memory usage that was only used for the \"what do I do when I get OOM'ed\" vs. the job as a whole, so we only report the one.\n\n</p><p>Now this becomes a problem if CGROUP_MEMORY_LIMIT_POLICY = soft, as the page cache can grow arbitrarily large.  Note that the sum of all memory usage, including user + kernel space is capped to the limit when the policy is hard.  Note that it is reflecting the true memory used, but it becomes not very useful if you have a PERIODIC_HOLD policy that looks at this value.\n\n</p><p>LIGO has both a periodic hold policy that puts the job when it is using more than 1.5 times the provisioned amount and a SOFT memory policy.  LIGO's goal is to only have a hard limit for the sum of all jobs on a machine \u2013 if there is just one job running (or one job using a significant amount of memory), LIGO is happy to let it use all of the memory on the machine, if none of the other jobs want it. (Or Maybe up to 1.5 times the provisioned amount, according to the SYSTEM_PERIODIC_REMOVE policy).\n\n</p><p>A modest proposal:\n\n</p><p>It was a mistake to support either a soft or hard memory limit policy.  These are two separate limits in cgroups v1 (there are three in v2, grid help us all)  I would suggest that we always set both, with the soft limit below the hard limit.\n\n</p><p>1) Add knobs that allow the startd (or is it starter?) CGROUP_MEMORY_LIMIT_HARD  and CGROUP_MEMORY_LIMIT_SOFT to set the hard and soft limit to a classad expression referencing aspects of machine (MY) or job (TARGET).\n\n</p><p>2) When CGROUP_MEMORY_LIMIT_POLICY is set to HARD, that sets the following values to the knobs added in step 1:\nCGROUP_MEMORY_LIMIT_HARD = Memory\nCGROUP_MEMORY_LIMIT_SOFT = 0.9 * Memory\n\n</p><p>3) When CGROUP_MEMORY_LIMIT_POLICY is set to SOFT, that sets the following values to the knobs added in step 1:\nCGROUP_MEMORY_LIMIT_HARD = <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=TotalMemory\" title=\"Total Memory\">TotalMemory</a></span>   (...this attribute is startd <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=DetectedMemory\" title=\"Detected Memory\">DetectedMemory</a></span> - <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=ReservedMemory\" title=\"Reserved Memory\">ReservedMemory</a></span> IIRC...)\nCGROUP_MEMORY_LIMIT_SOFT = 0.9 * Memory\n\n</p><p>4) We add a fourth \"CUSTOM\" option to CGROUP_MEMORY_LIMIT_POLICY.  If CGROUP_MEMORY_LIMIT_POLICY=CUSTOM, then the startd will EXCEPT with an error if either CGROUP_MEMORY_LIMIT_HARD and CGROUP_MEMORY_LIMIT_SOFT are not explicitly set in the config file, and/or if CGROUP_MEMORY_LIMIT_HARD is not  greater than CGROUP_MEMORY_LIMIT_SOFT.  If CGROUP_MEMORY_LIMIT_POLICY is anything but CUSTOM, then CGROUP_MEMORY_LIMIT_* knobs are ignored.  We can document the CUSTOM option as only for kernel tuning experts and is not recommended for general use.\n\n</p><p>5) Change the procd so that we only report RSS; specifically, we want to inspect the cgroup rss metric which we believe encapsulates the anonymous memory footprint of the set of processes in the group (but not cache pages).\n\n</p><p>6) When the starter catches the OOM signal, and we are using cgroups to monitor job usage, then we kill the job if all memory used by the cgroup (both RSS and active_file/cache) is greater than CGROUP_MEMORY_LIMIT_HARD.  If this is too difficult to code because of the way we channel everything through the silly procd, then alternative kill the job if <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=MemoryUsage\" title=\"Memory Usage\">MemoryUsage</a></span> (which default to RSS) &gt; CGROUP_MEMORY_LIMIT_SOFT .</p></blockquote>", "remarks": "<blockquote>\n</blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">2020-Nov-05 07:07</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/54e5b6c3e6105dcd8ca52720e7050ad6b13229ac\">[61606]</a></span>: Can build without libcgroup, needed on conda-forge <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=7882\" onclick=\"get_ticket_and_populate_wrapper('7882'); return false;\" title=\"Spurious out of memory errors with cgroup_memory_policy = soft\">#7882</a></span> HTCONDOR-17  (By Tim Theisen )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2020-Oct-19 16:37</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/1eca87a26e26af42816997b41ea009f4d532ebd2\">[61518]</a></span>: Document <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=7882\" onclick=\"get_ticket_and_populate_wrapper('7882'); return false;\" title=\"Spurious out of memory errors with cgroup_memory_policy = soft\">#7882</a></span>  (By Greg Thain )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2020-Oct-09 08:40</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/897aac4eeb0be812799d6647e270004d87ac7950\">[61457]</a></span>: Fix linux cgroup memory reporting to remove cache <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=7882\" onclick=\"get_ticket_and_populate_wrapper('7882'); return false;\" title=\"Spurious out of memory errors with cgroup_memory_policy = soft\">#7882</a></span> HTCONDOR-17  (By Greg Thain )</td></tr>\n</tbody></table>", "type": "defect", "last_change": "2020-Oct-19 16:38", "status": "resolved", "created": "2020-Oct-09 08:39", "fixed_version": "2020-Oct-09 08:39", "broken_version": "v080907", "priority": "1", "subsystem": "", "assigned_to": "gthain", "derived_from": "", "creator": "gthain", "rust": "", "customer_group": "ligo", "visibility": "public", "notify": "", "due_date": ""}
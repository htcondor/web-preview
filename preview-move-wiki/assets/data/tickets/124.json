{"id": 124, "title": "Ticket #124: Perform some HDFS testing", "description": "<blockquote>\nPerform some HDFS testing to gain experience and confidence.\n\n<p>Everybody focuses on testing scalability.  So for the moment we will not worry so much about HDFS scalability.  Instead, we will focus on\n</p><ul>\n<li><strong>availability</strong> - Can it run for days on end shuffling data?\n</li><li><strong>fault tolerance</strong> - What happens when block server(s) are killed/restarted?  What happens when file servers are killed/restarted?  What happens when block servers run out of disk space?  What happens when the network dies?</li></ul>\n</blockquote>", "remarks": "<blockquote>\n<em>2009-Jan-30 12:53:57 by faisal:</em> <br/>\n\nMost of these tests were conducted using JAVA API of Hadoop. The test suite uses a configurable way to generate directory and file structure with different mix of file sizes. Multiple reader and writer threads then read different files and report transfer rate. For some of the (future) tests, that directly concerns us, I will be using Hadoop's network protocol messages e.g placing all the blocks of a given file on a particular machine.\n\n<p>Additionally, these tests mostly deals with reliability and fault-tolerance of file transfers and to some extent feature verification. We expect Hadoop to scale fairly well in terms of files, their size and number of transfers.\n\n</p><p>Tests\n\n</p><p></p><ul>\n<li>Filling up storage\n<ul>\nOnce the storage is filled client will quit complaining that file could only be replicated to '0' nodes while '1' was required.\n</ul>\n\n<p></p></li><li>Adding a new datanode server after the storage is filled to capacity\n<ul>\nHadoop doesn't start to rebalance nodes automatically, but there are scripts to do this. In this tests HDFS started to use new node.\n</ul>\n\n<p></p></li><li>Setting higher replication than available data nodes\n<ul>\nDidn't see any problem, HDFS will start replicating file as soon as a new data node is available.\n</ul>\n\n<p></p></li><li>Nameserver got killed during write\n<ul>\nTransfer stopped after making couple of retires. Write call remained blocked during this time. These retires can be configured.\n</ul>\n\n<p></p></li><li>Taking a data node offline while writing to a file\n<ul>\nClient exits after making couple of tries to reconnect to datanode server. The dead-node remained in the list of naemnode's available storage for quite some time. I saw all my attempts to write a file failing as HDFS was trying to contact the dead datanode. I will try to get more insight about this behavior from Hadoop's mailing list.\n</ul>\n\n<p></p></li><li>Taking a datanode offline while reading a file (with no replica)\n<ul>\nRead call will be blocked and client will give up after making configurable number of retries.\n</ul>\n\n<p></p></li><li>Taking a datanode offline while reading a file (with at-least 2 replicas)\n<ul>\nTransfer continues seamlessly. It might have an affect on performance when doing lot of parallel transfers.\n</ul>\n</li></ul>\n\n<p>I might do some more tests, specially to see the performance of moving of individual blocks from one data-node to other.</p></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "todo", "last_change": "2009-Dec-26 22:02", "status": "active", "created": "2009-Jan-26 13:51", "fixed_version": "2009-Jan-26 13:51", "broken_version": "", "priority": "4", "subsystem": "", "assigned_to": "faisal", "derived_from": "#4", "creator": "tannenba", "rust": "", "customer_group": "other", "visibility": "public", "notify": "", "due_date": ""}
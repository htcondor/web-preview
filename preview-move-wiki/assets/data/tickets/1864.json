{"id": 1864, "title": "Ticket #1864: HDFS needs to test for a oracle java (not gcj)", "description": "<blockquote>\nCondor HDFS fails in a very hard to diagnose way if the version of java is gcj instead of oracle java.   We should check for the version of java we are trying to run with and throw a log error if its not one we expect.</blockquote>", "remarks": "<blockquote>\n<em>2011-Jan-20 15:31:15 by matt:</em> <br/>\n\nCould you add details of how it fails and how someone might diagnose? Users using a web search for issues will thank you.\n\n<p></p><hr/>\n<em>2011-Jan-20 16:59:17 by khahn:</em> <br/>\n\nthe hdfs daemon will try to run the hadoop daemon and it will fail with a return code of 256 almost immediately:\n\n<p>This is a sample of the outgoing error message:\nERROR \"hadoop daemon 3004 was killed unexpectedly\n\" at line 390 in file /home/condor/execute/dir_12342/userdir/src/condor_hdfs/had\noop.cpp\n\n</p><p></p><hr/>\n<em>2011-Jan-20 19:18:59 by bbockelm:</em> <br/>\n\nThe recommendation of the Hadoop developers is indeed Oracle Java (although some folks occasionally tinker around with JRockit and whatever IBM Java is called these days; these efforts aren't as thorough as the testing by the devs).\n\n<p>However, there doesn't appear to be many issues with using OpenJDK.  You probably want to allow that also.</p></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "defect", "last_change": "2015-Aug-01 20:59", "status": "abandoned", "created": "2011-Jan-20 15:10", "fixed_version": "2011-Jan-20 15:10", "broken_version": "v070505", "priority": "3", "subsystem": "Daemons", "assigned_to": "ilikhan", "derived_from": "", "creator": "khahn", "rust": "", "customer_group": "chtc", "visibility": "public", "notify": "khahn@cs.wisc.edu", "due_date": ""}
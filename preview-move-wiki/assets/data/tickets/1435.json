{"id": 1435, "title": "Ticket #1435: Schedd crashes trying to malloc", "description": "<blockquote>\nWe're seeing periodic crashes with a client's schedds. Losing a schedd or two a week now due to this issue.\n\n<p>There appears to be no correlation between load and the crash. Up time may be a factor here.\n\n</p><p>The stack trace on crash is always the same: malloc is causing the core dump. But the entry differs. I've seen it happen on allocating space for some socket IO descriptor. And today it happened in Scheduler::count_jobs:\n\n</p><p></p><div class=\"verbatim\">\n<pre>(gdb) bt\n#0  WriteCoreDump (file_name=0x84c2df0 \"\") at src/coredumper.c:137\n#1  0x081ce6cc in linux_sig_coredump (signum=11) at daemon_core_main.cpp:671\n#2  &lt;signal handler called&gt;\n#3  0x00a073b2 in abort () from /lib/tls/libc.so.6\n#4  0x00a39cda in __libc_message () from /lib/tls/libc.so.6\n#5  0x00a3fe2d in malloc_consolidate () from /lib/tls/libc.so.6\n#6  0x00a40c30 in _int_malloc () from /lib/tls/libc.so.6\n#7  0x00a42aa1 in malloc () from /lib/tls/libc.so.6\n#8  0x0019390e in operator new () from /usr/lib/libstdc++.so.5\n#9  0x00193a5f in operator new[] () from /usr/lib/libstdc++.so.5\n#10 0x08152a75 in ExtArray (this=0xbff98740, old=@0x85e0bcc) at ../condor_c++_util/extArray.h:94\n#11 0x08132eee in Scheduler::count_jobs (this=0x85e0a40) at schedd.cpp:725\n#12 0x08132ca4 in Scheduler::timeout (this=0x85e0a40) at schedd.cpp:640\n#13 0x081d87ef in TimerManager::Timeout (this=0x861f1ac) at timer_manager.cpp:394\n#14 0x081bc7a1 in DaemonCore::Driver (this=0x9862060) at daemon_core.cpp:2912\n#15 0x081d1658 in main (argc=1, argv=0xbff99198) at daemon_core_main.cpp:2276\n</pre></div>\n\n\n<p>The top of the stack is always a malloc call.\n\n</p><p>We put a memory watch on a schedd after the crash last week, suspecting it might be a memory leak but we're not seeing an ever increasing amount of memory being used.\n\n</p><p>Client is running:\n\n</p><p></p><div class=\"verbatim\">\n<pre>$CondorVersion: 7.4.2 Mar 29 2010 BuildID: 227044 $\n$CondorPlatform: I386-LINUX_RHEL3 $\n</pre></div>\n\n\n<p>On:\n\n</p><p></p><div class=\"verbatim\">\n<pre>Linux latlnh007 2.6.9-42.ELsmp #1 SMP Wed Jul 12 23:27:17 EDT 2006 i686 i686 i386 GNU/Linux\nRed Hat Enterprise Linux AS release 4 (Nahant Update 7)\n</pre></div>\n\n\n<p>Pre-7.4.2 things were stable for this setup.\n\n</p><p>I have the full <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=ScheddLog\" title=\"Schedd Log\">ScheddLog</a></span> with time before and after the crash captured. I'll attach it as a comment to the ticket.</p></blockquote>", "remarks": "<blockquote>\n<em>2010-Jun-09 07:40:17 by tannenba:</em> <br/>\n\nWhen you say \"Pre-7.4.2 things were stable\", does that mean v7.4.1 did not have this problem?  Hopefully so, since the list of code patches that happened between v7.4.1 and v7.4.2 isn't all that big...\n\n<p></p><hr/>\n<em>2010-Jun-09 08:45:06 by ichesal:</em> <br/>\n\nNo, unfortunately. They went from 7.0.5 -&gt; 7.4.2.\n\n<p></p><hr/>\n<em>2010-Jun-16 12:16:00 by matt:</em> <br/>\n\nHow many jobs does the Schedd have in its queue when it crashes? What's the mix of Clusters &amp; Procs &amp; job ad sizes (using getenv=true)? Does it crash on restart?\n\n<p></p><hr/>\n<em>2010-Jun-21 11:06:24 by ichesal:</em> <br/>\n\nSome answers to your questions:\n\n<p>Average attributes per job is about 100. Job ads average 4.3kB in size. Getenv=true isn't in use.\n\n</p><p>We're not seeing a strong correlation between number of jobs in the queue and the crash. We've seen bigger job loads with no crash. That being said right before a crash I've got: 1655 idle, 733 running. We've had up to 2000 jobs running from this  schedd shortly after it'd rebooted from the crash and it was fine.\n\n</p><p>The schedd does not crash on restart. Restarts just fine actually.\n\n</p><p>I'm trying to get you some more information like total number of jobs run between crashes.\n\n</p><p></p><hr/>\n<em>2010-Jul-02 08:26:03 by ichesal:</em> <br/>\n\nI set two monitors on the machine to gather some more data.\n\n<p>The first monitor ran:\n\n</p><p></p><pre>  uptime\n  free -m\n  vmstat 1 5\n  ps auxf --width=200\n</pre>\n\n<p>At 10 minute intervals. The output was memmon.log\n\n</p><p>The second monitor ran, for every schedd process it could find on the machine:\n\n</p><p></p><pre>  pmap $pid\n  top -n1 -b -p $pid\n</pre>\n\n<p>At 10 minute intervals. The output was scheddmon.log.\n\n</p><p>The results are...inconclusive. I'm attaching the log files from a period in time where the schedd was crashing with an OOM error. Take a look at 21:40 and then at 21:50 in the attached log files. 21:40 is just before the crash, 21:50 is after the crash. Only one schedd reported crashing in its log file with an OOM error, but all three schedds got new PIDs -- so they were all restarted.\n\n</p><p>The memory consumption on the machine looks sane righ up to the 21:20 mark or so and then it starts to climb. And at 21:40 it's at 12GB out of the 16GB available. But it does not appear that the schedds on the box are responsible for the increase in memory, all three having virtual sizes in the 100MB range and resident sizes less than that. No condor_shadow is using more than 10MB.\n\n</p><p></p><hr/>\n<em>2010-Jul-02 08:34:50 by ichesal:</em> <br/>\n\nMy zipped log file is 1.8 MB -- too big to attach. You can grab it from here:\n\n<p><a class=\"external\" href=\"http://dl.dropbox.com/u/870088/memory-monitor-logs.zip\">http://dl.dropbox.com/u/870088/memory-monitor-logs.zip</a>\n\n</p><p></p><hr/>\n<em>2010-Jul-02 10:41:45 by ichesal:</em> <br/>\n\nHere's a question: is the shadown exploiting Linux's in-memory cache/swap features in some new way in 7.4.x that it wasn't doing in 7.0.x?\n\n<p></p><hr/>\n<em>2010-Jul-02 19:48:38 by ichesal:</em> <br/>\n\nI have another crashed schedd. Same stack trace in the log file as the others:\n\n<p></p><pre>   Stack dump for process 4236 at timestamp 1278094718 (24 frames)\n   condor_schedd(dprintf_dump_stack+0xda)[0x81dd0fd]\n   condor_schedd(_Z18linux_sig_coredumpi+0x23)[0x81ce68b]\n   /lib/tls/libpthread.so.0[0xa43a98]\n   /lib/tls/libc.so.6[0xb8ec30]\n   /lib/tls/libc.so.6(malloc+0x81)[0xb90aa1]\n   /usr/lib/libstdc++.so.5(_Znwj+0x2e)[0xd4a90e]\n   /usr/lib/libstdc++.so.5(_Znaj+0x1f)[0xd4aa5f]\n   condor_schedd(_ZN3Buf9alloc_bufEv+0x21)[0x82b6dd3]\n   condor_schedd(_ZN3Buf4readEPKciii+0x12)[0x82b6f68]\n   condor_schedd(_ZN8ReliSock6RcvMsg10rcv_packetEPKcii+0x250)[0x82b37c0]\n   condor_schedd(_ZN8ReliSock22handle_incoming_packetEv+0x62)[0x82b2db6]\n   condor_schedd(_ZN8ReliSock9get_bytesEPvi+0x3a)[0x82b3228]\n   condor_schedd(_ZN6Stream3getERi+0x92)[0x82ac2e4]\n   condor_schedd(_ZN6Stream4codeERi+0x51)[0x82a9b97]\n   condor_schedd(_ZN10DaemonCore9HandleReqEP6StreamS1_+0xe54)[0x81bf3f6]\n   condor_schedd(_ZN10DaemonCore22HandleReqSocketHandlerEP6Stream+0x9b)[0x81be41b]\n   condor_schedd(_ZN10DaemonCore24CallSocketHandler_workerEibP6Stream+0x219)[0x81bdac5]\n   condor_schedd(_ZN10DaemonCore35CallSocketHandler_worker_demarshallEPv+0x2e)[0x81bd898]\n   condor_schedd(_ZN13CondorThreads8pool_addEPFvPvES0_PiPKc+0x29)[0x827e1f1]\n   condor_schedd(_ZN10DaemonCore17CallSocketHandlerERib+0x19a)[0x81bd862]\n   condor_schedd(_ZN10DaemonCore6DriverEv+0x1465)[0x81bd5b9]\n   condor_schedd(main+0x183a)[0x81d1658]\n   /lib/tls/libc.so.6(__libc_start_main+0xd3)[0xb40df3]\n   condor_schedd(ldexp+0x59)[0x812f3b1]\n</pre>\n\n<p>Unfortunately the core file was corrupted but that's the same as the others. It's dumping because the malloc is failing at the top of the stack.\n\n</p><p>Now here is the crazy part: this schedd was doing <strong>nothing</strong>. Some held jobs on it and that's it. Very little memory use, zero CPU use. And it'd been in that state for a while.\n\n</p><p>If you take a look at the scheddmon and memmon log files in the zip file here: <a class=\"external\" href=\"http://dl.dropbox.com/u/870088/crashing-schedd-2010-07-02.zip\">http://dl.dropbox.com/u/870088/crashing-schedd-2010-07-02.zip</a>\n\n</p><p>You'll see that the three schedd instances on this machine were all idle. Nothing was going on. The last snapshot before the schedd crashed happens at 14:10 (search for \"14:10\" in both files and you'll see all the details).\n\n</p><p>The ingredients for today's crash were: 32-bit RHEL3 Condor 7.4.2 statically linked binaries on 64-bit RHEL4.\n\n</p><p>(Note: guessing on static linking. The condor_schedd is 5.9 MB -- that seems like a statically linked sized binary file)\n\n</p><p>So I can rule out scheduler load as being a factor in causing the crash. It's not related to memory consumption from the scheduler process. I suppose it's possible the scheduler went from a few tens of megabytes to its upper limit in the time between when the last snapshot at 14:10 and the crash time at 14:18 it's highly unlikely.\n\n</p><p></p><hr/>\n<em>2010-Jul-12 13:50:35 by ichesal:</em> <br/>\n\nOn Peter Keller's advice I've put together a valgrind recipe for 7.4.2 in the customer environment. We were talking and because the crash can be triggered without the schedd using much memory on the machine (nearly nothing really) it might be memory corruption is occurring. Hence the valgrind request.\n\n<p>While I was testing the valgrind stuff though, looking at the valgrind output, I was seeing a whole lot of:\n\n</p><p></p><div class=\"verbatim\">\n<pre>Conditional jump or move depends on uninitialised value(s)</pre></div>\n\n\n<p>messages from valgrind. From all over the code base.\n\n</p><p></p><hr/>\n<em>2010-Jul-12 13:54:22 by ichesal:</em> <br/>\n\nAlso: before I get to running this is there any other option you'd want on the valgrind call? I'm running it with:\n\n<p></p><div class=\"verbatim\">\n<pre>#!/bin/sh\n# /opt/condor/sbin/condor_schedd_valgrind\nVALGRINDLOGDIR=/var/cycle/valgrind\nmkdir -p ${VALGRINDLOGDIR} 2&gt;&amp;1 &gt; /dev/null\nchmod 777 ${VALGRINDLOGIDR} 2&gt;&amp;1 &gt; /dev/null\n/usr/bin/valgrind --leak-check=yes --time-stamp=yes --log-file=${VALGRINDLOGDIR}/`hostname`.%p.log /opt/condor/sbin/condor_schedd $*\n</pre></div>\n\n\n<p></p><hr/>\n<em>2010-Jul-14 09:22:25 by alderman:</em> <br/>\n\nAt yesterday's flightworthy meeting, it came up that this may be the same issue Igor is seeing with SOAP submissions.  On the schedd where we're seeing the crashes described here, SOAP submissions are enabled and actively used.\n\n<p></p><hr/>\n<em>2010-Sep-23 10:53:44 by tstclair:</em> <br/>\n\nnew one from condor users: David Arthur &lt;mumrah@gmail.com&gt;\n\n<p>I've noticed when a transaction is not properly closed (committed or\naborted), Schedd has a tendency to crash. Shouldn't condor_master\nnotice this and bring condor_schedd back up? If I kill condor_schedd,\ncondor_master does its job and brings up a new one. This seems like a\nvery bad state to get in - schedd cannot receive new jobs (via SOAP or\ncondor_submit) but condor_master does not see it as failed.\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">condor_schedd(dprintf_dump_stack+0xb7)[0x5e54a5]\ncondor_schedd(_Z18linux_sig_coredumpi+0x2c)[0x5d5abc]\n/lib64/libpthread.so.0[0x3043c0eb10]\n/lib64/libc.so.6(abort+0x28f)[0x3043431e8f]\n/lib64/libc.so.6[0x304346a84b]\n/lib64/libc.so.6[0x30434739ac]\n/lib64/libc.so.6(__libc_malloc+0x6e)[0x3043474cde]\n/lib64/libc.so.6(__strdup+0x22)[0x3043479522]\ncondor_schedd(_ZNK17CondorVersionInfo22string_to_PlatformDataEPKcRNS_11VersionDataE+0x16f)[0x66c021]\ncondor_schedd(_ZN17CondorVersionInfoC1EPKcS1_S1_+0x88)[0x66cbea]\ncondor_schedd(param+0x25)[0x61d201]\ncondor_schedd(_Z13param_integerPKcRibibiiP7ClassAdS3_b+0x14a)[0x61edca]\ncondor_schedd(_Z13param_integerPKciiib+0x5c)[0x61f226]\ncondor_schedd(_Z24condor__beginTransactionP4soapiR32condor__beginTransactionResponse+0x3a)[0x5b7586]\ncondor_schedd(_Z35soap_serve_condor__beginTransactionP4soap+0xc0)[0x5b1dcc]\ncondor_schedd(_Z18soap_serve_requestP4soap+0xa6)[0x5b250c]\ncondor_schedd(_Z10soap_serveP4soap+0xc8)[0x5b29f0]\ncondor_schedd(_ZN10DaemonCore9HandleReqEP6StreamS1_+0xe9f)[0x5c9b9d]\ncondor_schedd(_ZN10DaemonCore22HandleReqSocketHandlerEP6Stream+0x99)[0x5cccbf]\ncondor_schedd(_ZN10DaemonCore24CallSocketHandler_workerEibP6Stream+0x279)[0x5cca33]\ncondor_schedd(_ZN10DaemonCore35CallSocketHandler_worker_demarshallEPv+0x39)[0x5ccc1b]\ncondor_schedd(_ZN13CondorThreads8pool_addEPFvPvES0_PiPKc+0x3f)[0x68f331]\ncondor_schedd(_ZN10DaemonCore17CallSocketHandlerERib+0x1cd)[0x5bee09]\ncondor_schedd(_ZN10DaemonCore6DriverEv+0x18c4)[0x5c074e]\ncondor_schedd(main+0x18eb)[0x5d7d6b]\n/lib64/libc.so.6(__libc_start_main+0xf4)[0x304341d994]\ncondor_schedd(__gxx_personality_v0+0x419)[0x5252a9]\n</pre></div>\n\n\n<p></p><hr/>\n<em>2010-Oct-21 12:58:10 by ichesal:</em> <br/>\n\nAny updates on this? Anything I can do to help solve this?\n\n<p>- Ian C.</p></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body><blockquote>\n<ul>\n<li><a href=\"../files/184/SchedLog.core.06082010.bz2\">SchedLog.core.06082010.bz2</a>\n235808 bytes added by ichesal on 2010-Jun-09 00:55:57 UTC.\n<br/>\nAdding the sanitized <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=ScheddLog\" title=\"Schedd Log\">ScheddLog</a></span> that captures the time before and after the crash event.<br/>\n</li></ul>\n</blockquote></body></html>", "check_ins": "", "type": "defect", "last_change": "2010-Oct-21 12:58", "status": "new", "created": "2010-Jun-08 14:16", "fixed_version": "2010-Jun-08 14:16", "broken_version": "v070402", "priority": "1", "subsystem": "Daemons", "assigned_to": "", "derived_from": "", "creator": "ichesal", "rust": "", "customer_group": "other", "visibility": "public", "notify": "ichesal@cyclecomputing.com matt@cs.wisc.edu tstclair@redhat.com ialderman@cyclecomputing.com", "due_date": ""}
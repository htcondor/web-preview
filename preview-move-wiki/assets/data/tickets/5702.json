{"id": 5702, "title": "Ticket #5702: condor_rm on scheduler universe job does not halt submission", "description": "<blockquote>\nIf you condor_rm a large DAG (say, 5k jobs) by condor_rm'ing the scheduler universe job(s) it/them-selves, you will find that jobs continue to be submitted and execute. These jobs survive the final removal of DAGMan itself which can take several minutes as the thousands of jobs are stopped.\n\n<p>Known to be true for as long as I've used condor. Poked here and there but it came up again today. Had to manually condor_rm the jobs that survived.\n\n</p><p>Tom</p></blockquote>", "remarks": "<blockquote>\n<em>2016-Jul-06 12:58:56 by wenger:</em> <br/>\n\nHmm -- I just saw this ticket.  Anyhow, it's interesting that you say you've seen this for a long time, because (some time around the 8.1 series, maybe) we changed how this works.\n\n<p>Anyhow, I'd like to see the dagman.out and dagman.log files from a case where this happened.\n\n</p><p></p><hr/>\n<em>2016-Jul-06 13:24:25 by wenger:</em> <br/>\n\nI'm less surprised that this might happen now than that it apparently happened with the \"old\" remove mechanism (where DAGMan itself removed the node jobs).\n\n<p>I could see something maybe happening with the current scheme, if there's a race condition where the schedd does its node job removal before DAGMan job itself has been killed.  (In the LIGO call today, Tom said that he thinks this tends to happen when the filesystem is heavily loaded, which seems like it adds credence to the \"race condition\" idea.)\n\n</p><p>Also, Tom says that he came on at around 8.0, so if the change to the removal mechanism was made before then, this behavior might not have been seen with the old mechanism.\n\n</p><p>(Also see <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=5175\" onclick=\"get_ticket_and_populate_wrapper('5175'); return false;\" title=\"Have condor_dagman condor_rm jobs on SIGUSR1 (revert #4618)\">#5175</a></span>: Have condor_dagman condor_rm jobs on SIGUSR1 (revert <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=4618\" onclick=\"get_ticket_and_populate_wrapper('4618'); return false;\" title=\"Remove DAGMan code that condor_rms node jobs on condor_rm of DAGMan\">#4618</a></span>).)\n\n</p><p>(Hmm -- <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=4618\" onclick=\"get_ticket_and_populate_wrapper('4618'); return false;\" title=\"Remove DAGMan code that condor_rms node jobs on condor_rm of DAGMan\">#4618</a></span> was only fixed in 8.3.2, so that's certainly since Tom was around.)\n\n</p><p>(I'm wondering if this could be reliably reproduced by introducing an artificial delay at the right point in the schedd code, assuming the \"race condition\" idea is correct.)\n\n</p><p></p><hr/>\n<em>2016-Jul-06 13:34:00 by tpdownes:</em> <br/>\n\nI can state with reasonable certainty (by memory) that the condor_dagman job remains on the system after it has entered the 'X' state according to the queue. It is during this period that it submits new jobs that are later \"orphaned.\"\n\n<p>If this occurs again, I'll provide the requested logs.\n\n</p><p></p><hr/>\n<em>2016-Jul-06 13:35:03 by tpdownes:</em> <br/>\n\nDuring this same period a bunch of the jobs managed by the DAG also enter the X state (except the orphans).\n\n<p></p><hr/>\n<em>2016-Jul-06 13:53:23 by wenger:</em> <br/>\n\nAlso see <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=4628\" onclick=\"get_ticket_and_populate_wrapper('4628'); return false;\" title=\"Should condor_rm'ed DAGMan wait for nodes to exit before FINAL runs?\">#4628</a></span>: Should condor_rm'ed DAGMan wait for nodes to exit before FINAL runs?\n\n<p></p><hr/>\n<em>2016-Jul-06 13:54:24 by wenger:</em> <br/>\n\nTom, something else that would be good to see is the output of 'condor_q -l' on one of the \"orphaned\" node jobs.\n\n<p></p><hr/>\n<em>2016-Jul-06 14:06:20 by tpdownes:</em> <br/>\n\nOperating on assumption that most recent job in history is an orphan I manually condor_rm'd as root. This user has no other jobs in any queue in the pool so it's a reasonable assumption.\n\n<p></p><div class=\"verbatim\">\n<pre>[pcdev3:~] condor_history -match 1 -long gstlalcbc\nRemoteSysCpu = 2.0\nMachineAttrCpus1 = 1\nJobCurrentStartDate = 1467830851\nNumJobStarts = 1\nMemoryUsage = ( ( ResidentSetSize + 1023 ) / 1024 )\nLastSuspensionTime = 0\nJobCurrentStartExecutingDate = 1467830851\nMachineAttrcpuinfo_model1 = \"45\"\nNumShadowStarts = 2\nRemoteUserCpu = 780.0\nLastRejMatchTime = 1467830850\nRemoveReason = \"via condor_rm (by user condor)\"\nLastRejMatchReason = \"no match found \"\nCumulativeSlotTime = 790.0\nRemoteWallClockTime = 790.0\nImageSize_RAW = 881292\nLastVacateTime = 1467831640\nBytesSent = 0.0\nJobRunCount = 2\nJobStartDate = 1467830644\nJobStatus = 3\nGlobalJobId = \"pcdev3.nemo.phys.uwm.edu#4742069.0#1467830615\"\nProcId = 0\nLastRemoteHost = \"slot2_1@execute1010.nemo.phys.uwm.edu\"\nEnteredCurrentStatus = 1467831640\nMachineAttrSlotWeight0 = 1\nMachineAttrSlotWeight1 = 1\nAutoClusterId = 188\nMachineAttrNUMAD_OK1 = false\nCurrentHosts = 0\nMaxHosts = 1\nAutoClusterAttrs = \"_condor_RequestCpus,_condor_RequestDisk,_condor_RequestMemory,JobUniverse,LastCheckpointPlatform,NumCkpts,Owner,RequestCpus,RequestDisk,RequestMemory,ICC_OK,MachineLastMatchTime,LigoSearchTag,_condor_RequestGPUs,RequestGPUs,ConcurrencyLimits,NiceUser,Rank,Requirements,INSTRUMENT_DATA_OK,DiskUsage,FileSystemDomain\"\nImageSize = 1000000\nResidentSetSize_RAW = 881292\nLastJobLeaseRenewal = 1467831639\nLastMatchTime = 1467830851\nLastJobStatus = 2\nOrigMaxHosts = 1\nJobLastStartDate = 1467830644\nMachineAttrCpus0 = 1\nStartdPrincipal = \"execute-side@matchsession/192.168.5.110\"\nLastPublicClaimId = \"&lt;192.168.5.110:9618&gt;#1467129804#5403#...\"\nNumJobMatches = 2\nMachineAttrcpuinfo_model0 = \"45\"\nResidentSetSize = 1000000\nJobFinishedHookDone = 1467831640\nDAGNodeName = \"gstlal_inspiral_calc_likelihood_inj_12EB\"\nCmd = \"/usr/bin/gstlal_inspiral_calc_likelihood\"\nCommittedSlotTime = 0\nTargetType = \"Machine\"\nQDate = 1467830614\nMachineAttrNUMAD_OK0 = false\nUserLog = \"/localscratch/gstlalcbc/tmp1GEoG7\"\nOut = \"logs/gstlal_inspiral_calc_likelihood_inj_12EB-4742069-0.out\"\nJobNotification = 0\nCompletionDate = 0\nNumSystemHolds = 0\nEnvDelim = \";\"\nExitStatus = 0\nNumCkpts = 0\nStreamOut = false\nDiskUsage_RAW = 13\nBytesRecvd = 0.0\nTotalSuspensions = 0\nWhenToTransferOutput = \"ON_EXIT\"\nCommittedSuspensionTime = 0\nPeriodicHold = false\nErr = \"logs/gstlal_inspiral_calc_likelihood_inj_12EB-4742069-0.err\"\nCommittedTime = 0\nStreamErr = false\nOnExitRemove = true\nDAGManJobId = 4735974\nFileSystemDomain = \"nemo.phys.uwm.edu\"\nMyType = \"Job\"\nWantCheckpoint = false\nMinHosts = 1\nRequestDisk = DiskUsage\nJobBatchName = \"trigger_pipe.dag+4735974\"\nNumRestarts = 0\nSubmitEventNotes = \"DAG Node: gstlal_inspiral_calc_likelihood_inj_12EB\"\nExecutableSize = 15\nDAGManNodesMask = \"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27\"\nClusterId = 4742069\nLeaveJobInQueue = false\nJobLeaseDuration = 2400\nOnExitHold = false\nPeriodicRemove = false\nLocalUserCpu = 0.0\nPeriodicRelease = false\nExecutableSize_RAW = 13\nNumCkpts_RAW = 0\nLocalSysCpu = 0.0\nShouldTransferFiles = \"IF_NEEDED\"\nEncryptExecuteDirectory = false\nBufferBlockSize = 32768\nBufferSize = 524288\nUser = \"gstlalcbc@nemo.phys.uwm.edu\"\nCoreSize = 0\nTransferIn = false\nOwner = \"gstlalcbc\"\nLigoSearchTag = \"ligo.prod.o1.cbc.bbh.gstlaloffline\"\nLigoSearchUser = \"debnandini.mukherjee\"\nRequestMemory = 2048\nCondorPlatform = \"$CondorPlatform: x86_64_Debian7 $\"\nExitBySignal = false\nCondorVersion = \"$CondorVersion: 8.5.5 Jun 03 2016 BuildID: 369308 $\"\nIwd = \"/home/gstlalcbc-work/uber_allinj_injrun\"\nCumulativeSuspensionTime = 0\nRequirements = ( TARGET.Arch == \"X86_64\" ) &amp;&amp; ( TARGET.OpSys == \"LINUX\" ) &amp;&amp; ( TARGET.Disk &gt;= RequestDisk ) &amp;&amp; ( TARGET.Memory &gt;= RequestMemory ) &amp;&amp; ( ( TARGET.HasFileTransfer ) || ( TARGET.FileSystemDomain == MY.FileSystemDomain ) )\nRank = 0.0\nNiceUser = false\nRootDir = \"/\"\nJobUniverse = 5\nKeepClaimIdle = 20\nDiskUsage = 15\nArguments = \"--likelihood-cache gstlal_inspiral_calc_likelihood_inj/likelihood_4843.cache --tmp-space _CONDOR_SCRATCH_DIR gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1126954463-9976.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1126987331-4830.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1126994090-10699.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127004179-10699.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127014268-10699.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127024357-10699.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127034446-10699.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127044535-10699.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127054624-10699.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127064713-10087.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127087510-5298.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127094122-6590.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127100102-6590.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127106082-5980.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127118437-5782.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127123609-5782.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127128781-5171.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127165805-6814.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127174758-8923.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127183071-8923.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127191384-8923.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127199697-8923.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127208010-8923.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127216323-8310.xml.gz gstlal_inspiral_inj/H1L1-236_LLOID_HL_INJECTIONS_BBH1_1126051217_1129383017_1126051217-1127233877-8393.xml.gz\"\nTransferInputSizeMB = 0\nWantRemoteIO = true\nWantRemoteSyscalls = false\nIn = \"/dev/null\"\nRequestCpus = 1\nJobPrio = 23\nEnv = \"_LMFILES_=/home/gstlalcbc/modules/modulefiles/gstlal-opt-icc;_condor_SCHEDD_ADDRESS_FILE=/var/lib/condor/spool/.schedd_address;TMPDIR=/localscratch/gstlalcbc;NLSPATH=/opt/intel/composer_xe_2015.5.223/compiler/lib/intel64/locale/%l_%t/%N:/opt/intel/composer_xe_2015.5.223/ipp/lib/intel64/locale/%l_%t/%N:/opt/intel/composer_xe_2015.5.223/mkl/lib/intel64/locale/%l_%t/%N:/opt/intel/composer_xe_2015.5.223/debugger/gdb/intel64_mic/share/locale/%l_%t/%N:/opt/intel/composer_xe_2015.5.223/debugger/gdb/intel64/share/locale/%l_%t/%N;GST_PLUGIN_PATH=/home/gstlalcbc/modules/O1-icc/opt/lib/gstreamer-0.10:/home/gstlalcbc/modules/O1-icc/opt/lib64/gstreamer-0.10:/home/gstlalcbc/modules/O1-icc/dep/lib/gstreamer-0.10:/home/gstlalcbc/modules/O1-icc/dep/lib64/gstreamer-0.10;CXX=icpc;INTEL_LICENSE_FILE=/opt/intel/composer_xe_2015.5.223/licenses:/opt/intel/licenses:/home/gstlalcbc/intel/licenses;SHLVL=3;TBBROOT=/opt/intel/composer_xe_2015.5.223/tbb;PWD=/home/gstlalcbc-work/uber_allinj_injrun;SSH_AUTH_SOCK=/tmp/ssh-OxsBFLUL5weO/agent.3267560;SSH_CLIENT=10.22.40.172 55369 22;GST_REGISTRY_UPDATE=no;_CONDOR_ANCESTOR_4890=3293288:1467752873:378893798;LIBRARY_PATH=/opt/intel/composer_xe_2015.5.223/compiler/lib/intel64:/opt/intel/composer_xe_2015.5.223/ipp/../compiler/lib/intel64:/opt/intel/composer_xe_2015.5.223/ipp/lib/intel64:/opt/intel/composer_xe_2015.5.223/compiler/lib/intel64:/opt/intel/composer_xe_2015.5.223/mkl/lib/intel64:/opt/intel/composer_xe_2015.5.223/tbb/lib/intel64/gcc4.4;PATH=/home/gstlalcbc/modules/O1-icc/opt/bin:/home/gstlalcbc/modules/O1-icc/dep/bin:/opt/intel/composer_xe_2015.5.223/bin/intel64:/opt/intel/composer_xe_2015.5.223/debugger/gdb/intel64_mic/bin:/opt/lscsoft/gst/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/var/cfengine/bin;MODULESHOME=/usr/share/modules;INFOPATH=/opt/intel/composer_xe_2015.5.223/debugger/gdb/intel64/share/info/:/opt/intel/composer_xe_2015.5.223/debugger/gdb/intel64_mic/share/info/;CFLAGS=-O3 -march=native -lfftw3f -lfftw3 -lmkl_core -lmkl_intel_lp64 -lmkl_sequential -lpthread -Wno-write-strings;S6_SEGMENT_SERVER=https://segdb.ligo.caltech.edu;GDB_HOST=/opt/intel/composer_xe_2015.5.223/debugger/gdb/intel64_mic/bin/gdb-ia-mic;GLOBUS_TCP_PORT_RANGE=40000,45000;INTEL_PYTHONHOME=/opt/intel/composer_xe_2015.5.223/debugger/python/intel64/;LIGO_DATAFIND_SERVER=nemo-dataserver1.nemo.phys.uwm.edu:80;IPPROOT=/opt/intel/composer_xe_2015.5.223/ipp;ONLINEDQ=/online/DQ;ONLINEHOFT=/online/frames/hoft;MKLROOT=/opt/intel/composer_xe_2015.5.223/mkl;LAL_PATH=/home/gstlalcbc/modules/O1-icc/opt;PKG_CONFIG_PATH=/home/gstlalcbc/modules/O1-icc/opt/lib/pkgconfig:/home/gstlalcbc/modules/O1-icc/opt/lib64/pkgconfig:/home/gstlalcbc/modules/O1-icc/dep/lib/pkgconfig:/home/gstlalcbc/modules/O1-icc/dep/lib64/pkgconfig:/opt/lscsoft/gst/lib/pkgconfig:;SSH_AGENT_PID=3267561;MIC_LIBRARY_PATH=/opt/intel/composer_xe_2015.5.223/compiler/lib/mic:/opt/intel/composer_xe_2015.5.223/mpirt/lib/mic:/opt/intel/composer_xe_2015.5.223/tbb/lib/mic;GDB_CROSS=/opt/intel/composer_xe_2015.5.223/debugger/gdb/intel64_mic/bin/gdb-mic;SSH_TTY=/dev/pts/2;SHELL=/bin/bash;MODULE_VERSION_STACK=3.2.9;MPM_LAUNCHER=/opt/intel/composer_xe_2015.5.223/debugger/mpm/bin/start_mpm.sh;MAIL=/var/mail/gstlalcbc;_CONDOR_MAX_DAGMAN_LOG=0;MANPATH=/opt/intel/composer_xe_2015.5.223/man/en_US:/opt/intel/composer_xe_2015.5.223/man/en_US:/opt/intel/composer_xe_2015.5.223/debugger/gdb/intel64/share/man/:/opt/intel/composer_xe_2015.5.223/debugger/gdb/intel64_mic/share/man/::/var/cfengine/share/man:;DEPENDENCIES_PATH=/home/gstlalcbc/modules/O1-icc/dep;_condor_SCHEDD_DAEMON_AD_FILE=/var/lib/condor/spool/.schedd_classad;_CONDOR_SCHEDD_ADDRESS_FILE=/var/lib/condor/spool/.schedd_address;LOADEDMODULES=gstlal-opt-icc;XDG_SESSION_COOKIE=88fc5b0c71f25316ed5c056f5449cdea-1467750269.836112-1175070150;CPATH=/opt/intel/composer_xe_2015.5.223/ipp/include:/opt/intel/composer_xe_2015.5.223/mkl/include:/opt/intel/composer_xe_2015.5.223/tbb/include;USER=gstlalcbc;SSH_CONNECTION=10.22.40.172 55369 129.89.61.93 22;LD_LIBRARY_PATH=/opt/intel/composer_xe_2015.5.223/compiler/lib/intel64:/opt/intel/composer_xe_2015.5.223/mpirt/lib/intel64:/opt/intel/composer_xe_2015.5.223/ipp/../compiler/lib/intel64:/opt/intel/composer_xe_2015.5.223/ipp/lib/intel64:/opt/intel/composer_xe_2015.5.223/ipp/tools/intel64/perfsys:/opt/intel/composer_xe_2015.5.223/compiler/lib/intel64:/opt/intel/composer_xe_2015.5.223/mkl/lib/intel64:/opt/intel/composer_xe_2015.5.223/tbb/lib/intel64/gcc4.4:/opt/intel/composer_xe_2015.5.223/debugger/libipt/intel64/lib;PYTHONPATH=/home/gstlalcbc/modules/O1-icc/opt/lib/python2.7/site-packages:/home/gstlalcbc/modules/O1-icc/opt/lib64/python2.7/site-packages:/home/gstlalcbc/modules/O1-icc/dep/lib/python2.7/site-packages:/home/gstlalcbc/modules/O1-icc/dep/lib64/python2.7/site-packages;MIC_LD_LIBRARY_PATH=/opt/intel/composer_xe_2015.5.223/compiler/lib/mic:/opt/intel/composer_xe_2015.5.223/mpirt/lib/mic:/opt/intel/composer_xe_2015.5.223/compiler/lib/mic:/opt/intel/composer_xe_2015.5.223/mkl/lib/mic:/opt/intel/composer_xe_2015.5.223/tbb/lib/mic;LDFLAGS=-L/opt/intel/composer_xe_2015.5.223/mkl/lib/intel64;INSTALL_FILES_PATH=/home/gstlalcbc/modules/O1-icc/src;HOME=/home/gstlalcbc;LOGNAME=gstlalcbc;MODULEPATH=/home/gstlalcbc/modules/modulefiles:/usr/share/modules/versions:/usr/Modules/$MODULE_VERSION/modulefiles:/usr/share/modules/modulefiles;GDBSERVER_MIC=/opt/intel/composer_xe_2015.5.223/debugger/gdb/target/mic/bin/gdbserver;GST_REGISTRY=/home/gstlalcbc/modules/O1-icc/opt/registry..bin;_=/usr/bin/condor_submit;CONDOR_ID=4735974.0;_CONDOR_SCHEDD_DAEMON_AD_FILE=/var/lib/condor/spool/.schedd_classad;CONDOR_PARENT_ID=pcdev3:3293288:1467752873;_CONDOR_JOB_AD=/var/lib/condor/spool/local_univ_execute/dir_4735974_0/.job.ad;CONDOR_LOCATION=/usr;_CONDOR_ANCESTOR_4769=4890:1467129871:2235137320;TERM=xterm-256color;CC=icc;MODULE_VERSION=3.2.9;X509_USER_PROXY=/tmp/x509up_p3267536.fileVpvTzn.1;_CONDOR_DAGMAN_LOG=/home/gstlalcbc-work/uber_allinj_injrun/trigger_pipe.dag.dagman.out\"\nDAGManNodesLog = \"/home/gstlalcbc-work/uber_allinj_injrun/./trigger_pipe.dag.nodes.log\"\n</pre></div>\n\n\n<p></p><hr/>\n<em>2016-Jul-06 14:07:12 by wenger:</em> <br/>\n\nMaybe when DAGMan gets condor_rm'ed, it should watch for terminated events from all of its jobs, and, if it hasn't seen them within a certain timeout, it should try to remove the jobs itself (and then maybe wait some more for the appropriate terminated events?).\n\n<p>One issue with this is what to do if you want to remove the DAGMan job <strong>immediately</strong>.  Maybe 'condor_rm -forcex'?\n\n</p><p></p><hr/>\n<em>2016-Jul-06 14:13:23 by tpdownes:</em> <br/>\n\nMy only goal is a \"one stop shop\" for guaranteed clean removal of a DAG even/especially under poor filesystem conditions.\n\n<p></p><hr/>\n<em>2016-Jul-08 10:21:51 by wenger:</em> <br/>\n\nLogs from Tom available here: <a class=\"external\" href=\"http://pages.cs.wisc.edu/~wenger/condor/trigger_pipe.dag.dagman.xz\">http://pages.cs.wisc.edu/~wenger/condor/trigger_pipe.dag.dagman.xz</a> (too large to attach directly to ticket).\n\n<p></p><hr/>\n<em>2016-Jul-08 11:17:13 by wenger:</em> <br/>\n\nThe \"orphan\" job above was submitted at or before 07/06/16 13:43:35; DAGMan got the SIGUSR1 at 07/06/16 13:44:56.  So the problem isn't in DAGMan itself.\n\n<p>Okay, here's a line from the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=SchedLog\" title=\"Sched Log\">SchedLog</a></span>:\n\n</p><p></p><pre>  07/06/16 13:41:32 (pid:4890) Constraint &lt;OtherJobRemoveRequirements = DAGManJobId =?= 4735974&gt; fired because job (4735974.0) was removed\n</pre>\n\n<p>So the schedd was starting to remove the node jobs at least by 13:41:32, but didn't actually kill DAGMan until 13:44:56.  Oops!\n\n</p><p>Also from the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=SchedLog\" title=\"Sched Log\">SchedLog</a></span>:\n\n</p><p></p><pre>  07/06/16 13:45:13 (pid:4890) scheduler universe job (4735974.0) pid 3293288 exited with status 2\n</pre>\n\n<p>So clearly the code in the schedd that removes the node jobs is firing <strong>before</strong> DAGMan actually gets killed.  Maybe most of the time the delay is short enough to not be a problem -- but in this case there is clearly several minutes of delay, and if DAGMan is submitting jobs that's clearly a problem...\n\n</p><p></p><hr/>\n<em>2016-Jul-08 14:12:03 by tpdownes:</em> <br/>\n\nFor this, I would run condor_rm on the DAGMan job as root. Maybe the root/SOS business kicks in? Anyhow, sounds like you're on the right track.\n\n<p></p><hr/>\n<em>2016-Jul-11 13:52:32 by wenger:</em> <br/>\n\nJust discussed this in the flightworthy meeting:  ToddT proposes having the node job removal rule fire in the reaper for the actual DAGMan process.  We'd also have to deal with the possibility of the schedd crashing before removing the node jobs -- one possibility is having the schedd remove \"orphan\" jobs at startup (look at DAGMan ID in a job's ad, remove the job if the DAGMan doesn't exist).  Note that that could get tricky for nested DAGs -- for lower-level node jobs, their immediate parent DAGMan might still be in the queue even if the top-level one isn't.\n\n<p>Also, there were other reasons that I was thinking about having DAGMan wait for its node jobs to terminate before actually quitting.  (See <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=3995\" onclick=\"get_ticket_and_populate_wrapper('3995'); return false;\" title=\"Condor_rm'ed DAGMan waits for node jobs to exit\">#3995</a></span>: Condor_rm'ed DAGMan waits for node jobs to exit, <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=4628\" onclick=\"get_ticket_and_populate_wrapper('4628'); return false;\" title=\"Should condor_rm'ed DAGMan wait for nodes to exit before FINAL runs?\">#4628</a></span>: Should condor_rm'ed DAGMan wait for nodes to exit before FINAL runs?.)  I think one case for this is ABORT-DAG-ON -- in that case, the schedd isn't involved at all in the node job removal.\n\n</p><p></p><hr/>\n<em>2016-Jul-11 16:06:25 by wenger:</em> <br/>\n\nHmm -- it looks like the code in the schedd kills the DAGMan process before removing the node jobs, so this isn't as simple as I though.\n\n<p>If DAGMan is in the middle of a submit cycle, does it not see the remove signal until it gets to the end of that cycle?  But in the example from LIGO (see above) the delay was really long -- DAGMan received the SIGUSR1 at 13:44:56, but the schedd started removing the node jobs at 13:41:32.\n\n</p><p>Okay, looking at the dagman.out file, it's looking like one submit cycle for DAGMan was taking around five minutes(!!), so I think that's the problem.  (I'm pretty sure that DAGMan doesn't see the SIGUSR1 until a submit cycle finishes and it goes back to daemoncore.)\n\n</p><p>Maybe set DAGMAN_SUBMIT_DELAY to a really high value to try to reproduce this?\n\n</p><p>Should we have some kind of timeout on the DAGMan submit cycle where it goes back to daemoncore before submitting DAGMAN_MAX_SUBMITS_PER_INTERVAL jobs, if that's taking too long?\n\n</p><p></p><hr/>\n<em>2016-Jul-11 16:10:51 by wenger:</em> <br/>\n\nYeah, part of the problem here is that a single condor_submit, at one point, took about a minute...\n\n<p></p><hr/>\n<em>2016-Jul-13 12:40:49 by wenger:</em> <br/>\n\nOkay, I was able to reproduce this with a DAG that has a bunch of sibling node, and the following config:\n\n<p></p><pre>  DAGMAN_MAX_SUBMITS_PER_INTERVAL = 100\n  DAGMAN_SUBMIT_DELAY = 10\n</pre>\n\n<p>I just fired up the DAG, and condor_rm'ed it, and eventually ended up with a bunch of \"orphan\" jobs.\n\n</p><p>So I think that part of the solution is that, if the submit cycle has lasted longer than DAGMAN_USER_LOG_SCAN_INTERVAL, we should go back to daemoncore even if there are more jobs ready and we haven't yet submitted DAGMAN_MAX_SUBMITS_PER_INTERVAL in the given submit cycle.\n\n</p><p></p><hr/>\n<em>2016-Jul-14 13:28:12 by wenger:</em> <br/>\n\nHmm -- I wonder if we should periodically go back to daemoncore while reading events in recovery mode.  Getting removed in that state obviously wouldn't result in orphan jobs, but if you're recovering a really big DAG, DAGMan could end up being in the X state for a long time...\n\n<p></p><hr/>\n<em>2016-Jul-20 13:46:43 by wenger:</em> <br/>\n\nAt this point, I have a partial implementation in place of the \"go back to daemoncore more often\" idea.  This will reduce, but not totally solve, the problem.\n\n<p>ToddT wants the schedd to remove the node jobs in the reaper for the DAGMan process, so that we're sure to get them all.  Something that would be quicker to implement would be having a config knob telling DAGMan to remove node jobs itself (see <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=5175\" onclick=\"get_ticket_and_populate_wrapper('5175'); return false;\" title=\"Have condor_dagman condor_rm jobs on SIGUSR1 (revert #4618)\">#5175</a></span>).  We probably don't want that as the complete solution for performance reasons, but it's more likely to make it into 8.5.7...\n\n</p><p></p><hr/>\n<em>2016-Jul-21 16:19:34 by wenger:</em> <br/>\n\nHmm -- when we remove node jobs in DAGMan (which we need to do some of the time, at least, even if the schedd removes them when we condor_rm DAGMan itself), I wonder if it would be more efficient for DAGMan to pass an explicit list of HTCondor IDs to condor_rm, rather than using the constraint of matching the parent DAGMan ID.  (I'm assuming that the schedd has things indexed by HTCondor ID, but has to scan every job for the parent DAGMan ID constraint.)\n\n<p>Assuming we do this, I think we'd be better off passing a long list of HTCondor IDs to a single condor_rm command (or a few, if necessary) rather than running a condor_rm command for each ID.\n\n</p><p></p><hr/>\n<em>2016-Aug-17 11:10:58 by wenger:</em> <br/>\n\nJust had some discussion about this with Greg and TJ, and the conclusion was that we should keep the constraint for removing node jobs (as opposed to passing a list of specific IDs).\n\n<p>Also, be sure to log condor_rm output to the dagman.out file.\n\n</p><p></p><hr/>\n<em>2016-Aug-19 11:50:36 by wenger:</em> <br/>\n\nHmm -- just thought of something:  DAGMan may not count a job as being queued until it sees the submit event (as opposed to having run condor_submit).  So if Tom saw problems with the old scheme of DAGMan removing its own node jobs, that might be the problem, especially when we removed jobs explicitly by ID instead of the current method of the parent DAGMan ID constraint.\n\n<p>So maybe that means we should do the remove even if we don't think there are any child jobs...\n\n</p><p></p><hr/>\n<em>2016-Aug-24 10:58:28 by wenger:</em> <br/>\n\nHmm -- right now I have the config knob for DAGMan removing its own node jobs defaulting to false -- I'm thinking that maybe it should default to true, and people can set it to false if performance is really a big issue...\n\n<p></p><hr/>\n<em>2016-Oct-11 16:47:19 by wenger:</em> <br/>\n\nJust did a build/test of V8_5-gittrac_5702-branch and everything passed (except for one failure of an unrelated test on Windows).</blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body><blockquote>\n<ul>\n<li><a href=\"../files/953/sleep_dag.tgz\">sleep_dag.tgz</a>\n371 bytes added by wenger on 2016-Jul-13 17:42:33 UTC.\n<br/>\nDAG to reproduce the problem.<br/>\n</li><li><a href=\"../files/969/gt5702.dif\">gt5702.dif</a>\n14274 bytes added by wenger on 2016-Oct-11 22:47:08 UTC.\n<br/>\nHere's a diff of the whole V8_5-gittrac_5702-branch.<br/>\n</li></ul>\n</blockquote></body></html>", "check_ins": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Oct-11 17:00</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/95b734042d3ad2848fb755c3d59aef50a03c04b4\">[49405]</a></span>: Gittrac <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5702\" onclick=\"get_ticket_and_populate_wrapper('5702'); return false;\" title=\"condor_rm on scheduler universe job does not halt submission\">#5702</a></span>: Oops -- deleting some debug code in the schedd.  (By Kent Wenger )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Oct-11 13:12</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/9286e9b1774b195c82c596f2cfb5db8ff269d189\">[49402]</a></span>: Gittrac <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5702\" onclick=\"get_ticket_and_populate_wrapper('5702'); return false;\" title=\"condor_rm on scheduler universe job does not halt submission\">#5702</a></span>: (Possibly final) cleanup.  (By Kent Wenger )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Oct-11 12:54</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/df780cefffb253c685a9e65ed7ee1058a5d1a4d1\">[49401]</a></span>: Gittrac <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5702\" onclick=\"get_ticket_and_populate_wrapper('5702'); return false;\" title=\"condor_rm on scheduler universe job does not halt submission\">#5702</a></span>: Oops -- forgot to add the config file for the test.  (By Kent Wenger )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Oct-11 11:44</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/fdff2bf07d1e23787da6e9921faa8c887360be81\">[49400]</a></span>: Gittrac <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5702\" onclick=\"get_ticket_and_populate_wrapper('5702'); return false;\" title=\"condor_rm on scheduler universe job does not halt submission\">#5702</a></span>: Changed DAGMAN_REMOVE_NODE_JOBS default to True.  (By Kent Wenger )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Oct-11 10:52</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/181faf797fcd33c626d62ddb7a5a5fc0c4ecf410\">[49399]</a></span>: Gittrac <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5702\" onclick=\"get_ticket_and_populate_wrapper('5702'); return false;\" title=\"condor_rm on scheduler universe job does not halt submission\">#5702</a></span>: Various cleanups, especially to the test.  (By Kent Wenger )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Aug-21 21:43</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/8ca454f84aa43584ff1962861a088392a62a7b27\">[49049]</a></span>: Gittrac <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5702\" onclick=\"get_ticket_and_populate_wrapper('5702'); return false;\" title=\"condor_rm on scheduler universe job does not halt submission\">#5702</a></span>: job_dagman_rm test now fails if orphan jobs are left in the queue -- not sure if this is the best way to check for them, though.  (By Kent Wenger )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Aug-17 14:25</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/89b5f1536a920dbab08cdceffc79c8babf572d86\">[49026]</a></span>: Gittrac <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5702\" onclick=\"get_ticket_and_populate_wrapper('5702'); return false;\" title=\"condor_rm on scheduler universe job does not halt submission\">#5702</a></span> / <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=5175\" onclick=\"get_ticket_and_populate_wrapper('5175'); return false;\" title=\"Have condor_dagman condor_rm jobs on SIGUSR1 (revert #4618)\">#5175</a></span>: Added new DAGMAN_REMOVE_NODE_JOBS config knob that allows users to enable the old behavior of DAGMan removing its node jobs itself when it is condor_rm'ed.  (By Kent Wenger )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Jul-29 09:53</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/7d845e95b69e6bf9897f897d41cccc19ab659917\">[48873]</a></span>: Gittrac <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5702\" onclick=\"get_ticket_and_populate_wrapper('5702'); return false;\" title=\"condor_rm on scheduler universe job does not halt submission\">#5702</a></span>: Committing temporary changes to job_dagman_rm.  (By Kent Wenger )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Jul-14 13:42</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/b7ef9329ad6b1ea7e1c80dab6bb372861a10a2bf\">[48789]</a></span>: Gittrac <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5702\" onclick=\"get_ticket_and_populate_wrapper('5702'); return false;\" title=\"condor_rm on scheduler universe job does not halt submission\">#5702</a></span>: Cleaned up the \"bail out of the submit loop early\" code a bit.  (By Kent Wenger )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Jul-13 16:02</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/e1b55bf30fed7c6711d88b3696f75ce334be32e8\">[48782]</a></span>: Gittrac <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5702\" onclick=\"get_ticket_and_populate_wrapper('5702'); return false;\" title=\"condor_rm on scheduler universe job does not halt submission\">#5702</a></span>: Preliminary code to bail out of submit cycle early if it's taking too long. This doesn't totally fix the problem, but at least it <strong>reduces</strong> the number of orphan jobs you'll end up with.  (By Kent Wenger )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Jul-13 12:57</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/bb39ccd0826c098504f8bcbd5513a37de345de03\">[48779]</a></span>: Gittrac <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5702\" onclick=\"get_ticket_and_populate_wrapper('5702'); return false;\" title=\"condor_rm on scheduler universe job does not halt submission\">#5702</a></span>: A bit of diagnostic code -- no actual changes yet.  (By Kent Wenger )</td></tr>\n</tbody></table>", "type": "defect", "last_change": "2016-Oct-12 15:35", "status": "defer", "created": "2016-May-31 15:00", "fixed_version": "2016-May-31 15:00", "broken_version": "v080503", "priority": "5", "subsystem": "Dag", "assigned_to": "wenger", "derived_from": "", "creator": "tpdownes", "rust": "", "customer_group": "ligo", "visibility": "public", "notify": "wenger@cs.wisc.edu, downes@uwm.edu, tannenba@cs.wisc.edu, zmiller@cs.wisc.edu,pcouvare@caltech.edu", "due_date": ""}
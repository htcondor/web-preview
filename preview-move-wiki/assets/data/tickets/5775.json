{"id": 5775, "title": "Ticket #5775: OOM killed all jobs in htcondor cgroup rather than offending job", "description": "<blockquote>\nBelow are a bunch of logs from Tom at LIGO showing strange behavior with cgroups, where jobs that are well below there memory limit were being targeted by the OOM and thus put on hold by HTCondor.  After investigation, it turns out that if memory use_hierarchy is 1, then hitting the limit at the parent notifies all children, even if those children are below their usage.  The upshot we think when LIGO put all of htcondor into a cgroup hierarchy, and some other process outside of the HTCondor hierarchy on the machine exhausted memory, the OOM typically picked a HTCondor job to be killed (because we set the OOM priorites that way, to ensure jobs get killed before HTConnor daemons).\n\n<p>Obviously having HTCondor put jobs on hold due to memory when the jobs were not using anything close to their requested_memory is undesired.  Our approach to improve this situation is to add a boolean config knob, <code>IGNORE_LEAF_OOM</code>.  If set to True, upon notification from the OOM, the starter will check how much memory the job is using.  If the job is using more memory than allocated, the job is killed and put on hold as before.  But if the job us using less memory than allocated, the starter will then ignore the notification from the OOM and continue to let the job run.  Hopefully, some other process will be killed, and when it does, the job will get unfrozen and continue to run.\n\n</p><p>As a next step, if this approach works well in testing, we should a) set IGNORE_LEAF_OOM to True by default in the param table, and b) setup a more reasonable default for MEMORY that understands HTCondor cannot allocate all physical RAM - some must be set aside for the system OS and/or a few typical system services that are common on execute nodes(like monitoring, puppet, etc).\n\n</p><p></p><div class=\"restricted\"><hr/></div><em>Reported by Tom from LIGO:</em>\n\n<p>It looks to me like 1 job greatly exceeded its memory request and took down a number of other jobs that were respecting their memory request.\n\n</p><p>Is this Condor-intended behavior? Is it a consequence of my use of memory.use_hierarchy - the only way I know of to cap total Condor consumption before it uses all of RAM? If so, do you know of a way of guiding the OOM killer to target offending jobs within a hierarchy first? This is the first I've noticed a problem like this so it might be the case that the offending job in slot1_6 was so offensive that the OOM killer started killing everything in sight.\n\n</p><p></p><div class=\"verbatim\">\n<pre>/usr/bin/cgcreate -g blkio,cpu,cpuacct,freezer,memory:htcondor -a root:root -t root:root\n/usr/bin/cgset -r cpu.shares=1000 htcondor\n/usr/bin/cgset -r memory.use_hierarchy=1 htcondor\n/usr/bin/cgset -r memory.limit_in_bytes=${CONDOR_MEMORY_LIMIT_RAM}M htcondor\n/usr/bin/cgset -r memory.memsw.limit_in_bytes=${CONDOR_MEMORY_LIMIT_SWAP_RAM}M htcondor\n/usr/bin/cgset -r memory.swappiness=0 htcondor\n\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:32 (pid:1682733) Job was held due to OOM event: Job has gone over memory limit of 7168 megabytes. Peak usage: 6020 megabytes.\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:32 (pid:1682733) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:32 (pid:1682733) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:32 (pid:1682733) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:32 (pid:1682733) condor_write(): Socket closed when trying to write 21 bytes to &lt;192.168.5.3:7426&gt;, fd is 9\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:32 (pid:1682733) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:32 (pid:1682733) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:32 (pid:1682733) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:32 (pid:1682733) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:32 (pid:1682733) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:33 (pid:1682733) Process exited, pid=1682737, signal=9\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:33 (pid:1682733) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_1:07/01/16 09:29:34 (pid:1682733) **** condor_starter (condor_STARTER) pid 1682733 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:32 (pid:1630684) Job was held due to OOM event: Job has gone over memory limit of 7168 megabytes. Peak usage: 5209 megabytes.\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:32 (pid:1630684) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:32 (pid:1630684) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:32 (pid:1630684) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:32 (pid:1630684) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:32 (pid:1630684) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:32 (pid:1630684) condor_write(): Socket closed when trying to write 21 bytes to &lt;192.168.5.3:15141&gt;, fd is 9\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:32 (pid:1630684) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:32 (pid:1630684) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:32 (pid:1630684) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:33 (pid:1630684) Process exited, pid=1630688, signal=9\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:33 (pid:1630684) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_2:07/01/16 09:29:37 (pid:1630684) **** condor_starter (condor_STARTER) pid 1630684 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:32 (pid:1492741) Job was held due to OOM event: Job has gone over memory limit of 7168 megabytes. Peak usage: 5216 megabytes.\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:32 (pid:1492741) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:32 (pid:1492741) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:32 (pid:1492741) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:32 (pid:1492741) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:32 (pid:1492741) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:32 (pid:1492741) condor_write(): Socket closed when trying to write 21 bytes to &lt;192.168.5.3:19761&gt;, fd is 9\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:32 (pid:1492741) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:32 (pid:1492741) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:32 (pid:1492741) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:33 (pid:1492741) Process exited, pid=1492745, signal=9\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:33 (pid:1492741) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_3:07/01/16 09:29:36 (pid:1492741) **** condor_starter (condor_STARTER) pid 1492741 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:32 (pid:1581517) Job was held due to OOM event: Job has gone over memory limit of 7168 megabytes. Peak usage: 5004 megabytes.\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:32 (pid:1581517) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:32 (pid:1581517) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:32 (pid:1581517) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:32 (pid:1581517) condor_write(): Socket closed when trying to write 21 bytes to &lt;192.168.5.3:28808&gt;, fd is 9\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:32 (pid:1581517) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:32 (pid:1581517) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:32 (pid:1581517) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:32 (pid:1581517) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:32 (pid:1581517) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:33 (pid:1581517) Process exited, pid=1581521, signal=9\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:33 (pid:1581517) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_4:07/01/16 09:29:36 (pid:1581517) **** condor_starter (condor_STARTER) pid 1581517 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:32 (pid:1663061) Job was held due to OOM event: Job has gone over memory limit of 7168 megabytes. Peak usage: 5543 megabytes.\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:32 (pid:1663061) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:32 (pid:1663061) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:32 (pid:1663061) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:32 (pid:1663061) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:32 (pid:1663061) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:32 (pid:1663061) condor_write(): Socket closed when trying to write 21 bytes to &lt;192.168.5.3:2254&gt;, fd is 9\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:32 (pid:1663061) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:32 (pid:1663061) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:32 (pid:1663061) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:33 (pid:1663061) Process exited, pid=1663084, signal=9\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:33 (pid:1663061) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_5:07/01/16 09:29:37 (pid:1663061) **** condor_starter (condor_STARTER) pid 1663061 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:32 (pid:1676960) Job was held due to OOM event: Job has gone over memory limit of 2048 megabytes. Peak usage: 17469 megabytes.\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:32 (pid:1676960) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:32 (pid:1676960) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:32 (pid:1676960) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:32 (pid:1676960) condor_write(): Socket closed when trying to write 21 bytes to &lt;192.168.5.1:61194&gt;, fd is 9\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:32 (pid:1676960) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:32 (pid:1676960) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:32 (pid:1676960) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:32 (pid:1676960) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:32 (pid:1676960) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:35 (pid:1676960) Process exited, pid=1676966, signal=9\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:35 (pid:1676960) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_6:07/01/16 09:29:35 (pid:1676960) **** condor_starter (condor_STARTER) pid 1676960 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) Job was held due to OOM event: Job has gone over memory limit of 2048 megabytes. Peak usage: 523 megabytes.\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) condor_write(): Socket closed when trying to write 21 bytes to &lt;192.168.5.2:63099&gt;, fd is 8\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) Process exited, pid=29235, signal=9\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_7:07/01/16 09:29:32 (pid:29234) **** condor_starter (condor_STARTER) pid 29234 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) Job was held due to OOM event: Job has gone over memory limit of 2048 megabytes. Peak usage: 517 megabytes.\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) condor_write(): Socket closed when trying to write 21 bytes to &lt;192.168.5.2:52476&gt;, fd is 8\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) Process exited, pid=29239, signal=9\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_8:07/01/16 09:29:32 (pid:29238) **** condor_starter (condor_STARTER) pid 29238 EXITING WITH STATUS 0\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) Job was held due to OOM event: Job has gone over memory limit of 2048 megabytes. Peak usage: 81 megabytes.\n\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) condor_write(): Socket closed when trying to write 21 bytes to &lt;192.168.5.1:14643&gt;, fd is 9\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) Process exited, pid=1679388, signal=9\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_9:07/01/16 09:29:32 (pid:1679387) **** condor_starter (condor_STARTER) pid 1679387 EXITING WITH STATUS 0\n</pre></div>\n</blockquote>", "remarks": "<blockquote>\n<em>2016-Jul-09 19:50:01 by tpdownes:</em> <br/>\n\nThe process in slot1_8 appears to have triggered the OOM killer...\n\n<p></p><div class=\"verbatim\">\n<pre>Jul  1 09:29:32 execute0009 kernel: [253614.597847] lalinference_ne invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=800\nJul  1 09:29:32 execute0009 kernel: [253614.597852] lalinference_ne cpuset=/ mems_allowed=0-1\nJul  1 09:29:32 execute0009 kernel: [253614.597857] CPU: 9 PID: 29239 Comm: lalinference_ne Tainted: P           O  3.16.0-0.bpo.4-amd64 #1 Debian 3.16.7-ckt25-2~bpo70+1\nJul  1 09:29:32 execute0009 kernel: [253614.597859] Hardware name: Supermicro X8DTT-H/X8DTT-H, BIOS 080016  10/05/2010\nJul  1 09:29:32 execute0009 kernel: [253614.597861]  0000000000000286 0000000000000000 ffffffff81548b0d 0000000000000007\nJul  1 09:29:32 execute0009 kernel: [253614.597864]  ffff880995de2090 ffff88061ead9000 ffffffff81545fa7 ffff880c00000000\nJul  1 09:29:32 execute0009 kernel: [253614.597866]  ffffffff810a06bf ffff880995de25a8 0000000000000000 000000000000001b\nJul  1 09:29:32 execute0009 kernel: [253614.597869] Call Trace:\nJul  1 09:29:32 execute0009 kernel: [253614.597878]  [&lt;ffffffff81548b0d&gt;] ? dump_stack+0x5e/0x7a\nJul  1 09:29:32 execute0009 kernel: [253614.597881]  [&lt;ffffffff81545fa7&gt;] ? dump_header+0x76/0x1ec\nJul  1 09:29:32 execute0009 kernel: [253614.597885]  [&lt;ffffffff810a06bf&gt;] ? try_to_wake_up+0xcf/0x310\nJul  1 09:29:32 execute0009 kernel: [253614.597891]  [&lt;ffffffff81150f4c&gt;] ? find_lock_task_mm+0x4c/0xa0\nJul  1 09:29:32 execute0009 kernel: [253614.597893]  [&lt;ffffffff8115145a&gt;] ? oom_kill_process+0x28a/0x3e0\nJul  1 09:29:32 execute0009 kernel: [253614.597898]  [&lt;ffffffff810f1b65&gt;] ? css_next_descendant_pre+0x55/0x60\nJul  1 09:29:32 execute0009 kernel: [253614.597902]  [&lt;ffffffff811b176b&gt;] ? mem_cgroup_iter+0x13b/0x2f0\nJul  1 09:29:32 execute0009 kernel: [253614.597905]  [&lt;ffffffff811b564f&gt;] ? mem_cgroup_oom_synchronize+0x50f/0x580\nJul  1 09:29:32 execute0009 kernel: [253614.597908]  [&lt;ffffffff811b4b30&gt;] ? mem_cgroup_try_charge_mm+0xb0/0xb0\nJul  1 09:29:32 execute0009 kernel: [253614.597912]  [&lt;ffffffff810a68ab&gt;] ? update_curr+0x1ab/0x1b0\nJul  1 09:29:32 execute0009 kernel: [253614.597914]  [&lt;ffffffff81151ce0&gt;] ? pagefault_out_of_memory+0x10/0x80\nJul  1 09:29:32 execute0009 kernel: [253614.597918]  [&lt;ffffffff8105d1f6&gt;] ? __do_page_fault+0x536/0x540\nJul  1 09:29:32 execute0009 kernel: [253614.597920]  [&lt;ffffffff810a49ca&gt;] ? set_next_entity+0x3a/0x80\nJul  1 09:29:32 execute0009 kernel: [253614.597925]  [&lt;ffffffff81013760&gt;] ? __switch_to+0x280/0x5c0\nJul  1 09:29:32 execute0009 kernel: [253614.597930]  [&lt;ffffffff813725e0&gt;] ? ghes_print_estatus.constprop.12+0x80/0x80\nJul  1 09:29:32 execute0009 kernel: [253614.597934]  [&lt;ffffffff8154b30e&gt;] ? __schedule+0x2de/0x760\nJul  1 09:29:32 execute0009 kernel: [253614.597939]  [&lt;ffffffff810194d7&gt;] ? default_do_nmi+0x47/0x130\nJul  1 09:29:32 execute0009 kernel: [253614.597942]  [&lt;ffffffff81551128&gt;] ? page_fault+0x28/0x30\nJul  1 09:29:32 execute0009 kernel: [253614.597943] Task in /htcondor/condor_localscratch_condor_execute_slot1_6@execute0009.nemo.phys.uwm.edu killed as a result of limit of /htcondor\nJul  1 09:29:32 execute0009 kernel: [253614.597949] memory: usage 46077788kB, limit 49152000kB, failcnt 0\nJul  1 09:29:32 execute0009 kernel: [253614.597951] memory+swap: usage 49152000kB, limit 49152000kB, failcnt 880664\nJul  1 09:29:32 execute0009 kernel: [253614.597952] kmem: usage 0kB, limit 18014398509481983kB, failcnt 0\nJul  1 09:29:32 execute0009 kernel: [253614.597953] Memory cgroup stats for /htcondor: cache:8KB rss:0KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:4KB active_file:4KB unevictable:0KB\nJul  1 09:29:32 execute0009 kernel: [253614.597968] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_7@execute0009.nemo.phys.uwm.edu: cache:16KB rss:370604KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:179936KB inactive_anon:205016KB active_anon:165588KB inactive_file:0KB active_file:0KB unevictable:0KB\nJul  1 09:29:32 execute0009 kernel: [253614.597979] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_8@execute0009.nemo.phys.uwm.edu: cache:24KB rss:375912KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:166988KB inactive_anon:190684KB active_anon:185228KB inactive_file:0KB active_file:0KB unevictable:0KB\nJul  1 09:29:32 execute0009 kernel: [253614.597991] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_3@execute0009.nemo.phys.uwm.edu: cache:520KB rss:5315000KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:0KB inactive_anon:1324276KB active_anon:3990724KB inactive_file:352KB active_file:0KB unevictable:0KB\nJul  1 09:29:32 execute0009 kernel: [253614.598003] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_4@execute0009.nemo.phys.uwm.edu: cache:608KB rss:5044472KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:3160KB inactive_anon:1199232KB active_anon:3845240KB inactive_file:608KB active_file:0KB unevictable:0KB\nJul  1 09:29:32 execute0009 kernel: [253614.598014] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_2@execute0009.nemo.phys.uwm.edu: cache:408KB rss:5320820KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:6920KB inactive_anon:1219388KB active_anon:4101432KB inactive_file:280KB active_file:0KB unevictable:0KB\nJul  1 09:29:32 execute0009 kernel: [253614.598026] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_5@execute0009.nemo.phys.uwm.edu: cache:424KB rss:5527228KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:4652KB inactive_anon:1235200KB active_anon:4292028KB inactive_file:424KB active_file:0KB unevictable:0KB\nJul  1 09:29:32 execute0009 kernel: [253614.598036] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_6@execute0009.nemo.phys.uwm.edu: cache:12KB rss:17895764KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:2704604KB inactive_anon:2359328KB active_anon:15536436KB inactive_file:0KB active_file:0KB unevictable:0KB\nJul  1 09:29:32 execute0009 kernel: [253614.598048] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_9@execute0009.nemo.phys.uwm.edu: cache:0KB rss:61452KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:5212KB inactive_anon:40068KB active_anon:21384KB inactive_file:0KB active_file:0KB unevictable:0KB\nJul  1 09:29:32 execute0009 kernel: [253614.598060] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_1@execute0009.nemo.phys.uwm.edu: cache:72KB rss:6164444KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:2740KB inactive_anon:1314384KB active_anon:4850060KB inactive_file:60KB active_file:0KB unevictable:0KB\nJul  1 09:29:32 execute0009 kernel: [253614.598072] [ pid ]   uid  tgid total_vm      rss nr_ptes swapents oom_score_adj name\nJul  1 09:29:32 execute0009 kernel: [253614.598213] [29235]  1294 29235   159908    93036     311    45027           800 lalinference_ne\nJul  1 09:29:32 execute0009 kernel: [253614.598215] [29239]  1294 29239   157996    93780     311    42437           800 lalinference_ne\nJul  1 09:29:32 execute0009 kernel: [253614.598223] [1492745]   706 1492745  2066754  1323172    3970        0           800 python\nJul  1 09:29:32 execute0009 kernel: [253614.598226] [1581521]   706 1581521  2001526  1255072    3864     1092           800 python\nJul  1 09:29:32 execute0009 kernel: [253614.598229] [1630688]   706 1630688  2154487  1324109    4124     1730           800 python\nJul  1 09:29:32 execute0009 kernel: [253614.598232] [1663084]   706 1663084  2143283  1372687    4160     1341           800 python\nJul  1 09:29:32 execute0009 kernel: [253614.598235] [1676966]   699 1676966  5172411  4466318   10104   684222           800 lalinference_ne\nJul  1 09:29:32 execute0009 kernel: [253614.598237] [1679388]   699 1679388    39114    15720      76     1303           800 lalinference_ne\nJul  1 09:29:32 execute0009 kernel: [253614.598240] [1682737]   706 1682737  2367682  1530764    4526      740           800 python\nJul  1 09:29:32 execute0009 kernel: [253614.598243] Memory cgroup out of memory: Kill process 1676966 (lalinference_ne) score 1219 or sacrifice child\nJul  1 09:29:32 execute0009 kernel: [253614.598293] Killed process 1676966 (lalinference_ne) total-vm:20689644kB, anon-rss:17863480kB, file-rss:1792kB\n</pre></div>\n\n\n<p></p><hr/>\n<em>2016-Jul-20 13:21:45 by tpdownes:</em> <br/>\n\n<div class=\"verbatim\">\n<pre># enable RAM limit based upon ratio of 2000MB per core\nCONDOR_MEMORY_LIMIT_RAM=$((`nproc`*2000))\nCONDOR_MEMORY_LIMIT_SWAP=0\nCONDOR_MEMORY_LIMIT_SWAP_RAM=$((CONDOR_MEMORY_LIMIT_RAM+CONDOR_MEMORY_LIMIT_SWAP))\n\n# create htcondor cgroup and ensure that subgroups inherit memory limits\n/usr/bin/cgcreate -g blkio,cpu,cpuacct,freezer,memory:htcondor -a root:root -t root:root\n/usr/bin/cgset -r cpu.shares=1000 htcondor\n/usr/bin/cgset -r memory.use_hierarchy=1 htcondor\n/usr/bin/cgset -r memory.limit_in_bytes=${CONDOR_MEMORY_LIMIT_RAM}M htcondor\n/usr/bin/cgset -r memory.memsw.limit_in_bytes=${CONDOR_MEMORY_LIMIT_SWAP_RAM}M htcondor\n/usr/bin/cgset -r memory.swappiness=0 htcondor\n</pre></div>\n\n\n<p></p><hr/>\n<em>2016-Jul-28 11:30:35 by tpdownes:</em> <br/>\n\nSeeing a lot more of these. Curious whether you changed anything potentially related in last few 8.5 releases? It's a new-ish problem though I see a few instances dating back to May 10.\n\n<p></p><div class=\"verbatim\">\n<pre>8860036.0   riccardo.stura  7/13 16:21 Error from slot1_2@execute1046.nemo.phys.uwm.edu: Job has gone over memory limit of 2048 megabytes. Peak usage: 481 megabytes.\n8860038.0   riccardo.stura  7/13 16:21 Error from slot1_14@execute1046.nemo.phys.uwm.edu: Job has gone over memory limit of 2048 megabytes. Peak usage: 420 megabytes.\n8860039.0   riccardo.stura  7/13 16:21 Error from slot1_15@execute1046.nemo.phys.uwm.edu: Job has gone over memory limit of 2048 megabytes. Peak usage: 445 megabytes.\n8860040.0   riccardo.stura  7/13 16:21 Error from slot1_16@execute1046.nemo.phys.uwm.edu: Job has gone over memory limit of 2048 megabytes. Peak usage: 465 megabytes.\n8860042.0   riccardo.stura  7/24 19:53 Error from slot1_6@execute0011.nemo.phys.uwm.edu: Job has gone over memory limit of 2048 megabytes. Peak usage: 519 megabytes.\n8860067.0   riccardo.stura  7/13 16:21 Error from slot1_8@execute1046.nemo.phys.uwm.edu: Job has gone over memory limit of 2048 megabytes. Peak usage: 372 megabytes.\n8860068.0   riccardo.stura  7/13 16:21 Error from slot1_1@execute1046.nemo.phys.uwm.edu: Job has gone over memory limit of 2048 megabytes. Peak usage: 467 megabytes.\n8860069.0   riccardo.stura  7/13 16:21 Error from slot1_3@execute1046.nemo.phys.uwm.edu: Job has gone over memory limit of 2048 megabytes. Peak usage: 360 megabytes.\n8860071.0   riccardo.stura  7/13 16:21 Error from slot1_5@execute1046.nemo.phys.uwm.edu: Job has gone over memory limit of 2048 megabytes. Peak usage: 414 megabytes.\n8860499.0   pozzo           7/27 07:17 Error from slot1_13@execute1040.nemo.phys.uwm.edu: Job has gone over memory limit of 2048 megabytes. Peak usage: 499 megabytes.\n</pre></div>\n\n\n<p></p><hr/>\n<em>2016-Sep-28 21:24:22 by tpdownes:</em> <br/>\n\nHere's a recent example. The kernel kills one job (in slot 1_10) which has gone greatly over the memory limit.\n\n<p></p><div class=\"verbatim\">\n<pre>Sep 28 17:00:09 execute0143 kernel: [2422003.088605] python invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=800\nSep 28 17:00:09 execute0143 kernel: [2422003.088608] python cpuset=/ mems_allowed=0-1\nSep 28 17:00:09 execute0143 kernel: [2422003.088613] CPU: 1 PID: 394689 Comm: python Tainted: P           O  3.16.0-4-amd64 #1 Debian 3.16.7-ckt25-2+deb8u3\nSep 28 17:00:09 execute0143 kernel: [2422003.088614] Hardware name: Supermicro X8DTT-H/X8DTT-H, BIOS 080016  10/05/2010\nSep 28 17:00:09 execute0143 kernel: [2422003.088616]  0000000000000000 ffffffff8150e08f ffff88061ecfcaa0 ffff88061eed9800\nSep 28 17:00:09 execute0143 kernel: [2422003.088618]  ffffffff8150bc8b ffffffff81096dd3 ffff880107d6c258 0000000000000000\nSep 28 17:00:09 execute0143 kernel: [2422003.088620]  ffff88090cc05cd0 0000000000000001 0000000000000000 0000000000000003\nSep 28 17:00:09 execute0143 kernel: [2422003.088622] Call Trace:\nSep 28 17:00:09 execute0143 kernel: [2422003.088630]  [&lt;ffffffff8150e08f&gt;] ? dump_stack+0x5d/0x78\nSep 28 17:00:09 execute0143 kernel: [2422003.088632]  [&lt;ffffffff8150bc8b&gt;] ? dump_header+0x76/0x1e8\nSep 28 17:00:09 execute0143 kernel: [2422003.088636]  [&lt;ffffffff81096dd3&gt;] ? try_to_wake_up+0xd3/0x2d0\nSep 28 17:00:09 execute0143 kernel: [2422003.088640]  [&lt;ffffffff811404cd&gt;] ? find_lock_task_mm+0x3d/0x90\nSep 28 17:00:09 execute0143 kernel: [2422003.088642]  [&lt;ffffffff8114090d&gt;] ? oom_kill_process+0x21d/0x370\nSep 28 17:00:09 execute0143 kernel: [2422003.088644]  [&lt;ffffffff811404cd&gt;] ? find_lock_task_mm+0x3d/0x90\nSep 28 17:00:09 execute0143 kernel: [2422003.088648]  [&lt;ffffffff811a05aa&gt;] ? mem_cgroup_oom_synchronize+0x52a/0x590\nSep 28 17:00:09 execute0143 kernel: [2422003.088650]  [&lt;ffffffff8119fb30&gt;] ? mem_cgroup_try_charge_mm+0xa0/0xa0\nSep 28 17:00:09 execute0143 kernel: [2422003.088653]  [&lt;ffffffff811410c0&gt;] ? pagefault_out_of_memory+0x10/0x80\nSep 28 17:00:09 execute0143 kernel: [2422003.088656]  [&lt;ffffffff810574f5&gt;] ? __do_page_fault+0x3c5/0x4f0\nSep 28 17:00:09 execute0143 kernel: [2422003.088659]  [&lt;ffffffff8109d237&gt;] ? put_prev_entity+0x57/0x350\nSep 28 17:00:09 execute0143 kernel: [2422003.088661]  [&lt;ffffffff8109c0a6&gt;] ? set_next_entity+0x56/0x70\nSep 28 17:00:09 execute0143 kernel: [2422003.088664]  [&lt;ffffffff810a2e71&gt;] ? pick_next_task_fair+0x6e1/0x820\nSep 28 17:00:09 execute0143 kernel: [2422003.088668]  [&lt;ffffffff810d4ace&gt;] ? SyS_futex+0x6e/0x150\nSep 28 17:00:09 execute0143 kernel: [2422003.088670]  [&lt;ffffffff81516228&gt;] ? page_fault+0x28/0x30\nSep 28 17:00:09 execute0143 kernel: [2422003.088672] Task in /htcondor/condor_localscratch_condor_execute_slot1_10@execute0143.nemo.uwm.edu killed as a result of limit of /htcondor\nSep 28 17:00:09 execute0143 kernel: [2422003.088676] memory: usage 45190788kB, limit 49152000kB, failcnt 0\nSep 28 17:00:09 execute0143 kernel: [2422003.088677] memory+swap: usage 49151992kB, limit 49152000kB, failcnt 4152726\nSep 28 17:00:10 execute0143 kernel: [2422003.088678] kmem: usage 0kB, limit 18014398509481983kB, failcnt 0\nSep 28 17:00:10 execute0143 kernel: [2422003.088679] Memory cgroup stats for /htcondor: cache:0KB rss:40676KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:0KB inactive_anon:40676KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB\nSep 28 17:00:10 execute0143 kernel: [2422003.088691] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_5@execute0143.nemo.uwm.edu: cache:376KB rss:278664KB rss_huge:0KB mapped_file:4KB writeback:0KB swap:83564KB inactive_anon:162324KB active_anon:118640KB inactive_file:224KB active_file:132KB unevictable:0KB\nSep 28 17:00:10 execute0143 kernel: [2422003.088703] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_7@execute0143.nemo.uwm.edu: cache:396KB rss:267620KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:96840KB inactive_anon:147504KB active_anon:120116KB inactive_file:328KB active_file:68KB unevictable:0KB\nSep 28 17:00:10 execute0143 kernel: [2422003.088713] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_6@execute0143.nemo.uwm.edu: cache:0KB rss:3651072KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:1629844KB inactive_anon:982872KB active_anon:2668200KB inactive_file:0KB active_file:0KB unevictable:0KB\nSep 28 17:00:10 execute0143 kernel: [2422003.088725] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_3@execute0143.nemo.uwm.edu: cache:128KB rss:5594920KB rss_huge:0KB mapped_file:16KB writeback:0KB swap:86492KB inactive_anon:1250012KB active_anon:4344908KB inactive_file:96KB active_file:16KB unevictable:0KB\nSep 28 17:00:10 execute0143 kernel: [2422003.088736] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_2@execute0143.nemo.uwm.edu: cache:172KB rss:4197380KB rss_huge:0KB mapped_file:44KB writeback:0KB swap:141224KB inactive_anon:1113080KB active_anon:3084300KB inactive_file:116KB active_file:0KB unevictable:0KB\nSep 28 17:00:10 execute0143 kernel: [2422003.088747] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_1@execute0143.nemo.uwm.edu: cache:32KB rss:4140988KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:53756KB inactive_anon:1053912KB active_anon:3087076KB inactive_file:32KB active_file:0KB unevictable:0KB\nSep 28 17:00:10 execute0143 kernel: [2422003.088758] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_10@execute0143.nemo.uwm.edu: cache:0KB rss:26038612KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:1495384KB inactive_anon:2795848KB active_anon:23242764KB inactive_file:0KB active_file:0KB unevictable:0KB\nSep 28 17:00:10 execute0143 kernel: [2422003.088768] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_4@execute0143.nemo.uwm.edu: cache:80KB rss:344176KB rss_huge:0KB mapped_file:20KB writeback:12KB swap:136496KB inactive_anon:175236KB active_anon:168940KB inactive_file:48KB active_file:0KB unevictable:0KB\nSep 28 17:00:10 execute0143 kernel: [2422003.088779] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_8@execute0143.nemo.uwm.edu: cache:76KB rss:369984KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:110972KB inactive_anon:188556KB active_anon:181428KB inactive_file:76KB active_file:0KB unevictable:0KB\nSep 28 17:00:10 execute0143 kernel: [2422003.088789] Memory cgroup stats for /htcondor/condor_localscratch_condor_execute_slot1_9@execute0143.nemo.uwm.edu: cache:32KB rss:265348KB rss_huge:0KB mapped_file:0KB writeback:0KB swap:126632KB inactive_anon:134400KB active_anon:130948KB inactive_file:32KB active_file:0KB unevictable:0KB\nSep 28 17:00:10 execute0143 kernel: [2422003.088800] [ pid ]   uid  tgid total_vm      rss nr_ptes swapents oom_score_adj name\nSep 28 17:00:10 execute0143 kernel: [2422003.088909] [199191]  1141 199191   113473    70367     223    21259           800 lalinference_ne\nSep 28 17:00:10 execute0143 kernel: [2422003.088912] [199192]  1141 199192   113482    67115     225    24265           800 lalinference_ne\nSep 28 17:00:10 execute0143 kernel: [2422003.088915] [264696] 29998 264696  4626422   910436    3696   409584           800 python\nSep 28 17:00:10 execute0143 kernel: [2422003.088921] [385278] 29998 385278  4637313  1397711    3643    23528           800 python\nSep 28 17:00:10 execute0143 kernel: [2422003.088924] [392504] 29998 392504  4298295  1050187    2939    35408           800 python\nSep 28 17:00:10 execute0143 kernel: [2422003.088926] [394689] 29998 394689  4237232  1036166    3026    13640           800 python\nSep 28 17:00:10 execute0143 kernel: [2422003.088929] [400483] 29998 400483  6974359  6506523   13586   378090           800 ligolw_inspinjf\nSep 28 17:00:10 execute0143 kernel: [2422003.088931] [400485] 29998 400485   217160    87371     375    34124           800 python\nSep 28 17:00:10 execute0143 kernel: [2422003.088933] [400486] 29998 400486   217244    93604     383    27898           800 python\nSep 28 17:00:10 execute0143 kernel: [2422003.088936] [400487] 29998 400487   194962    67648     341    31658           800 python\nSep 28 17:00:10 execute0143 kernel: [2422003.088938] Memory cgroup out of memory: Kill process 400483 (ligolw_inspinjf) score 1361 or sacrifice child\nSep 28 17:00:10 execute0143 kernel: [2422003.088988] Killed process 400483 (ligolw_inspinjf) total-vm:27897436kB, anon-rss:26021536kB, file-rss:4556kB\n</pre></div>\n\n\n<p>But I think Condor is killing all jobs under the htcondor cgroup. I suspect the correct interpretation of this is that the assumptions outlined at\n\n</p><p><a class=\"external\" href=\"https://github.com/htcondor/htcondor/blob/4b0052c19353a8e3fabd2660dd3bcccbd976a282/src/condor_starter.V6.1/vanilla_proc.cpp#L1074\">https://github.com/htcondor/htcondor/blob/4b0052c19353a8e3fabd2660dd3bcccbd976a282/src/condor_starter.V6.1/vanilla_proc.cpp#L1074</a>\n\n</p><p>are not correct when memory.use_hierarchy is enabled. The way I read the kernel cgroups docs\n\n</p><p>\"If one of the ancestors goes over its limit, the reclaim algorithm reclaims from the tasks in the ancestor and the children of the ancestor.\"\n\n</p><p>it may be generating event notifications for every child cgroup of the parent cgroup (/htcondor) that triggered. Therefore, Condor may be wrong to place the jobs on hold.\n\n</p><p></p><div class=\"verbatim\">\n<pre>/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:10 (pid:394688) Job was held due to OOM event: Job has gone over memory limit of 7168 megabytes. Peak usage: 4050 megabytes.\n/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:11 (pid:394688) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:11 (pid:394688) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:11 (pid:394688) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:11 (pid:394688) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:13 (pid:394688) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:13 (pid:394688) condor_write(): Socket closed when trying to write 21 bytes to &lt;172.20.1.33:34792&gt;, fd is 11\n/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:13 (pid:394688) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:13 (pid:394688) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:13 (pid:394688) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:13 (pid:394688) Process exited, pid=394689, signal=9\n/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:13 (pid:394688) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_1:09/28/16 17:00:15 (pid:394688) **** condor_starter (condor_STARTER) pid 394688 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_2:09/28/16 17:00:09 (pid:392503) Job was held due to OOM event: Job has gone over memory limit of 7168 megabytes. Peak usage: 4098 megabytes.\n/var/log/condor/StarterLog.slot1_2:09/28/16 17:00:10 (pid:392503) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_2:09/28/16 17:00:10 (pid:392503) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_2:09/28/16 17:00:10 (pid:392503) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_2:09/28/16 17:00:10 (pid:392503) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_2:09/28/16 17:00:11 (pid:392503) Process exited, pid=392504, signal=9\n/var/log/condor/StarterLog.slot1_2:09/28/16 17:00:14 (pid:392503) condor_write(): Socket closed when trying to write 665 bytes to &lt;172.20.1.33:17514&gt;, fd is 11\n/var/log/condor/StarterLog.slot1_2:09/28/16 17:00:14 (pid:392503) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_2:09/28/16 17:00:14 (pid:392503) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_2:09/28/16 17:00:14 (pid:392503) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_2:09/28/16 17:00:15 (pid:392503) **** condor_starter (condor_STARTER) pid 392503 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:10 (pid:385277) Job was held due to OOM event: Job has gone over memory limit of 7168 megabytes. Peak usage: 5471 megabytes.\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:11 (pid:385277) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:11 (pid:385277) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:11 (pid:385277) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:11 (pid:385277) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:13 (pid:385277) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:13 (pid:385277) condor_write(): Socket closed when trying to write 21 bytes to &lt;172.20.1.33:44923&gt;, fd is 11\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:13 (pid:385277) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:13 (pid:385277) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:13 (pid:385277) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:13 (pid:385277) Process exited, pid=385278, signal=9\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:13 (pid:385277) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_3:09/28/16 17:00:15 (pid:385277) **** condor_starter (condor_STARTER) pid 385277 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:10 (pid:400480) Job was held due to OOM event: Job has gone over memory limit of 2048 megabytes. Peak usage: 470 megabytes.\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:11 (pid:400480) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:11 (pid:400480) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:11 (pid:400480) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:11 (pid:400480) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:13 (pid:400480) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:13 (pid:400480) condor_write(): Socket closed when trying to write 21 bytes to &lt;172.20.1.33:34237&gt;, fd is 11\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:13 (pid:400480) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:13 (pid:400480) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:13 (pid:400480) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:13 (pid:400480) Process exited, pid=400485, signal=9\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:13 (pid:400480) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_4:09/28/16 17:00:13 (pid:400480) **** condor_starter (condor_STARTER) pid 400480 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:09 (pid:199189) Job was held due to OOM event: Job has gone over memory limit of 6016 megabytes. Peak usage: 353 megabytes.\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:10 (pid:199189) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:10 (pid:199189) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:10 (pid:199189) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:10 (pid:199189) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:10 (pid:199189) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:10 (pid:199189) condor_write(): Socket closed when trying to write 21 bytes to &lt;172.20.1.35:2203&gt;, fd is 11\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:10 (pid:199189) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:10 (pid:199189) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:10 (pid:199189) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:10 (pid:199189) Process exited, pid=199191, signal=9\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:10 (pid:199189) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_5:09/28/16 17:00:11 (pid:199189) **** condor_starter (condor_STARTER) pid 199189 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:09 (pid:264695) Job was held due to OOM event: Job has gone over memory limit of 7168 megabytes. Peak usage: 5158 megabytes.\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:10 (pid:264695) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:10 (pid:264695) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:10 (pid:264695) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:10 (pid:264695) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:10 (pid:264695) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:10 (pid:264695) condor_write(): Socket closed when trying to write 21 bytes to &lt;172.20.1.33:17101&gt;, fd is 11\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:10 (pid:264695) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:10 (pid:264695) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:10 (pid:264695) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:11 (pid:264695) Process exited, pid=264696, signal=9\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:13 (pid:264695) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_6:09/28/16 17:00:15 (pid:264695) **** condor_starter (condor_STARTER) pid 264695 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:10 (pid:199190) Job was held due to OOM event: Job has gone over memory limit of 6016 megabytes. Peak usage: 352 megabytes.\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:10 (pid:199190) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:10 (pid:199190) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:10 (pid:199190) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:10 (pid:199190) condor_write(): Socket closed when trying to write 21 bytes to &lt;172.20.1.35:13207&gt;, fd is 11\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:10 (pid:199190) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:10 (pid:199190) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:10 (pid:199190) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:10 (pid:199190) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:10 (pid:199190) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:10 (pid:199190) Process exited, pid=199192, signal=9\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:10 (pid:199190) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_7:09/28/16 17:00:11 (pid:199190) **** condor_starter (condor_STARTER) pid 199190 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:10 (pid:400482) Job was held due to OOM event: Job has gone over memory limit of 2048 megabytes. Peak usage: 454 megabytes.\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:11 (pid:400482) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:11 (pid:400482) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:11 (pid:400482) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:11 (pid:400482) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:13 (pid:400482) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:13 (pid:400482) condor_write(): Socket closed when trying to write 21 bytes to &lt;172.20.1.33:12369&gt;, fd is 11\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:13 (pid:400482) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:13 (pid:400482) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:13 (pid:400482) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:13 (pid:400482) Process exited, pid=400486, signal=9\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:14 (pid:400482) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_8:09/28/16 17:00:14 (pid:400482) **** condor_starter (condor_STARTER) pid 400482 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:10 (pid:400484) Job was held due to OOM event: Job has gone over memory limit of 2048 megabytes. Peak usage: 384 megabytes.\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:11 (pid:400484) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:11 (pid:400484) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:11 (pid:400484) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:11 (pid:400484) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:13 (pid:400484) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:13 (pid:400484) condor_write(): Socket closed when trying to write 21 bytes to &lt;172.20.1.33:8014&gt;, fd is 11\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:13 (pid:400484) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:13 (pid:400484) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:13 (pid:400484) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:13 (pid:400484) Process exited, pid=400487, signal=9\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:14 (pid:400484) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_9:09/28/16 17:00:14 (pid:400484) **** condor_starter (condor_STARTER) pid 400484 EXITING WITH STATUS 0\n\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:09 (pid:400481) Job was held due to OOM event: Job has gone over memory limit of 2048 megabytes. Peak usage: 25364 megabytes.\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:10 (pid:400481) Got SIGQUIT.  Performing fast shutdown.\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:10 (pid:400481) ShutdownFast all jobs.\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:10 (pid:400481) Got SIGTERM. Performing graceful shutdown.\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:10 (pid:400481) ShutdownGraceful all jobs.\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:10 (pid:400481) Connection to shadow may be lost, will test by sending whoami request.\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:10 (pid:400481) condor_write(): Socket closed when trying to write 21 bytes to &lt;172.20.1.33:29237&gt;, fd is 11\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:10 (pid:400481) Buf::write(): condor_write() failed\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:10 (pid:400481) i/o error result is 0, errno is 0\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:10 (pid:400481) Lost connection to shadow, waiting 2400 secs for reconnect\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:12 (pid:400481) Process exited, pid=400483, signal=9\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:13 (pid:400481) Last process exited, now Starter is exiting\n/var/log/condor/StarterLog.slot1_10:09/28/16 17:00:13 (pid:400481) **** condor_starter (condor_STARTER) pid 400481 EXITING WITH STATUS 0\n</pre></div>\n\n\n<p></p><hr/>\n<em>2016-Sep-29 18:24:56 by tpdownes:</em> <br/>\n\nI find this funny:\n\n<p></p><div class=\"verbatim\">\n<pre>SYSTEM_PERIODIC_RELEASE   = (JobStatus == 5 &amp;&amp; MemoryUsage &lt; RequestMemory)\n</pre></div>\n\n\n<p></p><hr/>\n<em>2017-Jan-18 10:08:52 by tim:</em> <br/>\n\n<strong>CODE REVIEW:</strong> looks good. Not documenting until we make this the default.</blockquote>", "derived_tickets": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=6925\" onclick=\"get_ticket_and_populate_wrapper('6925'); return false;\" title=\"Jobs killed by OOM even when using less RAM than requested\">#6925</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nJobs killed by OOM even when using less RAM than requested</td></tr>\n</tbody></table>", "attachments": "<html><head></head><body></body></html>", "check_ins": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">2017-Jan-26 16:03</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"chngview?cn=50066\">[50066]</a></span>: (<span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5775\" onclick=\"get_ticket_and_populate_wrapper('5775'); return false;\" title=\"OOM killed all jobs in htcondor cgroup rather than offending job\">#5775</a></span>, <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=6040\" onclick=\"get_ticket_and_populate_wrapper('6040'); return false;\" title=\"Cedar should set SO_REUSEADDR on all sockets\">#6040</a></span>) Put knobs in the param table  (By Tim Theisen )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2017-Jan-25 16:46</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"chngview?cn=50058\">[50058]</a></span>: Added ALWAYS_REUSEADDR and IGNORE_LEAF_OOM config knobs to the manual, plus some fixes to the version history (see gittrac <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5775\" onclick=\"get_ticket_and_populate_wrapper('5775'); return false;\" title=\"OOM killed all jobs in htcondor cgroup rather than offending job\">#5775</a></span>, <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=6040\" onclick=\"get_ticket_and_populate_wrapper('6040'); return false;\" title=\"Cedar should set SO_REUSEADDR on all sockets\">#6040</a></span>)  (By Kent Wenger )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Nov-29 09:01</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"chngview?cn=49669\">[49669]</a></span>: ifdef cgroups control out for non-linux <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5775\" onclick=\"get_ticket_and_populate_wrapper('5775'); return false;\" title=\"OOM killed all jobs in htcondor cgroup rather than offending job\">#5775</a></span>  (By Greg Thain )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2016-Nov-28 16:43</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"chngview?cn=49661\">[49661]</a></span>: Better deal with memory cgroups which use_hierarchy=1 <span class=\"ticket\"><a class=\"defer\" href=\"/tickets?ticket=5775\" onclick=\"get_ticket_and_populate_wrapper('5775'); return false;\" title=\"OOM killed all jobs in htcondor cgroup rather than offending job\">#5775</a></span>  (By Greg Thain )</td></tr>\n</tbody></table>", "type": "defect", "last_change": "2017-Jan-18 10:11", "status": "defer", "created": "2016-Jul-05 14:03", "fixed_version": "2016-Jul-05 14:03", "broken_version": "v080505", "priority": "5", "subsystem": "", "assigned_to": "gthain", "derived_from": "", "creator": "tpdownes", "rust": "", "customer_group": "ligo", "visibility": "public", "notify": "tannenba@cs.wisc.edu,pcouvare@caltech.edu", "due_date": ""}
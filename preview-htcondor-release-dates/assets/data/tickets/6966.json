{"id": 6966, "title": "Ticket #6966: Allow user to trigger DAGMan node do-overs.", "description": "<blockquote>\nCMS runs a web portal which allows users to retry tasks.  Each task corresponds to a DAGMan node, which may already have a postscript and a retry count.  Sometimes, a node will use all of its retries and still fail because of a user-correctable problem; others, it may succeed but need to be retried because of an error not detected during the run.  Thus, the user wants to be able to cause DAGMan to run the node again, perhaps some considerable time after it fails its last retry.  Of course, for efficiency reasons, CMS doesn't want to have to kill off the entire DAG and start over again.\n\n<p>Instead, their software (CRAB v3) uses the Python bindings from a process on the web server to (a) insert an attribute into the scheduler universe job ad and (b) put DAGMan on hold.  The DAGMan job was already configured to make the signal sent when it is held SIGKILL; this forces DAGMan to reread the whole log when its jobs is released from hold.  This allows the wrapper script to edit the log, changing the last exit code (to one which forces a retry), and increasing that node's retry count.  Because the wrapper script reads in the whole log, edits one event, and then writes the whole log back out, it must hold the HTCondor-level lock to ensure that the uninterrupted jobs don't write events (to the wrong fd).\n\n</p><p>Both of this operations cause us consternation.  We'd like to remove the locking mechanism from the Python bindings, because editing the log would be its only use, and doing so is both fraught with peril and expensive (since DAGMan has to reread the whole thing).  The locking may not even be necessary with any reasonably recent version of HTCondor (since the shadows now use append semantics when writing events, and the exit codes are the same size, and in-place edit should be safe).\n\n</p><p>More importantly, it would be better to do away with the need to edit history entirely.  Instead, we came up with the following vague idea, which Mark will elaborate on in a design document; BrianB will also contribute, especially in the area of security design.\n\n</p><p>The general idea is for DAGMan to start listening on a command port for two commands: the 'retry a node' command, and the 'kill a node' command.  (The latter is mostly so that CRAB v3+ can use the same mechanism for both operations it needs to accomplish.)  The former will verify that the node either failed (and thus has no children in the ready or running lists) or was a leaf node; we do not propose to come up with anything sane to do when retrying an internal node that had previously been thought to succeed.  Because of shared port (this feature can be made conditional on that configuration, and/or a directive in the DAG file), we aren't worried about running out of sockets.  We'll have to test to make sure the shared port daemon/system can handle a large number of long-term idle listen sockets.  (The shadows' listen sockets are transient.)  The other issue is how DAGMan will verify that the remote process is authorized to edit the DAG in question.  Since it will be running without privileges, we can't use host certificates.  The current thinking is to use the new TOKEN infrastructure to generate a capability, which would then be shared via DAGMan's job ad; this would require extending secure attributes to include job ads, which could be very expensive.  (Another option would be to authenticate daemon-to-daemon and obtain a token for the DAGMan user, then use an encrypted execute directory (in the local universe) to run the DAGMan.)\n\n</p><p>Loose implementation ideas: the DAGMan command protocol will be its command int, a version int, and then a <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=ClassAd\" title=\"Class Ad\">ClassAd</a></span>.  DAGMan may have to adjust its internals, or record commands in a separate file read first (to, e.g., adjust its expectation about how many times a given node can fail).</p></blockquote>", "remarks": "<blockquote>\n<em>2019-Mar-26 16:06:10 by johnkn:</em> <br/>\n\nIt should be noted that Leica has repeatedly asked for something like \"retry node\"\n\n<p></p><hr/>\n<em>2019-Mar-26 19:48:01 by bbockelm:</em> <br/>\n\nOne thing to consider: how does the condor_dagman authenticate with the condor_shared_port?\n\n<p>The daemons find the shared port by being born with a shared secret from the condor_master.  We don't want to leak this secret to the dagman job -- but we may allow them access to the shared port directory?  Perhaps be born with a file descriptor?\n\n</p><p>Might be quite reasonable to utilize the shared port directory in this case and make improved security a follow-up ticket.</p></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "enhance", "last_change": "2020-Nov-09 10:55", "status": "new", "created": "2019-Mar-26 15:38", "fixed_version": "2019-Mar-26 15:38", "broken_version": "", "priority": "3", "subsystem": "Dag", "assigned_to": "coatsworth", "derived_from": "", "creator": "tlmiller", "rust": "", "customer_group": "cms", "visibility": "public", "notify": "tlmiller@cs.wisc.edu, johnkn@cs.wisc.edu, coatsworth@cs.wisc.edu, bbockelman@morgridge.org", "due_date": ""}
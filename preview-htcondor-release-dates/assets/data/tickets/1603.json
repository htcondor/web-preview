{"id": 1603, "title": "Ticket #1603: Bug with spool subdir creation when named schedd config used", "description": "<blockquote>\nUsing the named configuration stuff that's available in 7.4.2 to create multiple schedulers on a single machine I came across the following strange behaviour. This on Windows XP SP3. The pool has a shared file system.\n\n<p>I created a single scheduler on my machine using the named scheduler approach that's available now (this is condor_config.local in attached tarball):\n\n</p><p></p><pre>   SCHEDD1 = $(SCHEDD)\n   SCHEDD1_ARGS = -f -local-name Q1\n   SCHEDD1_LOG = $(LOG)/ScheddLog.1\n   SCHEDD.Q1.SCHEDD_NAME = Q1@$(HOSTNAME)\n   SCHEDD.Q1.SPOOL = $(LOCAL_DIR)/spool/1\n   SCHEDD.Q1.SCHEDD_LOG = $(SCHEDD1_LOG)\n   SCHEDD.Q1.HISTORY = $(LOCAL_DIR)/spool/1/history\n   SCHEDD.Q1.HISTORY_LOG = $(LOCAL_DIR)/spool/1/history\n</pre>\n\n<p></p><pre>   DAEMON_LIST = $(DAEMON_LIST), SCHEDD1\n</pre>\n\n<p>Fairly straightforward and for the most part: this works just as planned.\n\n</p><p>The weird behaviour comes when you run jobs. When I submit the attached test job with:\n\n</p><p></p><pre>   condor_submit -name q1@winxpcm python.sub\n</pre>\n\n<p>The jobs spool correctly, they even start and run correctly. But in the course of running a mirror set of cluster#.proc#.subproc# directories is created <strong>outside</strong> of the c:\\condor\\spool\\1 directory and in the c:\\condor\\spool directory.\n\n</p><p>It's mostly benign behaviour. Nothing ever gets spooled in those mirror directories. If I set leave_in_queue=true then the output is spooled in the c:\\condor\\spool\\1 sub-directories for the jobs which is great.\n\n</p><p>EXCEPT: the mirror directories that get created in c:\\condor\\spool <strong>never</strong> <strong>ever</strong> get cleaned up. They keep building and accumulating. Which means eventually they're going to not be benign anymore because they'll chew up inodes and make viewing that directory more and more of a pain.\n\n</p><p>I'm not sure who is creating the mirror directories. It almost feels like it's the shadow daemons. The proper directories appear in c:\\condor\\spool\\1 and then some time later, on the order 10 - 20 seconds, the mirror directories in c:\\condor\\spool show up.\n\n</p><p>So not critical, but eventually anyone going with this approach is going to hit a wall as those directories, even though they're empty, do chew up disk resources.\n\n</p><p>The attached archive contains the exact configuration files I used on this scheduler node (which was also my CM). I also included my job executable (a py2exe wrapped python script that dumps some info about the node and then sleeps) and the submit ticket I used to put it in the queue. Submit command is above. The screen captures show the state of the directories at two points in time: early one as the first few jobs from the 10 job cluster have started to run. And later, after all the jobs have completed.</p></blockquote>", "remarks": "<blockquote>\n<em>2010-Aug-24 11:51:58 by ichesal:</em> <br/>\n\n2.7MB is beyond the attachment limit so here's everything you need, stashed in my Dropbox account: <a class=\"external\" href=\"http://bit.ly/d4l03O\">http://bit.ly/d4l03O</a></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "defect", "last_change": "2010-Aug-24 11:56", "status": "new", "created": "2010-Aug-24 11:48", "fixed_version": "2010-Aug-24 11:48", "broken_version": "v070402", "priority": "3", "subsystem": "Daemons", "assigned_to": "", "derived_from": "", "creator": "ichesal", "rust": "", "customer_group": "other", "visibility": "public", "notify": "ichesal@cyclecomputing.com ialderman@cyclecomputing.com", "due_date": ""}
diff --git a/src/condor_examples/condor_config.generic b/src/condor_examples/condor_config.generic
index 3e125dd..b92d04a 100644
--- a/src/condor_examples/condor_config.generic
+++ b/src/condor_examples/condor_config.generic
@@ -2452,7 +2452,7 @@ LeaseManager.CLASSAD_LOG	= $(SPOOL)/LeaseManagerState
 ## The host and port for hadoop's name node. If this machine is the
 ## name node (see HDFS_SERVICES) then the specified port will be used
 ## to run name node. 
-HDFS_NAMENODE = example.com:9000
+HDFS_NAMENODE = hdfs://example.com:9000
 HDFS_NAMENODE_WEB = example.com:8000
 
 ## You need to pick one machine as name node by setting this parameter
@@ -2495,6 +2495,7 @@ HDFS_DATANODE_DIR = /scratch/tmp/hadoop_data
 #Fully qualified name for Name node and Datanode class.
 #HDFS_NAMENODE_CLASS=org.apache.hadoop.hdfs.server.namenode.NameNode
 #HDFS_DATANODE_CLASS=org.apache.hadoop.hdfs.server.datanode.DataNode
+#HDFS_DFSADMIN_CLASS=org.apache.hadoop.hdfs.tools.DFSAdmin
 
 ## In case an old name for hdfs configuration files is required.
 #HDFS_SITE_FILE = hadoop-site.xml
diff --git a/src/condor_hdfs/hadoop.cpp b/src/condor_hdfs/hadoop.cpp
index 7179fa9..c5792e6 100644
--- a/src/condor_hdfs/hadoop.cpp
+++ b/src/condor_hdfs/hadoop.cpp
@@ -120,13 +120,22 @@ void Hadoop::initialize() {
                 free(snclass);
         }
 
-        m_siteFile = "hdfs-site.xml";
+        m_dfsAdminClass = "org.apache.hadoop.hdfs.tools.DFSAdmin";
+        char *dfsAdminclass = param("HDFS_DFSADMIN_CLASS");
+        if (dfsAdminclass != NULL) {
+                m_dfsAdminClass = dfsAdminclass;
+                free(dfsAdminclass);
+        }
+
+        m_hdfsSiteFile = "hdfs-site.xml";
         char *sitef = param("HDFS_SITE_FILE");
         if (sitef != NULL) {
-                m_siteFile = sitef;
+                m_hdfsSiteFile = sitef;
                 free(sitef);
         }
 
+        m_coreSiteFile = "core-site.xml";
+
         char *adPubInt = param("HDFS_AD_PUBLISH_INTERVAL");
         if (adPubInt != NULL) {
                 int intv =  atoi(adPubInt);
@@ -144,6 +153,7 @@ void Hadoop::initialize() {
 
         ASSERT(m_reaper != FALSE);
 
+        writeCoreSiteFile();  
         writeConfigFile();
 
         startServices();
@@ -152,6 +162,7 @@ void Hadoop::initialize() {
         m_hdfsAd.SetMyTypeName(GENERIC_ADTYPE);
         m_hdfsAd.SetTargetTypeName("hdfs");
         m_hdfsAd.Assign(ATTR_NAME, "hdfs");
+        m_hdfsAd.Assign("ServiceType", getServiceNameByType( m_serviceType) );
         daemonCore->publish(&m_hdfsAd); 
 
         //Register a timer for periodically pushing classads.
@@ -161,13 +172,50 @@ void Hadoop::initialize() {
 
 }
 
+void Hadoop::writeCoreSiteFile() {
+        MyString confFile;
+        char *logdir = param("LOG");
+        if (logdir == NULL) 
+            EXCEPT("Misconfigured HDFS!: log directory is not specified\n");
+
+        confFile.sprintf("%s/%s", logdir, m_coreSiteFile.Value());
+        free(logdir);
+
+        int fd = safe_create_replace_if_exists(confFile.Value(), O_CREAT|O_WRONLY);
+        if (fd == -1) {
+                dprintf(D_ALWAYS, "Failed to create hadoop configuration file\n");
+                exit(1);
+        }
+
+        StringList xml("", "\n");;
+        xml.append("<?xml version=\"1.0\"?>");
+        xml.append("<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>");
+        xml.append("<!-- DON'T MODIFY this file manually, as it is overwritten by CONDOR.-->");
+        xml.append("<configuration>");
+
+        char *namenode = param("HDFS_NAMENODE");
+        if (namenode != NULL) {
+                writeXMLParam("fs.default.name", namenode, &xml);
+                free(namenode);
+        } 
+
+        xml.append("</configuration>");
+
+        char *str = xml.print_to_delimed_string(NULL);
+        ASSERT(str != NULL);
+        int len = full_write(fd, str, strlen(str));
+        ASSERT(len == strlen(str));
+        close(fd);
+        free(str);
+}
+
 void Hadoop::writeConfigFile() {
         MyString confFile;
         char *logdir = param("LOG");
         if (logdir == NULL) 
             EXCEPT("Misconfigured HDFS!: log directory is not specified\n");
 
-        confFile.sprintf("%s/%s", logdir, m_siteFile.Value());
+        confFile.sprintf("%s/%s", logdir, m_hdfsSiteFile.Value());
         free(logdir);
         dprintf(D_ALWAYS, "Config file location %s\n", confFile.Value());
 
@@ -354,25 +402,23 @@ void Hadoop::killTimer() {
 }
 
 void Hadoop::startServices() {
-        char *services = param ("HDFS_SERVICES");
-
-        if (services == NULL) {
-                startService(HADOOP_DATANODE);
-        } 
-        else {
-                MyString s(services);
-                
-                if (s == "HDFS_NAMENODE")
-                        startService(HADOOP_NAMENODE);
-                else if (s == "HDFS_DATANODE")
-                        startService(HADOOP_DATANODE);
-                else
-                        startService(HADOOP_SECONDARY);
-        }
+        char *services = param ("HDFS_SERVICES");
+
+        if (services == NULL) //by default run system as DataNode
+             m_serviceType = HADOOP_DATANODE;
+
+        else {
+            MyString s(services);
+            m_serviceType = getServiceTypeByName( s );
+        }
+
+        startService( m_serviceType );
 }
 
-void Hadoop::startService(int type) {
-        dprintf(D_ALWAYS, "Starting hadoop node service type = %d\n", type);
+void Hadoop::startService( NodeType type ) {
+        dprintf(D_ALWAYS, "Starting hadoop node service type = %s\n",
+                getServiceNameByType(type).Value() );
+
         ArgList arglist;
 
         java_config(m_java, &arglist, &m_classpath);
@@ -493,10 +539,61 @@ void Hadoop::startService(int type) {
         if (m_pid == FALSE) 
                 EXCEPT("Failed to launch hadoop process using Create_Process.\n ");
 
-        dprintf(D_ALWAYS, "Launched hadoop process %d pid=%d\n", type, m_pid);
+        dprintf(D_ALWAYS, "Launched hadoop process %s with pid=%d\n", 
+                getServiceNameByType(type).Value(), m_pid);
+
         m_state = STATE_RUNNING;
 }
 
+MyString Hadoop::runDFSAdmin( const char * cmd) {
+
+        MyString result;
+
+        ArgList arglist;
+
+        java_config(m_java, &arglist, &m_classpath);
+      
+        arglist.RemoveArg(0);
+        arglist.InsertArg(m_java.Value(), 0);
+        
+        arglist.AppendArg(m_dfsAdminClass);
+        arglist.AppendArg(cmd);
+
+        MyString argString;
+        arglist.GetArgsStringForDisplay(&argString);
+        dprintf(D_FULLDEBUG, "%s\n", argString.Value());
+
+        char ** args = arglist.GetStringArray();
+
+        FILE* fp;
+	fp = my_popenv( args, "r", FALSE );
+	if( ! fp ) {
+	       dprintf( D_ALWAYS, "Failed to execute %s %s\n",
+                     arglist.GetArg( arglist.Count()-2), 
+                     arglist.GetArg( arglist.Count()-1) );
+	       return "";
+	}
+
+        bool short_report=false;
+        char buf[ STDOUT_READBUF_SIZE ];
+
+        while ( fgets(buf, STDOUT_READBUF_SIZE, fp) ) {
+            MyString line = buf;
+
+            if( line.find("---")>=0 ){ //check end of namenode report
+                short_report=true; // set flag to parse one more line
+                continue;
+            }
+            result += line;
+            if( short_report ) //do not grab details of each node
+                break;
+        }
+        my_pclose( fp );
+        free( args );
+        dprintf(D_FULLDEBUG,"RESULT:%s\n", result.Value());
+        return result;
+}
+
 void Hadoop::writeXMLParam(const char *key, const char *value, StringList *buff) {
         MyString temp;
 
@@ -526,8 +623,52 @@ void Hadoop::recurrBuildClasspath(const char *path) {
                 }
         }
 }
+void Hadoop::updateClassAd( MyString safemode, MyString stats) {
+
+       MyString adKey, adValue;
+
+       adKey.sprintf("SAFEMODE");
+       int pos = safemode.find("OFF", 0);
+       if (pos >= 0)
+           adValue.sprintf("ON");
+       else
+           adValue.sprintf("OFF");
+
+       m_hdfsAd.Assign(adKey.Value(), adValue.Value() );
+       dprintf( D_ALWAYS, "Key=%s:Value=%s\n", adKey.Value(), adValue.Value() );
+
+       int ln_begin = 0;
+       int ln_end = 0;
+
+       while( (ln_end = stats.FindChar('\n', ln_begin+1)) > 0 ) {
+            MyString line = stats.Substr(ln_begin, ln_end-1);
+            ln_begin = ln_end;
+            line.trim();
+            if( line.Length() < 2 )
+                continue; 
+
+            pos = line.FindChar(':', 0);
+            if( pos < 0 ) //omit unncessary output
+                continue;
+
+            adKey = line.Substr(0, pos-1);
+            adKey.replaceString(" ","_");
+            adKey.replaceString("%","Percent");
+            adValue = line.Substr(pos+1, line.Length() );
+            adValue.replaceString("ï¿½", "0"); //this a bug in hdfs
+            adValue.trim();
+            m_hdfsAd.Assign(adKey.Value(), adValue.Value() );
+            dprintf( D_ALWAYS, "Key=%s:Value=%s\n", adKey.Value(), adValue.Value() );
+      }
+}
 
 void Hadoop::publishClassAd() {
+
+       if( m_serviceType == HADOOP_NAMENODE) { 
+           MyString mode  = runDFSAdmin("-safemode get");
+           MyString stats = runDFSAdmin("-report");
+
+           updateClassAd( mode, stats );       }
        daemonCore->UpdateLocalAd(&m_hdfsAd);
        int stat = daemonCore->sendUpdates(UPDATE_AD_GENERIC, &m_hdfsAd, NULL, true);
        dprintf(D_FULLDEBUG, "Updated ClassAds (status = %d)\n", stat);
@@ -655,3 +796,38 @@ int Hadoop::getKeyValue(MyString line, MyString *key, MyString *value) {
 end:
         return type;
 }
+
+NodeType Hadoop::getServiceTypeByName( MyString s )
+{
+	NodeType type;
+	if( s == "HDFS_NAMENODE" )
+     		type = HADOOP_NAMENODE;
+	
+	else if( s == "HDFS_SECONDARY" )
+		type = HADOOP_SECONDARY;
+	
+	else //by default run system as DataNode
+		type = HADOOP_DATANODE;
+	
+	return type;
+}
+
+MyString Hadoop::getServiceNameByType( NodeType type)
+{
+	MyString s;
+
+	switch (type){
+		case HADOOP_NAMENODE: 
+			s.sprintf("HADOOP_NAMENODE");
+			break;
+		
+		case HADOOP_SECONDARY:
+			s.sprintf("HADOOP_SECONDARY");
+			break;
+
+		default: //by default run system as DataNode
+			s.sprintf("HADOOP_DATANODE");
+	}
+
+	return s;
+}
diff --git a/src/condor_hdfs/hadoop.h b/src/condor_hdfs/hadoop.h
index aa58251..2e092cf 100644
--- a/src/condor_hdfs/hadoop.h
+++ b/src/condor_hdfs/hadoop.h
@@ -22,7 +22,7 @@
 
 #include "condor_daemon_core.h"
 
-enum {
+enum NodeType {
     HADOOP_NAMENODE,
     HADOOP_DATANODE,
     HADOOP_SECONDARY
@@ -84,6 +84,8 @@ class Hadoop : public Service {
 
         ClassAd m_hdfsAd;
 
+        NodeType m_serviceType;
+
         //keeps tracks of std output and error of  hadoop process
         MyString m_line_stdout, m_line_stderr;
 
@@ -101,10 +103,14 @@ class Hadoop : public Service {
 
         MyString m_secondaryNodeClass;
 
+        MyString m_dfsAdminClass;
+
         //Name of hdfs's site configuration files differs among hadoop version
         //Versions > 0.19 has hdfs-site.xml 
         //versions < 0.19 has hadoop-site.xml
-        MyString m_siteFile;
+        MyString m_hdfsSiteFile;
+
+        MyString m_coreSiteFile;
 
         //contains path of all jar files required to run
         //hadoop services.
@@ -113,17 +119,24 @@ class Hadoop : public Service {
         //Write the hadoop-site.xml file based  on parameters specified in
         //condor_config files.
         void writeConfigFile();
+        
+        //core-site.xml file is necassary for DFSAdmin commands
+        void writeCoreSiteFile();
 
         //Identifies the role of this machines (data node, name node or both)
         //and calls appropriate methods.
         void startServices();
 
-        void startService(int /*type*/);
+        void startService( NodeType );
+
+        MyString runDFSAdmin( const char *);
 
         void writeXMLParam(const char *key, const char *value, StringList *buff);
 
         void recurrBuildClasspath(const char *file);
 
+        void updateClassAd( MyString, MyString );
+
         void publishClassAd();
 
         void stdoutHandler(int /*pipe*/);
@@ -131,6 +144,10 @@ class Hadoop : public Service {
         void stderrHandler(int /*pipe*/);
 
         int getKeyValue(MyString line, MyString *key, MyString *value);
+
+	NodeType getServiceTypeByName( MyString );
+
+	MyString getServiceNameByType( NodeType );
 };
 
 #endif

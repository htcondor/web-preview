{"id": 3245, "title": "Ticket #3245: fix/update MPI submit script for SL6", "description": "<blockquote>\nThere isn't currently a working script for SL6.\n\n<p>There is some urgency to getting this working by November 2012 so that LIGO-India can evaluate Condor for use on their clusters (which have non-LIGO MPI users).</p></blockquote>", "remarks": "<blockquote>\n<em>2012-Oct-02 14:12:17 by tannenba:</em> <br/>\n\nSpecifically, we want the parallel universe startup scripts for OpenMPI updated. Esp the version of OpenMPI that is packaged with RHEL6/SL6. Secondary priority is to do the same for MPICH.\n\n<p></p><hr/>\n<em>2012-Oct-09 11:16:30 by tstclair:</em> <br/>\n\nHow is openmpi not supported on RHEL6 exactly?\n\n<p></p><div class=\"code\">\n<pre class=\"code\">git log openmpiscript\n\ncommit 849d8008542b2de1aced8502d0d4ba426d0c820e\nAuthor: Timothy St. Clair &lt;tstclair@redhat.com&gt;\nDate:   Mon Mar 12 10:42:34 2012 -0500\n\n    Another minor update to openmpiscript when testing on rhel6\n    ===VersionHistory:None===\n\ncommit cd5cb32121abf80b8c7b87191d0ed3e5b0604d56\nAuthor: Timothy St. Clair &lt;tstclair@redhat.com&gt;\nDate:   Thu Mar 8 15:01:09 2012 -0600\n\n    Fix openmpiscript, it was using deprecated params to openmpi.  It didn't\n    actually fail, but stderr would output a warning.\n    ===VersionHistory:None===\n</pre></div>\n\n\n<p></p><hr/>\n<em>2012-Oct-09 11:45:27 by tannenba:</em> <br/>\n\nAs part of this effort, the condor-users archive should be searched for various bits of wisdom, trouble areas, and/or helpful scripts.... e.g. threads <a class=\"external\" href=\"https://lists.cs.wisc.edu/archive/condor-users/2012-August/msg00119.shtml\">like this one</a>.\n\n<p></p><hr/>\n<em>2012-Oct-15 14:14:04 by samf:</em> <br/>\n\nI investigated this and do not see what the bug is. Further explanation would be appreciated. But let me explain what I did.\n\n<p>I installed SL6 (6.3) from a live CD on a virtual machine. I used <code>yum</code> to install <code>openmpi</code>, V.1.5.4. I then built the current development version of Condor (7.9.2 pre-release) on the virtual machine. After having to install numerous other packages and having to play around with the configuration script, Condor finally installed.\n\n</p><p>The <code>openmpiscript</code> example file works just fine and the sample scripts listed in <a class=\"external\" href=\"https://lists.cs.wisc.edu/archive/condor-users/2012-August/msg00119.shtml\">this message</a> did not work well. After some modification (namely removing the machine file information and the MCA part), I got it to work.\n\n</p><p>What versions of SL6, openmpi, and Condor did you use?\n\n</p><p></p><hr/>\n<em>2012-Oct-24 16:08:58 by samf:</em> <br/>\n\nTim and I both agree that the <code>openmpi</code> script is fine. As for <code>MPICH2</code>, I have a working script now provided I am using MPICH2 Version 1.5.\n<hr/>\n<em>2012-Oct-25 17:52:16 by johnkn:</em> <br/>\n\nBulk change of target version from v070806 to v070807 using ./ticket-target-mover.\n\n<p></p><hr/>\n<em>2012-Oct-26 09:52:08 by samf:</em> <br/>\n\nThe OpenMPI script works just fine. I used a Hello World code (with an executable called <code>mpi_hw</code>) and used the default <code>openmpiscript</code>\n\n<p></p><div class=\"code\">\n<pre class=\"code\">#############################################\n##   submit description file for a parallel program\n#############################################\nuniverse = parallel\nexecutable = openmpiscript\ngetenv=true\narguments = mpi_hw\n#actual_mpi_job arg1 arg2 arg3\n\nshould_transfer_files = yes\nwhen_to_transfer_output = on_exit_or_evict\n\ntransfer_input_files = mpi_hw\n\noutput = o.$(NODE)\nerror  = e.$(NODE)\nlog    = l\n\nnotification = never\nmachine_count = 2\nqueue\n</pre></div>\n\n\n<p>For MPICH2, I used version 1.5. I tried using MPICH2 Version 1.2.1 and had lots of trouble. That said, 1.2.1 is 2.5-3 years old already. I had to make two changes to the file <code>mp1script</code> for me to get it to work. Both involved changing some flags.  <code>sort</code> no longer recognizes certain old school flags (<code>+0</code>) and nor does <code>mpirun</code> recognize the <code>-v</code> flag.\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">#Old code\nsort -n +0 &lt; $CONDOR_CONTACT_FILE | awk '{print $2}' &gt; machines\n\n#New code\nsort -n -k 1 &lt; $CONDOR_CONTACT_FILE | awk '{print $2}' &gt; machines\n</pre></div>\n\n\n<p></p><div class=\"code\">\n<pre class=\"code\">#Old code\nmpirun -v -np $_CONDOR_NPROCS -machinefile machines $EXECUTABLE $@\n\n#New code\nmpirun -np $_CONDOR_NPROCS -machinefile machines $EXECUTABLE $@\n</pre></div>\n\n\n<p></p><hr/>\n<em>2013-Jan-08 13:57:20 by tstclair:</em> <br/>\n\nOur QE folks have found issue when testing the openscript against RHEL5 so I've updated to be agnostic to 32/64 bit &amp;&amp; orte/non-orte options.\n\n<p>post review please consider (8fc0d10ee03bd470012c5467955003f6f832482d) for stable series inclusion. It was been placed on master, and tested locally.\n\n</p><p></p><hr/>\n<em>2013-Jan-22 10:20:19 by pfc:</em> <br/>\n\nVivien reported the following.  It's not obvious to me where the permissions issue lies on the remote (?) system.  Can you advise?\n\n<p></p><div class=\"verbatim\">\n<pre>\nHi,\n\nI am trying to use the new openmpiscript to submit MPI jobs with condor and I am having difficulties having a successful test. Submitting a simple \"hello world\" MPI program fails with \"Permission denied\". Any help would be very appreciated !\n\nI am submitting:\n\n\n\nuniverse = parallel\nexecutable = /usr/share/doc/condor-7.8.7/etc/examples/openmpiscript\ngetenv=true\narguments = mpi_hw\nshould_transfer_files = yes\nwhen_to_transfer_output = on_exit_or_evict\ntransfer_input_files = mpi_hw\noutput = o.$(NODE)\nerror = e.$(NODE)\nlog = l\nnotification = never\nmachine_count = 2\nqueue\n\n\n\nwhere mpi_hw is a simple \"hello world\" MPI program:\n\n\n\n#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\n\nint MPIrank, MPIsize;\n\nint main(int argc, char *argv[]){\n  MPI_Init(&amp;argc, &amp;argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;MPIrank);\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;MPIsize);\n\n  fprintf(stdout,\"Hello World ! MPIrank=%d\\tMPIsize=%d\\n\",MPIrank,MPIsize);\n\n  MPI_Finalize();\n  return 0;\n}\n\n\n\nI get the log file:\n\n\n\n000 (55901299.000.000) 12/31 01:02:32 Job submitted from host: &lt;10.14.0.18:37450&gt;\n...\n014 (55901299.000.000) 12/31 01:03:12 Node 0 executing on host: &lt;10.14.1.7:55132&gt;\n...\n014 (55901299.000.001) 12/31 01:03:12 Node 1 executing on host: &lt;10.14.3.2:53190&gt;\n...\n001 (55901299.000.000) 12/31 01:03:12 Job executing on host: MPI_job\n...\n015 (55901299.000.000) 12/31 01:03:16 Node 0 terminated.\n\t(1) Normal termination (return value 0)\n\t\tUsr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage\n\t\tUsr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage\n\t\tUsr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage\n\t\tUsr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage\n\t1021  -  Run Bytes Sent By Node\n\t11862  -  Run Bytes Received By Node\n\t1021  -  Total Bytes Sent By Node\n\t11862  -  Total Bytes Received By Node\n...\n005 (55901299.000.000) 12/31 01:03:16 Job terminated.\n\t(1) Normal termination (return value 0)\n\t\tUsr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage\n\t\tUsr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage\n\t\tUsr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage\n\t\tUsr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage\n\t1021  -  Run Bytes Sent By Job\n\t23724  -  Run Bytes Received By Job\n\t1021  -  Total Bytes Sent By Job\n\t23724  -  Total Bytes Received By Job\n\tPartitionable Resources :    Usage  Request\n\t   Cpus                 :                 1\n\t   Disk (KB)            :       25       25\n\t   Memory (MB)          :        3      500\n...\n\n\n\n\n\nAnd the error file:\n\n\nPermission denied, please try again.\nPermission denied, please try again.\nPermission denied (publickey,gssapi-keyex,gssapi-with-mic,password).\n--------------------------------------------------------------------------\nA daemon (pid 8559) died unexpectedly with status 255 while attempting\nto launch so we are aborting.\n\nThere may be more information reported by the environment (see above).\n\nThis may be because the daemon was unable to find all the needed shared\nlibraries on the remote node. You may set your LD_LIBRARY_PATH to have the\nlocation of the shared libraries on the remote nodes and this will\nautomatically be forwarded to the remote nodes.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nmpirun noticed that the job aborted, but has no info as to the process\nthat caused that situation.\n--------------------------------------------------------------------------\nmpirun: clean termination accomplished\n\n</pre></div>\n\n\n<p></p><hr/>\n<em>2013-Jan-22 10:27:14 by samf:</em> <br/>\n\nI've been in touch with Vivien, but have not heard back from her regarding her problems. I've asked to see her log files so I can figure out what is going on.  All I know is that we have a problem with sshd.sh given the return value of 255. But I don't know what the exact problem is because sshd.sh has four error cases and the information given is insufficient for me to determine the problem.\n\n<p></p><hr/>\n<em>2013-Jan-28 14:53:36 by samf:</em> <br/>\n\nI heard back from Vivien.\n\n<p>Juan Barayoga at Caltech gave me a solution that worked.\n\n</p><p>1.  Copy .ssh/id_rsa.pub to .ssh/authorized_keys\n\n</p><p>2.  Create .ssh/config: $ cat config Host * <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=StrictHostKeyChecking\" title=\"Strict Host Key Checking\">StrictHostKeyChecking</a></span> no\n\n</p><p>with this, I successfully submitted jobs with openmpiscript.\n\n</p><p></p><hr/>\n<em>2013-Jan-28 14:59:26 by samf:</em> <br/>\n\nI also looked at Tim's submission. It works for me.\n\n<p></p><hr/>\n<em>2013-Mar-05 13:04:24 by pfc:</em> <br/>\n\nWait, this seems broken.  How do we get MPI jobs to work in the Parallel Universe to run without manually pre-installing the extra user credentials that Juan setup for Vivien?\n\n<p>I.e., is it by design that we need to manually set up ssh login authorization to every compute node for a user in order for MPI to work for them?\n\n</p><p></p><hr/>\n<em>2013-Mar-05 14:02:02 by gthain:</em> <br/>\n\nThe design is that the keys should NOT have to be set up a-priori.\n<hr/>\n<em>2013-Mar-28 17:16:00 by johnkn:</em> <br/>\n\nBulk change of target version from v070808 to v070809 using ./ticket-target-mover.\n\n<p></p><hr/>\n<em>2013-Jun-14 14:26:26 by samf:</em> <br/>\n\nGiven the error message previously obtained, I suspect that this is a problem when the keys get transferred around as the problem went away when the keys were transferred in another way. We should change the exit codes in <code>sshd.sh</code> so that we can distinguish what is going on. Additionally, if we had the regular stdout of the code, we might be able to diagnose what is going on since <code>sshd.sh</code> prints the actual error message to stdout, not strderr.\n\n<p>Greg mentioned that we might want to consider using <code>condor_ssh_to_job</code> to transfer the keys around instead of using <code>sshd.sh</code>.\n\n</p><p></p><hr/>\n<em>2013-Aug-20 14:41:36 by tannenba:</em> <br/>\n\nGreg, can we have an update on this ticket?\n\n<p></p><hr/>\n<em>2013-Sep-03 11:15:53 by gthain:</em> <br/>\n\nI've rerun my tests on SL6 here at UW, and things seem to work with openmpi.  I wonder if there's something about the environment there that's causing problems.  Would it be possible for me to get an account on Vivien's submit machine and reproduce the problem?\n\n<p></p><hr/>\n<em>2013-Sep-11 17:56:45 by gthain:</em> <br/>\n\nWorking with the cluster at Caltech, I was able to reproduce a problem, but it didn't look like the same error message.\n\n<p>I've put my work in ~gthain/partest on ldas-pcdev1\n\n</p><p>First, I build an MPI program with my favorite passing a token around a ring example, ring.c.  Note that the mpicc doesn't statically link the mpi libraries, so you need to set an rpath in order for it to work reliably:\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">PATH=/usr/lib64/openmpi/bin:$PATH\nmpicc -Wl,-rpath /usr/lib64/openmpi/lib -o ring ring.c\n</pre></div>\n\n\n<p>One difference between Caltech and the UW cluster is that caltech does not use slot users, and it does have a shared filesystem.  This caused me some surprises with openmpi, but, by putting the binary in the shared filesystem, which is really what openmpi wants, my tests ran.\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">$ cp ring ~/ring\n$ cp /usr/share/doc/condor-8.0.2/etc/examples/openmpiscript .\n$ cat parsubmit\nuniverse = parallel\nexecutable = openmpiscript\narguments = /home/gthain/ring\n\ngetenv = true\n\nshould_transfer_files = yes\ntransfer_input_files = /home/gthain/ring\nwhen_to_transfer_output = on_exit\n\noutput = output.$(NODE)\nerror  = error.$(NODE)\n\nlog = log\nmachine_count = 4\nqueue\n\n$ condor_submit parsubmit\nSubmitting job(s).\n1 job(s) submitted to cluster 59449841.\n</pre></div>\n\n\n<p></p><hr/>\n<em>2015-Sep-22 10:33:36 by tpdownes:</em> <br/>\n\nHaven't seen in a while. Closing after in-person discussion with GregT.</blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">2013-Jan-08 13:53</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/8fc0d10ee03bd470012c5467955003f6f832482d\">[34558]</a></span>: Ticket <span class=\"ticket\"><a class=\"defer\" href=\"/wiki-archive/tickets/?ticket=3245\" onclick=\"get_ticket_and_populate_wrapper('3245'); return false;\" title=\"fix/update MPI submit script for SL6\">#3245</a></span> Fix openmpiscript to be agnostic to 32/64 bit, and orte/non-orte envs.  (By Timothy St. Clair )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2012-Dec-17 14:48</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/9381ef109751b2964024d5d338f4d17af3870bb5\">[34422]</a></span>: Updating MPI example script so that it can run with MPICH Version 1.5 as well as the current version of sort. Ticket <span class=\"ticket\"><a class=\"defer\" href=\"/wiki-archive/tickets/?ticket=3245\" onclick=\"get_ticket_and_populate_wrapper('3245'); return false;\" title=\"fix/update MPI submit script for SL6\">#3245</a></span>  (By Samuel Friedman )</td></tr>\n</tbody></table>", "type": "defect", "last_change": "2017-Jul-06 11:29", "status": "defer", "created": "2012-Oct-02 14:07", "fixed_version": "2012-Oct-02 14:07", "broken_version": "v070804", "priority": "5", "subsystem": "Parallel", "assigned_to": "gthain", "derived_from": "", "creator": "pfc", "rust": "", "customer_group": "ligo", "visibility": "public", "notify": "pfcouvar@syr.edu, anderson@ligo.caltech.edu, tannenba@cs.wisc.edu, gthain@cs.wisc.edu, tstclair@redhat.com, samf@cs.wisc.edu, johnkn@cs.wisc.edu,pcouvare@caltech.edu", "due_date": ""}
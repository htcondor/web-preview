{"id": 2958, "title": "Ticket #2958: weighted slots not negotiating", "description": "<blockquote>\n<div class=\"verbatim\">\n<pre>I'm seeing strange behavior when I created a new group for 8-core jobs.\n I have a new group like:\n\ngroup_atlas.multicore\n\nwith a static quota of 400 and accept_surplus off.  There are 50 slots\nwith a special flag set and cpus=8.  When I first submit these jobs I\nget the following in the logs:\n\n04/25/12 16:38:04 group quotas: group group_atlas.multicore assigned\nquota= 400\n04/25/12 16:38:04 group quotas: group= group_atlas.multicore  cquota=\n400  static= 1  accept= 0  quota= 400  req= 1  usage= 0\n\nand the very first group being negotiated for I see:\n\n04/25/12 16:38:04 Group group_atlas.multicore - sortkey= 0\n04/25/12 16:38:04 Group group_atlas.multicore - BEGIN NEGOTIATION\n04/25/12 16:38:04 Phase 3:  Sorting submitter ads by priority ...\n04/25/12 16:38:04    maxAllowed  = 1.000000 groupQuota  = 1.000000\ngroupusage  = 0.000000\n04/25/12 16:38:04 Phase 4.1:  Negotiating with schedds ...\n04/25/12 16:38:04     numSlots = 12051\n04/25/12 16:38:04     slotWeightTotal = 1.000000\n04/25/12 16:38:04     pieLeft = 1.000\n04/25/12 16:38:04     NormalFactor = 1.000000\n04/25/12 16:38:04     MaxPrioValue = 1.250000\n04/25/12 16:38:04     NumSubmitterAds = 1\n04/25/12 16:38:04   Negotiating with\ngroup_atlas.multicore.usatlas1@bnl.gov at &lt;130.199.185.164:47859&gt;\n04/25/12 16:38:04 0 seconds so far\n04/25/12 16:38:04    maxAllowed  = 1.000000 groupQuota  = 1.000000\ngroupusage  = 0.000000\n04/25/12 16:38:04   Calculating submitter limit with the following\nparameters\n04/25/12 16:38:04     SubmitterPrio       = 1.250000\n04/25/12 16:38:04     SubmitterPrioFactor = 2.500000\n04/25/12 16:38:04     submitterShare      = 1.000000\n04/25/12 16:38:04     submitterAbsShare   = 1.000000\n04/25/12 16:38:04     submitterLimit    = 1.000000\n04/25/12 16:38:04     submitterUsage    = 0.000000\n04/25/12 16:38:04 Socket to group_atlas.multicore.usatlas1@bnl.gov\n(&lt;130.199.185.164:47859&gt;) already in cache, reusing\n04/25/12 16:38:04     Sending SEND_JOB_INFO/eom\n04/25/12 16:38:04     Getting reply from schedd ...\n04/25/12 16:38:04     Got JOB_INFO command; getting classad/eom\n04/25/12 16:38:04     Request 160101.00000:\n04/25/12 16:38:04 matchmakingAlgorithm: limit 1.000000 used 0.000000\npieLeft 1.000000\n04/25/12 16:38:05       Rejected 160101.0\ngroup_atlas.multicore.usatlas1@bnl.gov &lt;130.199.185.164:47859&gt;: group\nquota exceeded\n04/25/12 16:38:05     Sending SEND_JOB_INFO/eom\n04/25/12 16:38:05     Getting reply from schedd ...\n04/25/12 16:38:05     Got NO_MORE_JOBS;  done negotiating\n</pre></div>\n</blockquote>", "remarks": "<blockquote>\nTEST:\n\n<p>configuration, using weighted static slots:\n</p><div class=\"code\">\n<pre class=\"code\">NEGOTIATOR_USE_SLOT_WEIGHTS = True\nNEGOTIATOR_DEBUG = D_FULLDEBUG\n\nNUM_CPUS = 40\n\nSLOT_TYPE_1 = cpus=8\nSLOT_TYPE_1_PARTITIONABLE = False\nNUM_SLOTS_TYPE_1 = 5\n\nGROUP_NAMES = a\nGROUP_QUOTA_a = 40\n</pre></div>\n\n\n<p>First test job: verify that a single job can negotiate when all slots have weight &gt; 1:\n</p><div class=\"code\">\n<pre class=\"code\">universe = vanilla\ncmd = /bin/sleep\nargs = 300\nshould_transfer_files = if_needed\nwhen_to_transfer_output = on_exit\n+AccountingGroup=\"a.u1\"\nqueue 1\n</pre></div>\n\n\n<p>When this job is submitted, you should see it negotiate successfully and run.  Negotiator log output:\n</p><div class=\"code\">\n<pre class=\"code\">04/26/12 14:47:15   Calculating submitter limit with the following parameters\n04/26/12 14:47:15     SubmitterPrio       = 0.500000\n04/26/12 14:47:15     SubmitterPrioFactor = 1.000000\n04/26/12 14:47:15     submitterShare      = 1.000000\n04/26/12 14:47:15     submitterAbsShare   = 1.000000\n04/26/12 14:47:15     submitterLimit    = 40.000000\n04/26/12 14:47:15     submitterUsage    = 0.000000\n04/26/12 14:47:15 Socket to a.u1@localdomain (&lt;192.168.1.2:40576&gt;) not in cache, creating one\n04/26/12 14:47:15 SocketCache:  Found unused slot 0\n04/26/12 14:47:15     Sending SEND_JOB_INFO/eom\n04/26/12 14:47:15     Getting reply from schedd ...\n04/26/12 14:47:15     Got JOB_INFO command; getting classad/eom\n04/26/12 14:47:15     Request 00001.00000:\n04/26/12 14:47:15 matchmakingAlgorithm: limit 40.000000 used 0.000000 pieLeft 40.000000\n04/26/12 14:47:15 Start of sorting MatchList (len=5)\n04/26/12 14:47:15 Finished sorting MatchList\n04/26/12 14:47:15       Connecting to startd slot1@rorschach at &lt;192.168.1.2:53645&gt;\n04/26/12 14:47:15 File descriptor limits: max 1024, safe 820\n04/26/12 14:47:15       Sending PERMISSION, claim id, startdAd to schedd\n04/26/12 14:47:15       Matched 1.0 a.u1@localdomain &lt;192.168.1.2:40576&gt; preempting none &lt;192.168.1.2:53645&gt; slot1@rorschach\n04/26/12 14:47:15       Notifying the accountant\n04/26/12 14:47:15       Successfully matched with slot1@rorschach\n04/26/12 14:47:15     Sending SEND_JOB_INFO/eom\n04/26/12 14:47:15     Getting reply from schedd ...\n04/26/12 14:47:15     Got NO_MORE_JOBS;  done negotiating\n04/26/12 14:47:15   Submitter a.u1@localdomain got all it wants; removing it.\n04/26/12 14:47:15  resources used by a.u1@localdomain are 8.000000\n04/26/12 14:47:15  resources used scheddUsed= 8.000000\n</pre></div>\n\n\n<p>Second test job, to verify that jobs can negotiate when number submitters exceeds number of slots, and submitter limits are all &lt; the minimum slot weight\n</p><div class=\"code\">\n<pre class=\"code\">universe = vanilla\ncmd = /bin/sleep\nargs = 300\nshould_transfer_files = if_needed\nwhen_to_transfer_output = on_exit\n+AccountingGroup=\"a.u1\"\nqueue 1\n+AccountingGroup=\"a.u2\"\nqueue 1\n+AccountingGroup=\"a.u3\"\nqueue 1\n+AccountingGroup=\"a.u4\"\nqueue 1\n+AccountingGroup=\"a.u5\"\nqueue 1\n+AccountingGroup=\"a.u6\"\nqueue 1\n</pre></div>\n\n\n<p>Submit this job, and you should see 5 of the 6 jobs negotiate and run.  Negotiator log output\n</p><div class=\"code\">\n<pre class=\"code\">04/26/12 15:09:48 matchmakingAlgorithm: limit 6.700822 used 0.000000 pieLeft 8.000000\n04/26/12 15:09:48       Connecting to startd slot5@rorschach at &lt;192.168.1.2:53645&gt;\n04/26/12 15:09:48       Sending MATCH_INFO/claim id to slot5@rorschach\n04/26/12 15:09:48       (Claim ID is \"&lt;192.168.1.2:53645&gt;#1335476813#5#...\" )\n04/26/12 15:09:48       Sending PERMISSION, claim id, startdAd to schedd\n04/26/12 15:09:48       Matched 2.3 a.u6@localdomain &lt;192.168.1.2:40576&gt; preempting none &lt;192.168.1.2:53645&gt; slot5@rorschach\n04/26/12 15:09:48       Notifying the accountant\n04/26/12 15:09:48       Successfully matched with slot5@rorschach\n04/26/12 15:09:48     Reached submitter resource limit: 8.000000 ... stopping\n04/26/12 15:09:48   This submitter hit its submitterLimit.\n04/26/12 15:09:48  resources used scheddUsed= 40.000000\n04/26/12 15:09:48  negotiateWithGroup resources used scheddAds length 6\n04/26/12 15:09:48 Group &lt;none&gt; - sortkey= 3.40282e+38\n04/26/12 15:09:48 Group &lt;none&gt; - skipping, zero slots allocated\n04/26/12 15:09:48 group quotas: Group &lt;none&gt;  allocated= 0  usage= 0\n04/26/12 15:09:48 group quotas: Group a  allocated= 6  usage= 40\n04/26/12 15:09:48 Round 1 totals: allocated= 6  usage= 6\n</pre></div>\n\n\n<p></p><hr/>\n<em>2012-Apr-27 12:59:17 by tannenba:</em> <br/>\n\nThanks Erik for the fast turn-around on the patch! Very appreciated, I will buy you a beer next time I see you. Looking it over now... Meanwhile can you think of any reason off the top of your head why I should not apply this patch to v7.6 (assuming it applies cleanly) and merge forward?\n\n<p></p><hr/>\n<em>2012-May-01 10:25:03 by tannenba:</em> <br/>\n\n<strong>CODE REVIEW</strong>\n\n<p>Patch looks fine as far as it goes, and the testing looks like it will help resolve the specific incident that tripped up the folks at BNL, so pushing it into git. Thank you! However, looks like the patch surgically takes a few instances where we stop negotiating when the allocation is hit, and instead stops when the quota is hit.  There still look to be numerous places where we stop matching based on allocated instead of quota, even if surplus disabled. I think we really want a mode whereby we <em>always</em> go with quota instead of the computed allocation, regardless of surplus settings.  Going with the computed allocation is full of gotchas for all the well-known spinning pie reasons.\n\n</p><p>As the problem originated in v7.6.x, I rebased the patch to v7.6 and pushed into v7_6_7-branch, then merged upstream to V7_6-branch, V7_7-branch, V7_8-branch, master.</p></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body><blockquote>\n<ul>\n<li><a href=\"../files/558/gt2958-weighted-slots.patch\">gt2958-weighted-slots.patch</a>\n4865 bytes added by eje on 2012-Apr-26 22:18:37 UTC.\n<br/>\nA patch against 7.7-branch that negotiates with full group quota instead of allocated slots, when there is no surplus sharing in effect.  Allows negotiation to work when static slots have weight &gt; 1.\n<br/>\n</li></ul>\n</blockquote></body></html>", "check_ins": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">2012-May-08 11:27</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/a14f53c5a692f9c4bdc91141e310e669f693287a\">[31945]</a></span>: Fixed missing corner case in interaction between autoregroup and new no-surplus behavior, caught by regression testing ===GT=== <span class=\"ticket\"><a class=\"resolved\" href=\"/wiki-archive/tickets/?ticket=2958\" onclick=\"get_ticket_and_populate_wrapper('2958'); return false;\" title=\"weighted slots not negotiating\">#2958</a></span> Committer: John (TJ) Knoeller  (By Erik Erlandson )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2012-May-02 12:39</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/9c1bcc6efe2bdb686ffc8e62d3068285b7f760b1\">[31893]</a></span>: Fixed missing corner case in interaction between autoregroup and new no-surplus behavior, caught by regression testing ===GT=== <span class=\"ticket\"><a class=\"resolved\" href=\"/wiki-archive/tickets/?ticket=2958\" onclick=\"get_ticket_and_populate_wrapper('2958'); return false;\" title=\"weighted slots not negotiating\">#2958</a></span>  (By Erik Erlandson )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2012-May-01 07:14</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/b4a292ae28713d9a183cb0bb32c14a0ee89e0dd3\">[31880]</a></span>: Fix negotiator bug involving groups failing to claim weighted slots. <span class=\"ticket\"><a class=\"resolved\" href=\"/wiki-archive/tickets/?ticket=2958\" onclick=\"get_ticket_and_populate_wrapper('2958'); return false;\" title=\"weighted slots not negotiating\">#2958</a></span> Added logic to negotiator to negotiate with full group quota instead of the cooked-up allocated-slots when no surplus sharing is in effect.  (By Todd Tannenbaum )</td></tr>\n</tbody></table>", "type": "defect", "last_change": "2012-May-01 10:39", "status": "resolved", "created": "2012-Apr-26 13:20", "fixed_version": "2012-Apr-26 13:20", "broken_version": "v070606", "priority": "2", "subsystem": "Daemons", "assigned_to": "tannenba", "derived_from": "", "creator": "eje", "rust": "", "customer_group": "atlas", "visibility": "public", "notify": "zmiller@cs.wisc.edu, tannenba@cs.wisc.edu, willsk@bnl.gov, tstclair@cs.wisc.edu, matt@cs.wisc.edu", "due_date": ""}
{"id": 26, "title": "Ticket #26: Poor performance of Ligo Atlas Cluster", "description": "<blockquote>\nThe Condor installation on LIGO's new mega atlas cluster is performing very slowly, especially when used with DAGMan.</blockquote>", "remarks": "<blockquote>\n<em>2009-Jan-22 12:36:42 by zmiller:</em> <br/>\n\non 1/14 had an initial conference call with all involved to take a first pass at debugging.  they agreed to send us a bunch of logs.\n\n<p>on 1/16 had another conference call.  got login access to their schedd machine that is having trouble.  for some reason the dagman processes are consuming large amounts of CPU, and i believe the problem is related (if not caused directly) to this.\n\n</p><p>also suggested several other things for them to try to eliminate potential problem sources.\n\n</p><p>on 1/22 they replied that none of the suggestions have solved the problem.  i am now logging in to their machine to strace some of the processes.  sent them an email at 9am (central) to let them know we are working on it.\n\n</p><p></p><hr/>\n<em>2009-Jan-22 15:20:17 by psilord:</em> <br/>\n\nWhile working on <span class=\"ticket\"><a class=\"resolved\" href=\"/wiki-archive/tickets/?ticket=78\" onclick=\"get_ticket_and_populate_wrapper('78'); return false;\" title=\"Splice tests often timeout, but complete successfully later.\">#78</a></span>, I noticed that it seems scheduler universe jobs aren't run unless reschedule_negotiator() decides enough time has passed that it allows a reschedule. In the case I have, the negotiator interval is set to 25 seconds, and I'm rapid firing jobs down a linear chain of dag nodes. Dagman has a limit of 5 seconds before it considers submitting new jobs. So, if a job is submitted and then completed immediately, 5 seconds later a new job is submitted, then, if reschedule_negotiator() thinks less than 30 seconds has passed, it resets itself for 30 seconds in the future. 30 seconds later, the jobs starts running. So, the 5 second turnaround implied by dagman actually becomes a 30 second turnaround for each job. I think we need to investigate why running scheduler universe jobs is tied to reschedule_negotiator() and decouple them.\n\n<p></p><hr/>\n<em>2009-Jan-27 14:29:49 by zmiller:</em> <br/>\n\ni talked to pete.  his remarks are not the main issue for this ticket.\n\n<p>my belief is that shared filesystems are to blame.  they have moved some things to local disk (like job log files) but not everything.  i discovered that the dagman.out file is still being written into NFS, and that the average call to open() is on the order of 1/4 second.  i recommended they Stop Doing That(TM).\n\n</p><p></p><hr/>\n<em>2009-Jan-30 12:27:09 by zmiller:</em> <br/>\n\nran more tests on their submit node.\n\n<p>using a dagman with a local .dagman.out file i was able to submit jobs in bursts of 20 with no problem.\n\n</p><p>running condor_submit in a loop shows that jobs can be submitted at the rate of about 20 per second.\n\n</p><p></p><hr/>\n<em>2009-Jan-30 17:16:49 by zmiller:</em> <br/>\n\nheard in the LIGO phone call today that they are happy with my diagnosis and the ball is in their court to enact my recommendations.  resolving ticket until further notice.\n\n<p></p><hr/>\n<em>2009-Feb-03 14:38:32 by zmiller:</em> <br/>\n\nlogged in yesterday and today to check on the state of things.\n\n<p>overall, it looks like they followed at least some of my advice and have moved the .dagman.out files out of NFS.  however, not the .dagman.log.  the cluster is about 80% utilized now, which is an improvement, but there are still unclaimed slots and backfill jobs.  load average is about 8, and improvement from last week's 12.\n\n</p><p>i am still seeing dagman executables using a lot of CPU time, and they are indeed still accessing files in NFS.  now it is stat() and not open() that is taking a long time, about 1/8 second per call.\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">21:35:52.588719 stat(\"/home/spxiwh/ihopererun/852393970-854813170/playground/inspiral_hipe_playground_cat2_veto.PLAYGROUND_CAT_2_VETO.dag.dagman.log\", 0xbbe158) = -1 ENOENT (No such file or directory) &lt;0.128631&gt;\n21:35:52.717471 stat(\"/home/spxiwh/ihopererun/852393970-854813170/full_data/inspiral_hipe_full_data_cat2_veto.FULL_DATA_CAT_2_VETO.dag.dagman.log\", 0xbbe158) = -1 ENOENT (No such file or directory) &lt;0.128750&gt;\n</pre></div>\n\n\n<p>ignoring the reason for why the file is not found (more nfs bugs?) i would still recommend they move the .dagman.log file out of NFS.\n\n</p><p>i sent them email to this effect.\n\n</p><p></p><hr/>\n<em>2009-Mar-19 12:37:19 by zmiller:</em> <br/>\n\nsent followup email a month ago asking if they needed more help... did not receive a request for further investigation.  consider the matter closed.</blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "incident", "last_change": "2010-Jan-31 08:23", "status": "defer", "created": "2009-Jan-15 15:30", "fixed_version": "2009-Jan-15 15:30", "broken_version": "v070200", "priority": "5", "subsystem": "Daemons", "assigned_to": "zmiller", "derived_from": "", "creator": "tannenba", "rust": "", "customer_group": "ligo", "visibility": "public", "notify": "pcouvare@caltech.edu", "due_date": ""}
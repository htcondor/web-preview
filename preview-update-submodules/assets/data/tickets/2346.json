{"id": 2346, "title": "Ticket #2346: Condor-G GM_RESTART sequence does not appear to work with GT5", "description": "<blockquote>\nThis is with Condor 7.7.0 against a GT5.0.4-based CE.\n\n<p>When the gridmanager does its restart sequence, GT5 appears to get confused and cleans up the input directory (the one with the stderr/stdout files).  This causes the job - and stageout - to ultimately fail.\n\n</p><p>I have stared at this code for quite a bit, and I think it belongs in the hands of experts.  There's absolutely no indication from GRAM why it might be cleaning up the files - and the file removal isn't logged either.  Very befuddled.</p></blockquote>", "remarks": "<blockquote>\n<em>2011-Aug-15 17:10:50 by bbockelm:</em> <br/>\n\nJust FYI - this appears to still be an issue in the latest GT5.2 pre-release code.\n\n<p></p><hr/>\n<em>2011-Aug-15 17:37:42 by bbockelm:</em> <br/>\n\nTo reproduce, try the submit file and test script I list below.\n\n<p>Submit the first hundred jobs, and wait until a few start running on the remote CE.  Then, \"kill -9\" the client's gahp_server.  Let a few of the running jobs on the remote CE finish up.  Submit a new Condor-G job to get it to fire up gahp_server again.\n\n</p><p>The second gahp_server will be unable to restart the jobs.  The hold reason would be that files could not be transferred out.  On the remote host, one can verify that GRAM doesn't log destroying the job directory (even at TRACE), but indeed the files have been deleted.\n\n</p><p></p><div class=\"verbatim\">\n<pre>[bbockelm@brian-test gratia-probe]$ cat ~/tmp/testjob/test_g.submit\n\nuniverse = grid\nexecutable = test.sh\noutput = test_g.out\nerror = test_g.err\nlog = test_g.log\n\ngrid_resource = gt5 host.example.com:/jobmanager-condor\nremote_universe = vanilla\nShouldTransferFiles = YES\nWhenToTransferOutput = ON_EXIT\n+remote_ShouldTransferFiles = \"YES\"\n+remote_WhenToTransferOutput = \"ON_EXIT\"\n\nqueue 100\n\n[bbockelm@brian-test gratia-probe]$ cat ~/tmp/testjob/test.sh\n#!/bin/sh\n\nenv\nhostname\ndate\nsleep 1\ngrid-proxy-info\n</pre></div>\n</blockquote>", "derived_tickets": "", "attachments": "<blockquote>\n<ul>\n<li><a href=\"attach_get/444/gt5_failure.txt\">gt5_failure.txt</a>\n54530 bytes added by bbockelm on 2011-Aug-16 03:08:15 UTC.\n<br/>\nI was able to capture the failure.  I think the basic sequence is:\n<ul>\n<li>gahp_server dies.\n</li><li>Remote job completes, tries to stage out to dead gahp_server (and fails).  This puts the job-manager into state GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_TWO_PHASE.\n</li><li>gahp_server restarts, sends the STOP signal to the job-manager, and then instructs the job-manager to restart the job.\n</li><li>The job-manager restart code internally transitions the job from GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_TWO_PHASE to GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_CLOSE_OUTPUT.\n</li><li>The job-manager restarts the commit of the stageout failure error message.  The failure message is successfully committed, then transitions to GLOBUS_GRAM_JOB_MANAGER_STATE_FAILED_FILE_CLEAN_UP, which deletes the output files.\n</li></ul>\n\n<p>So, if the gahp_server is dead (or temporarily inaccessible) when the remote job-manager decides to stage out, the job will eventually be held and the output files will be deleted remotely upon restart.\n\n</p><p>What actions are in the transaction covered by the two-phase commit?  Is the transfer itself covered? I'm starting to think it should be.  If a job fails and the gridmanager is inaccessible, upon restart the job-manager should reset to the DONE/POLL1 state.<br/>\n</p></li><li><a href=\"attach_get/445/restart_on_failure.patch\">restart_on_failure.patch</a>\n6469 bytes added by bbockelm on 2011-Aug-17 17:22:16 UTC.\n<br/>\nThe attached patch, for globus-gram-job-manager, clears up this issue.  There were actually two issues:\n<ul>\n<li>Once a file transfer fails, it will never be retried (noted in the previous comment).  Now, if a file transfer fails and GRAM is unable to notify the remote client of the failure, a restart of the job-manager will place it back into a POLL1 state.\n</li><li>stdio_updates basically don't work in GT5 because Condor-G will update GRIDMANAGER_GASS_URL in the RSL, while the merge is done against the expanded RSL.  The merging is done against the in-memory RSL data structure, not rsl_spec which is written to disk.  Further, stakeout is done based on the todo list, not the RSL.  The patch fixes all of these things.\n</li></ul>\n\n<p>On the phone, Jaime and I decided that restarting the job-manager is no longer necessary in GT5 if the stdio_update works.  He'll be updating the gridmanager accordingly.<br/>\n</p></li></ul>\n</blockquote>", "check_ins": "", "type": "todo", "last_change": "2015-Aug-01 20:56", "status": "abandoned", "created": "2011-Aug-02 17:52", "fixed_version": "2011-Aug-02 17:52", "broken_version": "", "priority": "4", "subsystem": "Grid", "assigned_to": "jfrey", "derived_from": "", "creator": "bbockelm", "rust": "", "customer_group": "osg", "visibility": "public", "notify": "", "due_date": ""}
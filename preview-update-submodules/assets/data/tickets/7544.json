{"id": 7544, "title": "Ticket #7544: Document minimum GPU job duration to ensure recorded usage", "description": "<blockquote>\nIf I recall correctly, HTCondor requires a full sampling interval before recording any usage.  That sampling interval is computed for each GPU (being a third of the max sampling duration?) at run-time, but last time I looked was in practice one of two values which I don't presently recall.</blockquote>", "remarks": "<blockquote>\n<em>2020-Mar-11 16:18:34 by tlmiller:</em> <br/>\n\nThe documented minimum duration for a sample is 1/6 of a second.  At run-time, <code>condor_gpu_utilization</code> finds the GPU device with the smallest maximum sample count and divide that count by six; this give us the largest number of seconds we can wait between samples.  It then divides this count by three and samples at that interval.\n\n<p>However, it buffers the results and reports them at intervals not smaller than ten seconds each (but only immediately after a sample), so the delay between samples may be as large as 10 plus the sample interval.\n\n</p><p>(Two reference points: both a TITAN RTX and a Tesla V100 have max sample counts of 100.  We therefore sample every (100/6)/3 =~ 5.6 seconds.)\n\n</p><p>The other piece of the puzzle is that <code>condor_gpu_utilization</code> reports on a regular interval from when the startd started up, not from the beginning of any of the jobs.\n\n</p><p>The startd, therefore, receives a GPU utilization update roughly every 11 seconds in our example.  (The first sample is obtained after ~5.6 seconds, and the second at ~11.2 seconds, which is longer than 10 and triggers a report.)  To make sure we're reporting only the current job's utilization, the startd only counts full intervals.  Thus, if <code>condor_gpu_utilization</code> reports on GPU utilization at 21:00:00,  a job which starts at 21:00:01 will not record any GPU usage until (just after) 21:00:22.\n\n</p><p>There's additional complications regarding watching gpu utilization for jobs or slots based on how often the job and machine ads in the schedd and collector are actually updated, but the problem was with regard to short GPU jobs exiting without recording any activity.\n\n</p><p>If those jobs lasted for more than 23 seconds, however, that may indicate a problem in how we're reporting final GPU usage.\n\n</p><p></p><hr/>\n<em>2020-Mar-17 10:23:36 by tlmiller:</em> <br/>\n\n(This ticket may be about the \"RIFT\" tool?)\n\n<p></p><hr/>\n<em>2020-May-12 10:44:53 by pfc:</em> <br/>\n\nJames will check with Richard (yes this is RIFT).\n\n<p></p><hr/>\n<em>2020-Jun-09 10:31:57 by pfc:</em> <br/>\n\nDeferring until we can reconstruct the ultimate user-facing (or admin-facing) accounting problem (0 usage reported where we expect &gt;0?) that produced this bug report.</blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "doc", "last_change": "2020-Nov-03 10:33", "status": "abandoned", "created": "2020-Mar-10 10:49", "fixed_version": "2020-Mar-10 10:49", "broken_version": "", "priority": "3", "subsystem": "Unknown", "assigned_to": "tlmiller", "derived_from": "", "creator": "tlmiller", "rust": "", "customer_group": "ligo", "visibility": "public", "notify": "tlmiller@cs.wisc.edu", "due_date": ""}
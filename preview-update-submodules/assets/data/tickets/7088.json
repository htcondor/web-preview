{"id": 7088, "title": "Ticket #7088: DAGMan inefficiencies with very large graphs", "description": "<blockquote>\nLIGO is reporting problems with one of their DAGs running horribly slowly. It turns out their DAG is extremely large:\n<ul>\n<li>~300,000 vertices\n</li><li>~660,000,000 edges\n</li><li>~350 MB .dag file\n</li><li>~90 GB in memory footprint\n</li></ul>\n\n<p>Greg has done some additional profiling which reveals that fork/exec-ing condor_submit at this size takes ~3 seconds.\n\n</p><p>We need to quickly figure out ways to reduce the in-memory footprint of this DAG while also speeding up the submission process.\n\n</p><p>To speed up the submission process, we're modifying DAGMan to use the submit_utils library instead of forking condor_submit. TJ is currently working on this in <span class=\"ticket\"><a class=\"resolved\" href=\"/wiki-archive/tickets/?ticket=6974\" onclick=\"get_ticket_and_populate_wrapper('6974'); return false;\" title=\"condor_dagman should use submit_utils\">#6974</a></span>. We estimate this will reduce submit time from ~3 seconds per job to ~0.1 seconds.\n\n</p><p>As for reducing the in-memory footprint, one initial obvious approach is to reduce the number of edges by optimizing many-PARENT-many-CHILD relationships using intermediate nodes. This would reduce the number of edges in these cases from m*n to m+n. For LIGO's graph, this would reduce the ~660M edges to ~1.5M edges.\n\n</p><p>I'll continue updating this ticket with details about other metrics that DAGMan is producing.</p></blockquote>", "remarks": "<blockquote>\n<em>2019-Jun-03 15:46:49 by tannenba:</em> <br/>\n\nProfile of running the small dag\n\n<p></p><div class=\"verbatim\">\n<pre>  33% of time in dprintf\n  ~50%/50% parsing\n\n</pre></div>\n\n\n<p>114MB for dagman, 99MB was for edges\n\n</p><p></p><hr/>\n<em>2019-Jun-03 16:00:30 by gthain:</em> <br/>\n\nNote that a handful of lines have a PARENT x z y CHILD a b c relationships that create 50,000,000 or more edges:\n\n<p></p><div class=\"code\">\n<pre class=\"code\">/nobackup/gthain/ligo$ grep \"^PARENT \" bad.dag | awk '{for (i=0;i &lt; NF; i++) {if ($i == \"CHILD\") {print (i - 2) * (NF - i)}}}' |  sort -n | tail -8\n52094880\n52094880\n55550500\n55550500\n108219880\n108219880\n108219880\n108219880\n</pre></div>\n\n\n<p></p><hr/>\n<em>2019-Jun-03 20:24:43 by bbockelm:</em> <br/>\n\nRather than create an \"intermediate\" node when we have a X by Y PARENT / CHILD relationship, is it possible to do \"<span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=ClassAd\" title=\"Class Ad\">ClassAd</a></span> compression\"?\n\n<p>That is, introduce a reference object that can be shared by multiple nodes.\n\n</p><p></p><hr/>\n<em>2019-Jun-04 11:07:44 by gthain:</em> <br/>\n\nMark wrote a script to insert switch nodes between densely connected nodes.  With this, the footprint of dagman dropped from 90 Gb to 1 Gb, and the parsing startup time dropped from one hour to 20 seconds.  The fork time dropped from 3 seconds to 0.05 seconds, and when submitting real jobs to the schedd, we can get about 20-30 jobs submitted to the schedd per 10 seconds cycle.\n\n<p></p><hr/>\n<em>2019-Jun-04 13:46:17 by coatsworth:</em> <br/>\n\nI did another test where I removed the nodes' references to their parent nodes. This further improved the performance of the large dag:\n\n<p></p><ul>\n<li>Memory footprint reduced by a further 150 MB, a 10% improvement.\n</li><li>Parsing now takes only 5 seconds, down from 6 seconds. (I'm getting different parse speeds than Greg, likely due to a different CPU)\n</li></ul>\n\n<p>However there are three places where we depend on these parent nodes:\n\n</p><p></p><ul>\n<li>Cycle detection\n</li><li>Splices\n</li><li>Putting a list of parent node names in the job ad, so that <code>condor_q -dag</code> works correctly.\n</li></ul>\n\n<p>We can fix cycle detection by changing the graph traversal algorithm, and splices only care about the number of parents, so we could store this in a member variable. However getting the list of parent node names is trickier.\n\n</p><p>I think this is good information to be aware of, but not worth moving forward on.\n\n</p><p></p><hr/>\n<em>2019-Jun-04 15:20:17 by coatsworth:</em> <br/>\n\nMarked as abandoned and all wisdom merged into <span class=\"ticket\"><a class=\"resolved\" href=\"/wiki-archive/tickets/?ticket=7054\" onclick=\"get_ticket_and_populate_wrapper('7054'); return false;\" title=\"LIGO gstlal DAG scaling problem\">#7054</a></span></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body><blockquote>\n<ul>\n<li><a href=\"../files/1026/dagman-noschedd.patch\">dagman-noschedd.patch</a>\n853 bytes added by coatsworth on 2019-Jun-03 20:58:31 UTC.\n<br/>\nAdded batch to run DAGMan in full schedd bypass mode<br/>\n</li></ul>\n</blockquote></body></html>", "check_ins": "", "type": "enhance", "last_change": "2019-Jun-05 15:21", "status": "abandoned", "created": "2019-Jun-03 14:32", "fixed_version": "2019-Jun-03 14:32", "broken_version": "", "priority": "1", "subsystem": "Dag", "assigned_to": "", "derived_from": "#7054", "creator": "coatsworth", "rust": "", "customer_group": "other", "visibility": "public", "notify": "coatsworth@cs.wisc.edu, tannenba@cs.wisc.edu, gthain@cs.wisc.edu", "due_date": ""}
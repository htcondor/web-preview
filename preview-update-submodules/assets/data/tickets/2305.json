{"id": 2305, "title": "Ticket #2305: RFE: increase the efficiency of MatchList construction and use", "description": "<blockquote>\nFrom Jon Thomas:\n\n<p></p><div class=\"verbatim\">\n<pre>Some interesting MatchList stats from a weekend of logs. I looked at\nother weekends and they are similar.\n\nTime period: 05/19/11 11:21:46 - 05/21/11 14:51:09\n\nNegotiation cycles: 1603\nAverage/median/max/min usable slots per cycle: 362/372/555/113\n\nMatchLists built: 89410\nMatchLists used: 27947 (31%)\nMatchLists unused: 61463 (69%)\n\nNumber of slots successfully used from MatchLists used: 164052\nNumber of slots attempted but failed to be used from MatchLists used:\n9240\n\nAverage/median/max/min length of MatchList: 142/105/526/2\n\nAverage slots used per MatchList built: 1.83\nAverage slots used per MatchList used: 5.87\n</pre></div>\n\n\n<p>Statistics suggest that either (a) match lists are being built that aren't useful, or (b) match-lists aren't being utilized to their fullest extent, or some combination of both.\n\n</p><p>Some possible avenues of attack:\n</p><ul>\n<li>examine the size of auto-clusters, and ignore auto-clusters whose size is 1 (schedd-side?)\n</li><li>improve the key used to group auto-clusters\n</li><li>Keep match-lists around, instead of dumping them every time submitter changes.   The following comment is suggestive:\n</li></ul>\n\n<p></p><div class=\"code\">\n<pre class=\"code\">\t\t// Delete our old MatchList, since we know that if we made it here\n\t\t// we no longer are dealing with a job from the same autocluster.\n\t\t// (someday we will store it in case we see another job with\n\t\t// the same autocluster, but we aren't that smart yet...)\n\tDeleteMatchList();\n</pre></div>\n</blockquote>", "remarks": "<blockquote>\n<em>2011-Jul-14 15:51:00 by eje:</em> <br/>\n\nThis looks like a plausible place to insert autocluster size into the job ads.\n\n<p></p><div class=\"code\">\n<pre class=\"code\">(schedd.cpp:5214)\n\t\tif( !cluster || cluster-&gt;getAutoClusterId() != auto_cluster_id )\n\t\t{\n\t\t\tcluster = new ResourceRequestCluster( auto_cluster_id );\n\t\t\tresource_requests-&gt;push_back( cluster );\n\t\t}\n</pre></div>\n\n\n<p>Another possibility would be in DoBuildPrioRecArray(), after it assigns autoclusters in WalkJobQueue(get_job_prio)\n\n</p><p></p><hr/>\n<em>2011-Jul-14 17:33:14 by tannenba:</em> <br/>\n\nI have been looking into this a bit as well.  I also was the one who wrote the comment in the code :).\n\n<p>I definitely think the big win will be to keep match-lists around until either (a) the negotiation cycle ends or (b) we cannot find a match (e.g. all machines in the matchlist have been given away), instead of dumping them every time the submitter changes OR the autocluster ID changes.\n\n</p><p>Not creating a match-list when there is only one job in the autocluster won't save CPU time, just a small amount of RAM.  (not a bad thing, just not as important imho).\n\n</p><p>With your statistics above, it would be nice to know how many different users/autoclusters were involved....\n\n</p><p></p><hr/>\n<em>2011-Jul-14 17:36:20 by eje:</em> <br/>\n\nI've got a request out for some histograms of autocluster size from customer:\n<div class=\"code\">\n<pre class=\"code\">$ condor_q -format \"%d\\n\" 'ifThenElse(AutoClusterId =!= undefined, AutoClusterId, 0)' | sort -n | uniq -c | sort -r -n -k  1\n      10 1\n       5 2\n</pre></div>\n\n\n<p>Variation for multi-schedd pool from Matt:\n</p><div class=\"code\">\n<pre class=\"code\">$ condor_q -global -format \"%s\\t\" 'regexps(\"([^#]*)#.*\", GlobalJobId, \"\\1\")' -format \"%d\\n\" AutoClusterId -constraint 'AutoClusterId =!= UNDEFINED'  | sort | uniq -c | sort -r -n -k  1\n</pre></div>\n\n\n<p>I'll post some info on that when I get it\n\n</p><p></p><hr/>\n<em>2011-Jul-14 17:43:42 by eje:</em> <br/>\n\nA general comment: submitter names probably change with significantly greater frequency these days with recent HGQ logic than they would without group quotas.\n\n<p></p><hr/>\n<em>2011-Jul-15 15:39:30 by eje:</em> <br/>\n\nSome autoclustering signatures.  The last three have equivalent requirements expressions, which yield differing signatures:\n<div class=\"code\">\n<pre class=\"code\">07/15/11 12:51:32 (pid:1813) EJE: signature= 'JobUniverse = 5NumCkpts = 0FileSystemDomain = \"localhost.localdomain\"DiskUsage = 30ImageSize = 30RequestMemory = ceiling(ifThenElse(JobVMMemory =!= undefined,JobVMMemory,ImageSize / 1024.000000))Requirements = ( TARGET.Arch == \"X86_64\" ) &amp;&amp; ( TARGET.OpSys == \"LINUX\" ) &amp;&amp; ( TARGET.Disk &gt;= DiskUsage ) &amp;&amp; ( ( TARGET.Memory * 1024 ) &gt;= ImageSize ) &amp;&amp; ( ( RequestMemory * 1024 ) &gt;= ImageSize ) &amp;&amp; ( ( TARGET.HasFileTransfer ) || ( TARGET.FileSystemDomain == MY.FileSystemDomain ) )NiceUser = false'\n\n07/15/11 12:54:25 (pid:1813) EJE: signature= 'JobUniverse = 5NumCkpts = 0FileSystemDomain = \"localhost.localdomain\"DiskUsage = 30ImageSize = 30RequestMemory = ceiling(ifThenElse(JobVMMemory =!= undefined,JobVMMemory,ImageSize / 1024.000000))Requirements = ( ( SlotID &lt;= 1 ) ) &amp;&amp; ( TARGET.Arch == \"X86_64\" ) &amp;&amp; ( TARGET.OpSys == \"LINUX\" ) &amp;&amp; ( TARGET.Disk &gt;= DiskUsage ) &amp;&amp; ( ( TARGET.Memory * 1024 ) &gt;= ImageSize ) &amp;&amp; ( ( RequestMemory * 1024 ) &gt;= ImageSize ) &amp;&amp; ( ( TARGET.HasFileTransfer ) || ( TARGET.FileSystemDomain == MY.FileSystemDomain ) )NiceUser = false'\n\n07/15/11 12:56:46 (pid:1813) EJE: signature= 'JobUniverse = 5NumCkpts = 0FileSystemDomain = \"localhost.localdomain\"DiskUsage = 30ImageSize = 30RequestMemory = ceiling(ifThenElse(JobVMMemory =!= undefined,JobVMMemory,ImageSize / 1024.000000))Requirements = ( ( SlotID &lt;= 1 &amp;&amp; Target.OpSys == \"LINUX\" &amp;&amp; Target.Arch == \"X86_64\" ) ) &amp;&amp; ( TARGET.Disk &gt;= DiskUsage ) &amp;&amp; ( ( TARGET.Memory * 1024 ) &gt;= ImageSize ) &amp;&amp; ( ( RequestMemory * 1024 ) &gt;= ImageSize ) &amp;&amp; ( ( TARGET.HasFileTransfer ) || ( TARGET.FileSystemDomain == MY.FileSystemDomain ) )NiceUser = false'\n\n07/15/11 12:57:37 (pid:1813) EJE: signature= 'JobUniverse = 5NumCkpts = 0FileSystemDomain = \"localhost.localdomain\"DiskUsage = 30ImageSize = 30RequestMemory = ceiling(ifThenElse(JobVMMemory =!= undefined,JobVMMemory,ImageSize / 1024.000000))Requirements = ( ( SlotID &lt;= 1 &amp;&amp; Target.Arch == \"X86_64\" &amp;&amp; ( Target.OpSys == \"LINUX\" ) ) ) &amp;&amp; ( TARGET.Disk &gt;= DiskUsage ) &amp;&amp; ( ( TARGET.Memory * 1024 ) &gt;= ImageSize ) &amp;&amp; ( ( RequestMemory * 1024 ) &gt;= ImageSize ) &amp;&amp; ( ( TARGET.HasFileTransfer ) || ( TARGET.FileSystemDomain == MY.FileSystemDomain ) )NiceUser = false'\n</pre></div>\n\n\n<p></p><hr/>\n<em>2011-Jul-20 23:02:41 by eje:</em> <br/>\n\nHistogram of autocluster sizes, up to size 50.\n\n<p><img alt=\"autocluster_20110727_1100_hist.png\" src=\"attach_get/423/autocluster_20110727_1100_hist.png\"/>\n\n</p><p>Here is the CDF of autocluster sizes.  We see that most are of size 1, or small:\n\n</p><p><img alt=\"autocluster_20110727_1100_cdf.png\" src=\"attach_get/424/autocluster_20110727_1100_cdf.png\"/>\n\n</p><p></p><hr/>\n<em>2011-Jul-28 13:52:10 by eje:</em> <br/>\n\nThoughts about <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=MatchList\" title=\"Match List\">MatchList</a></span> memory requirements:   As a straw-man, consider a large-scale pool with 40,000 slots, and where there are around 5000 unique autoclusters in a given neg-cycle (which is what I'm seeing from the above, correctly accounting for scoping autoclusters by scheduler id)\n\n<p>Each <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=AdListEntry\" title=\"Ad List Entry\">AdListEntry</a></span> is 40 bytes (assuming 4-byte pointers).  Currently the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=MatchList\" title=\"Match List\">MatchList</a></span> just brute-force allocates an entry for each slot (the maximum possible match-list length), so in a pool with 40K slots the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=MatchList\" title=\"Match List\">MatchList</a></span> uses 1.6 megabytes.\n\n</p><p>In a hypothetical enhancement to keeping match-lists around for the duration of the neg-cycle, the worst case would be all 5000 cluster-ids X 1.6e6, or 8 gigabytes.  As-is, that would be a non-starter.\n\n</p><p>Clearly there is some low-hanging fruit.  For openers, use vector&lt;AdListEntry&gt;, which grows adaptively.  It doesn't prevent the worst-case, but the worst case of all slots matching all autoclusters is probably very unlikely.  match-list length is likely to be zipfian in the same way that autocluster size is, although it would be interesting to get some real measures on it.\n\n</p><p>Additionally, the histograms above show that we can straight-up ignore a large fraction of autoclusters, as their size is 1 -- they cannot provide any cost-savings to the negotiation loop.  Heuristically, one might also choose to ignore autoclusters of size &lt; N, for some N &gt; 1.  (There might also be a similar heuristic on match-list length, although by the time you know that length you've sunk the cost of building the list, so any motivation for dumping it would be for memory reasons only)\n\n</p><p>Ignoring autoclusters by size implies that we compute that size on the schedd side.  I noted a couple places that could be done above.   It would add some computation to the schedd, although it should be at worst nlogn on job ads.\n\n</p><p>One impact of storing multiple match-lists is that a slot ad may appear in multiple lists, and so when a slot is matched that ad may need to be annotated to prevent re-use in some other autocluster.   That should be easy as long as slot ads are re-used via pointer: just set a flag-attribute.</p></blockquote>", "derived_tickets": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"abandoned\" href=\"/wiki-archive/tickets/?ticket=2258\" onclick=\"get_ticket_and_populate_wrapper('2258'); return false;\" title=\"Use autocluster size to limit matchlist\">#2258</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nUse autocluster size to limit matchlist</td></tr>\n</tbody></table>", "attachments": "<html><head></head><body><blockquote>\n<ul>\n<li><a href=\"../files/416/autocluster_hist.png\">autocluster_hist.png</a>\n21950 bytes added by eje on 2011-Jul-21 03:54:53 UTC.\n<br/>\nHistogram of autocluster sizes up to 50<br/>\n</li><li><a href=\"../files/418/autocluster.dat\">autocluster.dat</a>\n8403 bytes added by eje on 2011-Jul-21 03:56:50 UTC.\n<br/>\nautocluster size data snapshot from a condor pool: first column is size, second column is the autocluster id<br/>\n</li><li><a href=\"../files/419/autocluster_cprob.png\">autocluster_cprob.png</a>\n18474 bytes added by eje on 2011-Jul-21 03:59:45 UTC.\n<br/>\nCDF of autocluster sizes up to 50<br/>\n</li><li><a href=\"../files/423/autocluster_20110727_1100_hist.png\">autocluster_20110727_1100_hist.png</a>\n25044 bytes added by eje on 2011-Jul-27 18:08:42 UTC.\n<br/>\nautocluster sizes with scheduler id properly accounted for<br/>\n</li><li><a href=\"../files/424/autocluster_20110727_1100_cdf.png\">autocluster_20110727_1100_cdf.png</a>\n18865 bytes added by eje on 2011-Jul-27 18:09:06 UTC.\n<br/>\nCDF of autocluster sizes<br/>\n</li></ul>\n</blockquote></body></html>", "check_ins": "", "type": "enhance", "last_change": "2012-Oct-17 13:19", "status": "new", "created": "2011-Jul-14 11:01", "fixed_version": "2011-Jul-14 11:01", "broken_version": "", "priority": "4", "subsystem": "Daemons", "assigned_to": "eje", "derived_from": "", "creator": "eje", "rust": "", "customer_group": "other", "visibility": "public", "notify": "tannenba@cs.wisc.edu, eje@cs.wisc.edu, jthomas@redhat.com, dan@hep.wisc.edu, tstclair@redhat.com", "due_date": ""}
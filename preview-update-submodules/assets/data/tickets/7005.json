{"id": 7005, "title": "Ticket #7005: Allow multiple jobs to share a GPU", "description": "<blockquote>\nWe would like to be able to have multiple jobs sharing a GPU. This works today\nin a sort of hacky way, but there are issues.\n\n<p></p><ul>\n<ol>\n<li>The total number of jobs that can share a GPU is set by configuration, but you would do this by post-processing the output of condor_gpu_discovery, or skipping the discovery step which is a bit ugly.\n\n<p></p></li><li>There is no accounting of the GPU memory, and no way for a job to claim <strong>whole</strong> GPU, if the Startd is configured to allow say 2 jobs per GPU.\n\n<p></p></li><li>Future versions of the CUDA runtime will require that processes that share a job all be owned by a single user, but there is no way to enforce this at present.\n</li></ol>\n</ul>\n\n<p>Currently you can configure a GPU to be sharable between two jobs merely by declaring that there are 2 instance of the GPU with that name.\n\n</p><p>For instance, this configuration\n</p><div class=\"snip\">\n<pre class=\"sniplabel\">configuration fragment</pre>\n<pre class=\"snip\">MACHINE_RESOURCE_GPUs = CUDA0, CUDA1, CUDA0, CUDA1\nENVIRONMENT_FOR_AssignedGPUs = CUDA_VISIBLE_DEVICES\nENVIRONMENT_VALUE_FOR_UnAssignedGPUs = 10000\n</pre></div>\n\nWill result in the STARTD being willing to create up to 4 slots each of which has one of 2 GPUs assigned.\n\n<p>Using the above configuration means that you don't get the extra attributes you get by using of using <code>condor_gpu_discovery -extra</code>.  If you want to use condor_gpu_discovery, then you would need to wrap that program in a script that rewrites the DetectedGPUs attribute to duplicate all of the devices listed there.\n\n</p><p>We could add an argument to condor_gpu_discovery telling it to over-report the GPUs it finds, to simplify the above configuration. But since the above configuration is a bit of hack, it seems like a bad idea to further entrench it.\n\n</p><p>What we really need is a way to deal with the fact that not all GPUs are the same, and that they contain fungible resources like memory and cores within themselves.\n\n</p><p>There needs to be a way to request only part of a GPU, and to have a specific GPU assigned to a slot, but only part of its internal resource set assigned to that slot.\n\n</p><p>One way to do this would be to marry a collection of sub-resources to a single GPU resource.  We might be able to get away with adding a constraint along with the request for a GPU, something like this..\n\n</p><p></p><div class=\"snip\">\n<pre class=\"sniplabel\">submit fragment</pre>\n<pre class=\"snip\">request_gpus = 1\nrequest_gpus_constraint = ResourceAttr(\"Name\") == \"Tesla\" &amp;&amp; ResouceAttr(\"AvailableMemory\") &gt; 1000\n</pre></div>\n\nThe details of this syntax and it's capabilities would need to be worked out, it is unlikely that the exact syntax above would serve the purpose, but I hope that it gets the idea across.\n\n<p>Or perhaps all we need to do is support requesting only a fraction of a GPU. like this\n\n</p><p></p><div class=\"snip\">\n<pre class=\"sniplabel\">submit fragment</pre>\n<pre class=\"snip\">request_gpus = 0.5\n</pre></div>\n\n\n<p>If there is only one pool of GPU memory (i.e. multiple GPUs share a pool of GPU memory because they are on the same physical card), then we can probably just switch from allocting GPUs to allocating GPU memory.  For instance this configuration:\n</p><div class=\"snip\">\n<pre class=\"sniplabel\">configuration fragment</pre>\n<pre class=\"snip\"># at least one instance of CUDA0 per CPU (or less to allow a maximum of N simultaneous users)\nMACHINE_RESOURCE_GPUs = CUDA0 CUDA0 CUDA0 CUDA0\nENVIRONMENT_FOR_AssignedGPUs = CUDA_VISIBLE_DEVICES\nENVIRONMENT_VALUE_FOR_UnAssignedGPUs = 10000\nMACHINE_RESOURCE_GPUMemory = 4000\n</pre></div>\n\nand this submit\n<div class=\"snip\">\n<pre class=\"sniplabel\">submit fragment</pre>\n<pre class=\"snip\">request_gpus = 1\nrequest_gpumemory = 1000\n</pre></div>\n\nwould be sufficient to allow up to 4 jobs to share a GPU, with each being allocated 1000 of the memory.  A job could claim the whole GPU merely by having\n<div class=\"snip\">\n<pre class=\"sniplabel\">submit fragment</pre>\n<pre class=\"snip\">request_gpumemory = TARGET.GPUMemory\n</pre></div>\n\nIf it were not for the fact that the CUDA_VISIBLE_DEVICES environment variable needs to be set, there would be no need have the job request a GPU at all, the request for gpumemory would be sufficient.</blockquote>", "remarks": "<blockquote>\n<em>2019-Jun-05 12:15:13 by johnkn:</em> <br/>\n\nThis works, but results in an abort of the startd on job completion.\n\n<p></p><pre>    \"/usr/sbin/condor_startd\" on \"node464.cluster.ldas.cit\" exited with status 4.\n</pre>\n\n<p></p><div class=\"snip\">\n<pre class=\"sniplabel\">from StartLog</pre>\n<pre class=\"snip\">05/21/19 20:26:30 slot1_3: Changing state: Unclaimed -&gt; Delete\n05/21/19 20:26:30 slot1_3: Resource no longer needed, deleting\n05/21/19 20:26:30 slot1_3: Unable to unset monitors in claim destructor.  The StartOfJob* attributes will be stale.  (0x1a543a0, (nil))\n05/21/19 20:28:11 ERROR \"Failed to bind local resource 'GPUS'\" at line 1238 in file /slots/10/dir_2560730/userdir/.tmpV7H12D/BUILD/condor-8.8.2/src/condor_startd.V6/ResAttributes.cpp\n05/21/19 20:28:11 slot1_2: Changing state and activity: Claimed/Busy -&gt; Preempting/Killing\n05/21/19 20:28:11 slot1_1: Changing state and activity: Claimed/Busy -&gt; Preempting/Killing\n05/21/19 20:28:11 startd exiting because of fatal exception.\n</pre></div>\n\n\n<p>I looked for this error message in my own <code>StartLog</code> and found it, so this problem is reproducible - further investigation is needed.\n\n</p><p></p><hr/>\n<em>2019-Jul-31 16:02:43 by tannenba:</em> <br/>\n\nRe the problems Sharon encountered when a GPU job sharing a device with another job completes:  TJ thinks that issue should be fixed in HTCondor v8.4.4 via the patches made in the derived ticket <span class=\"ticket\"><a class=\"resolved\" href=\"/wiki-archive/tickets/?ticket=7104\" onclick=\"get_ticket_and_populate_wrapper('7104'); return false;\" title=\"Custom resources sometimes not released from d-slot back to p-slot\">#7104</a></span>.  However, Sharon thinks there is still a problem.... as of 7/31/19 these two need to sort out what is wrong via direct communication...\n\n<p></p><hr/>\n<em>2019-Aug-14 13:15:10 by pfc:</em> <br/>\n\nWe agreed to close this ticket when all the sub-tickets are closed.\n\n<p></p><hr/>\n<em>2021-Aug-17 10:45:22 by pfc:</em> <br/>\n\nSee <a class=\"external\" href=\"https://opensciencegrid.atlassian.net/browse/HTCONDOR-457\">https://opensciencegrid.atlassian.net/browse/HTCONDOR-457</a></blockquote>", "derived_tickets": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"resolved\" href=\"/wiki-archive/tickets/?ticket=7104\" onclick=\"get_ticket_and_populate_wrapper('7104'); return false;\" title=\"Custom resources sometimes not released from d-slot back to p-slot\">#7104</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nCustom resources sometimes not released from d-slot back to p-slot</td></tr>\n</tbody></table>", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "enhance", "last_change": "2021-Aug-17 10:45", "status": "resolved", "created": "2019-Apr-17 17:38", "fixed_version": "2019-Apr-17 17:38", "broken_version": "", "priority": "2", "subsystem": "DaemonsExecNode", "assigned_to": "johnkn", "derived_from": "", "creator": "johnkn", "rust": "a97594", "customer_group": "ligo", "visibility": "public", "notify": "pcouvare@caltech.edu tannenba@cs.wisc.edu", "due_date": ""}
{"id": 3802, "title": "Ticket #3802: 8.0.0 negotiator hanging/killed", "description": "<blockquote>\n[This was sent to condor-admin but never got through, so I'm making a ticket here to track our discussion of the issue.]\n\n<p>Our negotiator was killed twice recently by the master, presumably due to a timeout.\n\n</p><p>Our pool is quite busy, although not unusually so.  Something like this happened in the spring due to MPI jobs (<span class=\"ticket\"><a class=\"defer\" href=\"/wiki-archive/tickets/?ticket=3527\" onclick=\"get_ticket_and_populate_wrapper('3527'); return false;\" title=\"condor_negotiator wegding when MPI jobs are in queue\">#3527</a></span>), but there are no MPI jobs in the queue at the moment, so I'm not sure if it's the same timeout.\n\n</p><p>I'd appreciate some help diagnosing the root cause and coming up with a fix or workaround.  I sent Greg (via email and Dropbox) the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=MasterLog\" title=\"Master Log\">MasterLog</a></span>, a recent section of the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=NegotiatorLog\" title=\"Negotiator Log\">NegotiatorLog</a></span>, the \"condor_config_val -dump\" output for the central manager, and the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=SchedLog\" title=\"Sched Log\">SchedLog</a></span> for sugar.</p></blockquote>", "remarks": "<blockquote>\n<em>2013-Jul-24 11:13:15 by pfc:</em> <br/>\n\nAt Greg's suggestion, as of today our pool is configured with:\n\n<p></p><div class=\"code\">\n<pre class=\"code\">NEGOTIATOR_MAX_TIME_PER_PIESPIN = 15 * $(MINUTE)\nNEGOTIATOR_MAX_TIME_PER_SUBMITTER = 15 * $(MINUTE)\n</pre></div>\n\n\n<p></p><hr/>\n<em>2013-Jul-29 12:37:51 by pfc:</em> <br/>\n\nI also set:\n<div class=\"code\">\n<pre class=\"code\">JOB_START_DELAY = 1\n</pre></div>\n\n...on our production schedds.\n\n<p></p><hr/>\n<em>2013-Aug-05 15:24:19 by tim:</em> <br/>\n\nTodd T noticed twice on his windows laptop negotiator CPU busy loop after rotating the negotiator log.... <em>but</em> that is not the situation here. For LIGO, the negotiator log was in the middle of giving out matches and clearly was taking to long.\n\n<p></p><hr/>\n<em>2013-Aug-06 14:26:38 by tannenba:</em> <br/>\n\nRoot cause of the trouble LIGO is seeing is their schedd is taking 3 to 4 seconds to send the next request to the matchmaker. Wtf is causing the schedd to respond so slowly?!?!\n\n<p></p><hr/>\n<em>2013-Aug-06 14:30:37 by pfc:</em> <br/>\n\nThe ball is now in Greg's court to contact Peter to schedule some interactive debugging to understand what's going on with the Schedd/Negotiator timing.\n\n<p></p><hr/>\n<em>2013-Sep-03 09:52:31 by pfc:</em> <br/>\n\nBumping down to priority==3, since we have a temporary workaround.\n\n<p></p><hr/>\n<em>2013-Sep-13 14:53:43 by pfc:</em> <br/>\n\nOne of our half-dozen schedds is on old hardware and can become very slow when many jobs are submitted to it.  Greg tells me that one overloaded schedd can cause the negotiator to hang (because the negotiator will wait indefinitely for it to finish).\n\n<p>I am not sure that this is what is causing this problem, but let's pause debugging it further until we can decomission and/or upgrade that schedd.\n\n</p><p>I.e., the ball is in our court to unblock this ticket.\n\n</p><p></p><hr/>\n<em>2013-Sep-16 08:05:00 by gthain:</em> <br/>\n\nOne knob you might want to try setting on the slow schedd is\n\n<p>MAX_ACCEPTS_PER_CYCLE=1\n\n</p><p>The default is 8, which may be one reason the schedd is taking so long to get back to the negotiator.\n\n</p><p>This knob just requires a reconfig, not a restart of the slow schedd.\n\n</p><p></p><hr/>\n<em>2013-Sep-16 08:43:49 by gthain:</em> <br/>\n\nNote that, when looking at the schedd in question, the load average was &gt; 17.  Now, most of this load was disk i/o, so it is a bit hard to reason about, but if there are a lot of requests queued to same disk that the schedd is trying to write job log events to, it will slow down the whole process.\n\n<p></p><hr/>\n<em>2013-Sep-16 08:53:20 by pfc:</em> <br/>\n\nThe jobs' userlogs are all on a local disk, however the DAGMan nodes log is on NFS.\n\n<p></p><hr/>\n<em>2013-Sep-16 15:41:12 by gthain:</em> <br/>\n\n\n<p>This is the setting in the DAGMan config file setting:\n\n</p><p>DAGMAN_DEFAULT_NODE_LOG\n\n</p><p>?  Perhaps we should explicitly put this on a local disk instead of NFS?\n\n</p><p></p><hr/>\n<em>2014-Feb-05 10:50:23 by tannenba:</em> <br/>\n\nAction item on how to improve:\n\n<p>Lets set the default keep-alive to the master to be longer than the default longest allowable negotiation cycle time.  This way there are only changes in the param table in the stable series....</p></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "defect", "last_change": "2015-Sep-22 14:53", "status": "defer", "created": "2013-Jul-24 11:12", "fixed_version": "2013-Jul-24 11:12", "broken_version": "v080000", "priority": "5", "subsystem": "DaemonsCM", "assigned_to": "gthain", "derived_from": "#3527", "creator": "pfc", "rust": "", "customer_group": "ligo", "visibility": "public", "notify": "pfcouvar@syr.edu tannenba@cs.wisc.edu,pcouvare@caltech.edu", "due_date": ""}
diff --git a/doc/admin-man/configure.tex b/doc/admin-man/configure.tex
index 6d3dde1..d867386 100644
--- a/doc/admin-man/configure.tex
+++ b/doc/admin-man/configure.tex
@@ -10228,6 +10228,13 @@ in section~\ref{sec:DAG-configuration}.
 
 \label{param:DAGManStorkSubmitExe}
 \item[\Macro{DAGMAN\_STORK\_SUBMIT\_EXE}]
+  This configuration variable is no longer used, because as of
+  version 8.3.4 \Condor{dagman} no longer supports Stork jobs.
+  Setting this configuration variable will result in a warning
+  from \Condor{dagman} (which will be turned into a fatal error
+  if \MacroNI{DAGMAN\_USE\_STRICT} is set to 1 or above).
+
+  For completeness, here is the definition for historical purposes: 
   The executable that \Condor{dagman} will use to submit Stork jobs.
   If not defined, \Condor{dagman} looks for \Prog{stork\_submit} in the path.
 
@@ -10238,6 +10245,13 @@ in section~\ref{sec:DAG-configuration}.
 
 \label{param:DAGManStorkRmExe}
 \item[\Macro{DAGMAN\_STORK\_RM\_EXE}]
+  This configuration variable is no longer used, because as of
+  version 8.3.4 \Condor{dagman} no longer supports Stork jobs.
+  Setting this configuration variable will result in a warning
+  from \Condor{dagman} (which will be turned into a fatal error
+  if \MacroNI{DAGMAN\_USE\_STRICT} is set to 1 or above).
+
+  For completeness, here is the definition for historical purposes: 
   The executable that \Condor{dagman} will use to remove Stork jobs.
   If not defined, \Condor{dagman} looks for \Prog{stork\_rm} in the path.
 
@@ -10581,11 +10595,13 @@ in section~\ref{sec:DAG-configuration}.
 
 \label{param:DAGmanAlwaysUseNodeLog}
 \item[\Macro{DAGMAN\_ALWAYS\_USE\_NODE\_LOG}]
-  As of HTCondor version 8.3.1, the value is always the default 
+  As of HTCondor version 8.3.1, the value must always be the default 
   value of \Expr{True}.  
-  Attempts to set it to \Expr{False} results in an error. 
+  Attempting to set it to \Expr{False} results in an error. 
   This causes incompatibility with using a \Condor{submit} executable 
   that is older than HTCondor version 7.9.0.
+
+  For completeness, here is the definition for historical purposes: 
   A boolean value that when \Expr{True}  causes \Condor{dagman} to read
   events from its default node log file, as defined by
   \Macro{DAGMAN\_DEFAULT\_NODE\_LOG}, instead of from the log file(s)
diff --git a/doc/user-man/dagman.tex b/doc/user-man/dagman.tex
index aad07a5..eff8484 100644
--- a/doc/user-man/dagman.tex
+++ b/doc/user-man/dagman.tex
@@ -1,3 +1,4 @@
+%TEMPTEMP -- remove all references to Stork once this gets merged w/ Karen's update
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \section{\label{sec:DAGMan}DAGMan Applications}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@@ -78,7 +79,7 @@ The input file used by DAGMan is called a DAG input file.
 It specifies the nodes of the DAG as well as the dependencies
 that order the DAG.
 All items are optional, except that there must be at least one \Arg{JOB}
-or \Arg{DATA} item.
+item.
 
 Comments may be placed in the DAG input file.
 The pound character (\verb@#@) as the first character on a
@@ -199,43 +200,7 @@ and the HTCondor job will not be submitted.
 \index{DAGMan input file!DATA key word}
 \item \Bold{DATA}
 
-The \Arg{DATA} key word specifies a job to be managed by the Stork data
-placement server.  
-Stork software is provided by the Stork project.
-Please refer to their website: 
-\URL{http://www.cct.lsu.edu/~kosar/stork/index.php}.
-
-The syntax used for each \Arg{DATA} entry is
-
-\Opt{DATA} \Arg{JobName} \Arg{SubmitDescriptionFileName}
-\oOptArg{DIR}{directory} \oOpt{NOOP} \oOpt{DONE}
-
-A \Arg{DATA} entry maps a \Arg{JobName} to a Stork submit description file.
-In all other respects, the \Arg{DATA} key word is identical to the
-\Arg{JOB} key word.
-
-The keywords \Arg{DIR}, \Arg{NOOP} and \Arg{DONE} 
-follow the same rules and restrictions, and they have the same effect
-for \Opt{DATA} nodes as they do for \Opt{JOB} nodes.
-
-Here is an example of a simple DAG that stages in data using Stork,
-processes the data using HTCondor, 
-and stages the processed data out using Stork.
-Depending upon the implementation, multiple data jobs to stage in data
-or to stage out data
-may be run in parallel.
-
-\footnotesize
-\begin{verbatim}
-    DATA    STAGE_IN1  stage_in1.stork
-    DATA    STAGE_IN2  stage_in2.stork
-    JOB     PROCESS    process.condor 
-    DATA    STAGE_OUT1 stage_out1.stork
-    DATA    STAGE_OUT2 stage_out2.stork
-    PARENT  STAGE_IN1 STAGE_IN2 CHILD PROCESS
-    PARENT  PROCESS CHILD STAGE_OUT1 STAGE_OUT2
-\end{verbatim}
-\normalsize
+As of version 8.3.4, \Condor{dagman} no longer supports DATA nodes.
 
 %----------------------------
 \label{dagman:ParentChild}
diff --git a/doc/version-history/8-3.history.tex b/doc/version-history/8-3.history.tex
index 7283869..2cbca59 100644
--- a/doc/version-history/8-3.history.tex
+++ b/doc/version-history/8-3.history.tex
@@ -33,6 +33,16 @@ The details of each version are described below.
 file when node jobs go on hold.
 \Ticket{4766}
 
+%TEMPTEMP -- this needs to get moved into the 8.3.4 section once things are merged
+\item \Condor{dagman} no longer supports Stork jobs as DAG nodes.
+\Ticket{4550}
+
+%TEMPTEMP -- this needs to get moved into the 8.3.4 section once things are merged
+\item \Condor{dagman} no longer has the capability to read individual per-
+job log files.  This means that recovery mode will no longer work on a
+DAG originally submitted with version 7.9.1 or earlier.
+\Ticket{4528}
+
 \end{itemize}
 
 \noindent Bugs Fixed:
diff --git a/src/condor_dagman/condor_submit_dag.cpp b/src/condor_dagman/condor_submit_dag.cpp
index 2bd725e..9d80e4f 100644
--- a/src/condor_dagman/condor_submit_dag.cpp
+++ b/src/condor_dagman/condor_submit_dag.cpp
@@ -838,9 +838,6 @@ void writeSubmitFile(/* const */ SubmitDagDeepOptions &deepOpts,
 	args.AppendArg(deepOpts.autoRescue);
 	args.AppendArg("-DoRescueFrom");
 	args.AppendArg(deepOpts.doRescueFrom);
-	if(!deepOpts.always_use_node_log) {
-		args.AppendArg("-dont_use_default_node_log");
-	}
 
 	shallowOpts.dagFiles.rewind();
 	while ( (dagFile = shallowOpts.dagFiles.next()) != NULL ) {
diff --git a/src/condor_dagman/dag.cpp b/src/condor_dagman/dag.cpp
index 82bbec1..5b16292 100644
--- a/src/condor_dagman/dag.cpp
+++ b/src/condor_dagman/dag.cpp
@@ -22,7 +22,7 @@
 // We are calling a *node* the combination of pre-script, job, and
 // post-script.
 // We are calling a *job* essentially what results from one invocation
-// of condor_submit or stork_submit.
+// of condor_submit.
 // We are calling a *job proc* an individual batch system process within
 // a cluster.
 // So nodes-to-jobs is 1-to-1; jobs to job procs is 1-to-n.
@@ -88,7 +88,7 @@ Dag::Dag( /* const */ StringList &dagFiles,
 		  bool allowLogError,
 		  bool useDagDir, int maxIdleJobProcs, bool retrySubmitFirst,
 		  bool retryNodeFirst, const char *condorRmExe,
-		  const char *storkRmExe, const CondorID *DAGManJobID,
+		  const CondorID *DAGManJobID,
 		  bool prohibitMultiJobs, bool submitDepthFirst,
 		  const char *defaultNodeLog, bool generateSubdagSubmits,
 		  SubmitDagDeepOptions *submitDagDeepOpts, bool isSplice,
@@ -103,7 +103,6 @@ Dag::Dag( /* const */ StringList &dagFiles,
 	_nodeNameHash		  (NODE_HASH_SIZE, MyStringHash, rejectDuplicateKeys),
 	_nodeIDHash			  (NODE_HASH_SIZE, hashFuncInt, rejectDuplicateKeys),
 	_condorIDHash		  (NODE_HASH_SIZE, hashFuncInt, rejectDuplicateKeys),
-	_storkIDHash		  (NODE_HASH_SIZE, hashFuncInt, rejectDuplicateKeys),
 	_noopIDHash			  (NODE_HASH_SIZE, hashFuncInt, rejectDuplicateKeys),
     _numNodesDone         (0),
     _numNodesFailed       (0),
@@ -116,12 +115,10 @@ Dag::Dag( /* const */ StringList &dagFiles,
 	m_retrySubmitFirst	  (retrySubmitFirst),
 	m_retryNodeFirst	  (retryNodeFirst),
 	_condorRmExe		  (condorRmExe),
-	_storkRmExe			  (storkRmExe),
 	_DAGManJobId		  (DAGManJobID),
 	_preRunNodeCount	  (0),
 	_postRunNodeCount	  (0),
 	_checkCondorEvents    (),
-	_checkStorkEvents     (),
 	_maxJobsDeferredCount (0),
 	_maxIdleDeferredCount (0),
 	_catThrottleDeferredCount (0),
@@ -137,7 +134,6 @@ Dag::Dag( /* const */ StringList &dagFiles,
 	_reject			  (false),
 	_alwaysRunPost		  (true),
 	_defaultPriority	  (0),
-	_use_default_node_log  (true),
 	_metrics			  (NULL)
 {
 
@@ -212,20 +208,6 @@ Dag::Dag( /* const */ StringList &dagFiles,
 	_dagFiles.rewind();
 	_haltFile = HaltFileName( _dagFiles.next() );
 	_dagStatus = DAG_STATUS_OK;
-	if( submitDagDeepOpts ) {
-		_use_default_node_log = submitDagDeepOpts->always_use_node_log;
-	}
-	_nfsLogIsError = param_boolean( "DAGMAN_LOG_ON_NFS_IS_ERROR", true );
-	if(_nfsLogIsError) {
-		bool userlog_locking = param_boolean( "ENABLE_USERLOG_LOCKING", true );
-		if(userlog_locking) {
-			bool locks_on_local = param_boolean( "CREATE_LOCKS_ON_LOCAL_DISK", true);
-			if(locks_on_local) {
-				dprintf( D_ALWAYS, "Ignoring value of DAGMAN_LOG_ON_NFS_IS_ERROR.\n");
-				_nfsLogIsError = false;
-			}
-		}
-	}
 
 	return;
 }
@@ -233,6 +215,10 @@ Dag::Dag( /* const */ StringList &dagFiles,
 //-------------------------------------------------------------------------
 Dag::~Dag()
 {
+	if ( _condorLogRdr.activeLogFileCount() > 0 ) {
+		(void) UnmonitorLogFile();
+	}
+
 		// remember kids, delete is safe *even* if ptr == NULL...
 
     // delete all jobs in _jobs
@@ -294,9 +280,11 @@ bool Dag::Bootstrap (bool recovery)
     debug_printf( DEBUG_VERBOSE, "Number of pre-completed nodes: %d\n",
 				  NumNodesDone( true ) );
     
-    if (recovery) {
-		_recovery = true;
+	_recovery = recovery;
+
+	(void) MonitorLogFile();
 
+    if (recovery) {
         debug_printf( DEBUG_NORMAL, "Running in RECOVERY mode... "
 					">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n" );
 		_jobstateLog.WriteRecoveryStarted();
@@ -312,32 +300,8 @@ bool Dag::Bootstrap (bool recovery)
 
 		debug_cache_start_caching();
 
-			// Here we're monitoring the log files of all ready nodes.
-   		jobs.ToBeforeFirst();
-   		while( jobs.Next( job ) ) {
-			if ( job->CanSubmit() ) {
-				if ( !job->MonitorLogFile( _condorLogRdr, _storkLogRdr,
-							_nfsLogIsError, recovery, _defaultNodeLog, _use_default_node_log ) ) {
-					debug_cache_stop_caching();
-					_jobstateLog.WriteRecoveryFailure();
-					return false;
-				}
-			}
-		}
-
-			// Note: I just realized that this will almost certainly fail
-			// on a combination of Condor and Stork events -- we probably
-			// need a loop around the event processing.  wenger 2009-06-18.
 		if( CondorLogFileCount() > 0 ) {
-			if( !ProcessLogEvents( CONDORLOG, recovery ) ) {
-				_recovery = false;
-				debug_cache_stop_caching();
-				_jobstateLog.WriteRecoveryFailure();
-				return false;
-			}
-		}
-		if( StorkLogFileCount() > 0 ) {
-			if( !ProcessLogEvents( DAPLOG, recovery ) ) {
+			if( !ProcessLogEvents( recovery ) ) {
 				_recovery = false;
 				debug_cache_stop_caching();
 				_jobstateLog.WriteRecoveryFailure();
@@ -467,29 +431,11 @@ Dag::DetectCondorLogGrowth () {
 }
 
 //-------------------------------------------------------------------------
-bool Dag::DetectDaPLogGrowth () {
-
-	if( StorkLogFileCount() <= 0 ) {
-		return false;
-	}
-
-	bool growth = _storkLogRdr.detectLogGrowth();
-    debug_printf( DEBUG_DEBUG_4, "%s\n",
-				  growth ? "Log GREW!" : "No log growth..." );
-    return growth;
-}
-
-//-------------------------------------------------------------------------
 // Developer's Note: returning false tells main_timer to abort the DAG
-bool Dag::ProcessLogEvents (int logsource, bool recovery) {
+bool Dag::ProcessLogEvents (bool recovery) {
 
-	if ( logsource == CONDORLOG ) {
-		debug_printf( DEBUG_VERBOSE, "Currently monitoring %d Condor "
-					"log file(s)\n", _condorLogRdr.activeLogFileCount() );
-	} else if ( logsource == DAPLOG ) {
-		debug_printf( DEBUG_VERBOSE, "Currently monitoring %d Stork "
-					"log file(s)\n", _storkLogRdr.activeLogFileCount() );
-	}
+	debug_printf( DEBUG_VERBOSE, "Currently monitoring %d Condor "
+				"log file(s)\n", _condorLogRdr.activeLogFileCount() );
 
 	bool done = false;  // Keep scanning until ULOG_NO_EVENT
 	bool result = true;
@@ -498,13 +444,9 @@ bool Dag::ProcessLogEvents (int logsource, bool recovery) {
 		ULogEvent* e = NULL;
 		ULogEventOutcome outcome = ULOG_NO_EVENT;
 
-		if ( logsource == CONDORLOG ) {
-			outcome = _condorLogRdr.readEvent(e);
-		} else if ( logsource == DAPLOG ){
-			outcome = _storkLogRdr.readEvent(e);
-		}
+		outcome = _condorLogRdr.readEvent(e);
 
-		bool tmpResult = ProcessOneEvent( logsource, outcome, e, recovery,
+		bool tmpResult = ProcessOneEvent( outcome, e, recovery,
 					done );
 			// If ProcessOneEvent returns false, the result here must
 			// be false.
@@ -517,7 +459,7 @@ bool Dag::ProcessLogEvents (int logsource, bool recovery) {
 	}
 
 	if (DEBUG_LEVEL(DEBUG_VERBOSE) && recovery) {
-		const char *name = (logsource == CONDORLOG) ? "Condor" : "Stork";
+		const char *name = "Condor";
 		debug_printf( DEBUG_NORMAL, "    ------------------------------\n");
 		debug_printf( DEBUG_NORMAL, "       %s Recovery Complete\n", name);
 		debug_printf( DEBUG_NORMAL, "    ------------------------------\n");
@@ -536,7 +478,7 @@ bool Dag::ProcessLogEvents (int logsource, bool recovery) {
 
 //---------------------------------------------------------------------------
 // Developer's Note: returning false tells main_timer to abort the DAG
-bool Dag::ProcessOneEvent (int logsource, ULogEventOutcome outcome,
+bool Dag::ProcessOneEvent (ULogEventOutcome outcome,
 		const ULogEvent *event, bool recovery, bool &done) {
 
 	bool result = true;
@@ -592,14 +534,14 @@ bool Dag::ProcessOneEvent (int logsource, ULogEventOutcome outcome,
 		{
 			ASSERT( event != NULL );
 			bool submitEventIsSane;
-			Job *job = LogEventNodeLookup( logsource, event,
+			Job *job = LogEventNodeLookup( event,
 						submitEventIsSane );
 			PrintEvent( DEBUG_VERBOSE, event, job, recovery );
 			if( !job ) {
 					// event is for a job outside this DAG; ignore it
 				break;
 			}
-			if( !EventSanityCheck( logsource, event, job, &result ) ) {
+			if( !EventSanityCheck( event, job, &result ) ) {
 					// this event is "impossible"; we will either
 					// abort the DAG (if result was set to false) or
 					// ignore it and hope for the best...
@@ -687,7 +629,6 @@ bool Dag::ProcessOneEvent (int logsource, ULogEventOutcome outcome,
 
 			case ULOG_PRESKIP:
 				TerminateJob( job, recovery );
-				job->UnmonitorLogFile( _condorLogRdr, _storkLogRdr );
 				if(!recovery) {
 					--_preRunNodeCount;
 				}
@@ -876,30 +817,16 @@ Dag::RemoveBatchJob(Job *node) {
 	ArgList args;
 	MyString constraint;
 
-	switch ( node->JobType() ) {
-	case Job::TYPE_CONDOR:
-		args.AppendArg( _condorRmExe );
-		args.AppendArg( "-const" );
-
-			// Adding this DAGMan's cluster ID as a constraint to
-			// be extra-careful to avoid removing someone else's
-			// job.
-		constraint.formatstr( "%s =?= %d && %s =?= %d",
-					ATTR_DAGMAN_JOB_ID, _DAGManJobId->_cluster,
-					ATTR_CLUSTER_ID, node->GetCluster() );
-		args.AppendArg( constraint.Value() );
-		break;
-
-	case Job::TYPE_STORK:
-		args.AppendArg( _storkRmExe );
-		args.AppendArg( node->GetCluster() );
-		break;
+	args.AppendArg( _condorRmExe );
+	args.AppendArg( "-const" );
 
-	default:
-		EXCEPT( "Illegal job (%d) type for node %s", node->JobType(),
-					node->GetJobName() );
-		break;
-	}
+		// Adding this DAGMan's cluster ID as a constraint to
+		// be extra-careful to avoid removing someone else's
+		// job.
+	constraint.formatstr( "%s =?= %d && %s =?= %d",
+				ATTR_DAGMAN_JOB_ID, _DAGManJobId->_cluster,
+				ATTR_CLUSTER_ID, node->GetCluster() );
+	args.AppendArg( constraint.Value() );
 	
 	MyString display;
 	args.GetArgsStringForDisplay( &display );
@@ -923,8 +850,6 @@ Dag::ProcessJobProcEnd(Job *job, bool recovery, bool failed) {
 	ASSERT ( _isSplice == false );
 
 	if ( job->_queuedNodeJobProcs == 0 ) {
-		(void)job->UnmonitorLogFile( _condorLogRdr, _storkLogRdr );
-
 			// Log job success or failure if necessary.
 		_jobstateLog.WriteJobSuccessOrFailure( job );
 	}
@@ -969,8 +894,6 @@ Dag::ProcessJobProcEnd(Job *job, bool recovery, bool failed) {
 			if ( recovery ) {
 				job->SetStatus( Job::STATUS_POSTRUN );
 				_postRunNodeCount++;
-				(void)job->MonitorLogFile( _condorLogRdr, _storkLogRdr,
-						_nfsLogIsError, _recovery, _defaultNodeLog, _use_default_node_log );
 			} else {
 				(void)RunPostScript( job, _alwaysRunPost, 0 );
 			}
@@ -988,8 +911,6 @@ Dag::ProcessPostTermEvent(const ULogEvent *event, Job *job,
 		bool recovery) {
 
 	if( job ) {
-		(void)job->UnmonitorLogFile( _condorLogRdr, _storkLogRdr );
-
 			// Note: "|| recovery" below is somewhat of a "quick and dirty"
 			// fix to Gnats PR 357.  The first part of the assert can fail
 			// in recovery mode because if any retries of a node failed
@@ -1367,7 +1288,7 @@ Dag::NodeExists( const char* nodeName ) const
 }
 
 //---------------------------------------------------------------------------
-Job * Dag::FindNodeByEventID ( int logsource, const CondorID condorID ) const {
+Job * Dag::FindNodeByEventID ( const CondorID condorID ) const {
 	if ( condorID._cluster == -1 ) {
 		return NULL;
 	}
@@ -1375,7 +1296,7 @@ Job * Dag::FindNodeByEventID ( int logsource, const CondorID condorID ) const {
 	Job *	node = NULL;
 	bool isNoop = JobIsNoop( condorID );
 	int id = GetIndexID( condorID );
-	if ( GetEventIDHash( isNoop, logsource )->lookup(id, node) != 0 ) {
+	if ( GetEventIDHash( isNoop )->lookup(id, node) != 0 ) {
 			// Note: eventually get rid of the "(might be because of
 			// node retries)" message here, and have code that explicitly
 			// figures out whether the node was not found because of a
@@ -1415,7 +1336,6 @@ void
 Dag::SetAllowEvents( int allowEvents)
 {
 	_checkCondorEvents.SetAllowEvents( allowEvents );
-	_checkStorkEvents.SetAllowEvents( allowEvents );
 }
 
 //-------------------------------------------------------------------------
@@ -1559,7 +1479,6 @@ Dag::SubmitReadyJobs(const Dagman &dm)
 		_preScriptQ->RunAllWaitingScripts();
 	}
 
-	bool didLogSleep = false;
 	while( numSubmitsThisCycle < dm.max_submits_per_interval ) {
 
 //		PrintReadyQ( DEBUG_DEBUG_4 );
@@ -1616,36 +1535,6 @@ Dag::SubmitReadyJobs(const Dagman &dm)
 			deferredJobs.Prepend( job, -job->_nodePriority );
 			_catThrottleDeferredCount++;
 		} else {
-				// The problem here is that we wouldn't need to sleep if
-				// we only have one total log file *after* we monitor the
-				// log file for the job we're going to submit.  I guess
-				// we could be smarter and move this test somewhere else,
-				// but I'm not going to deal with that right now.
-				// wenger 2009-05-27
-
-				// Okay, we're just going to skip the sleep if we're
-				// using the single workflow log file.  I think we
-				// shouldn't worry about being smart in any other cases.
-				// wenger 2013-01-24
-			if( !_use_default_node_log && dm.submit_delay == 0 && !didLogSleep ) {
-					// if we don't already have a submit_delay, sleep for one
-					// second here, so we can be sure that this job's submit
-					// event will be unambiguously later than the termination
-					// events of its parents, given that userlog timestamps
-					// only have a resolution of one second.  (Because of the
-					// new feature allowing distinct userlogs for each node, we
-					// can't just rely on the physical order in a single log
-					// file.)
-
-					// We sleep at most once per submit cycle, because
-					// we don't really have to worry about getting events
-					// for "sibling" nodes in the exact order they occurred.
-				debug_printf( DEBUG_VERBOSE,
-							"Sleeping for one second for log "
-							"file consistency\n" );
-				sleep( 1 );
-				didLogSleep = true;
-			}
 
     		CondorID condorID(0,0,0);
 			submit_result_t submit_result = SubmitNodeJob( dm, job, condorID );
@@ -1734,17 +1623,8 @@ Dag::PreScriptReaper( Job *job, int status )
 
 				// Mark the node as a skipped node.
 			CondorID id;
-				// This might be the first time we watch the file, so we
-				// monitor it.
-			if ( !job->MonitorLogFile( _condorLogRdr, _storkLogRdr, _nfsLogIsError,
-					_recovery, _defaultNodeLog, _use_default_node_log ) ) {
-				return 0;
-			}
-				
-			if(!writePreSkipEvent( id, job, job->GetJobName(),
-					job->GetDirectory(), _use_default_node_log ? DefaultNodeLog():
-					job->GetLogFile() , !_use_default_node_log
-					&& job->GetLogFileIsXml() ) ) {
+			if ( !writePreSkipEvent( id, job, job->GetJobName(),
+					job->GetDirectory(), DefaultNodeLog() ) ) {
 				debug_printf( DEBUG_NORMAL, "Failed to write PRE_SKIP event for node %s\n",
 					job->GetJobName() );
 				main_shutdown_rescue( EXIT_ERROR, DAG_STATUS_ERROR );
@@ -1822,13 +1702,6 @@ bool Dag::RunPostScript( Job *job, bool ignore_status, int status,
 	// a POST script is specified for the job, so run it
 	// We are told to ignore the result of the PRE script
 	job->SetStatus( Job::STATUS_POSTRUN );
-	if ( !job->MonitorLogFile( _condorLogRdr, _storkLogRdr,
-			_nfsLogIsError, _recovery, _defaultNodeLog, _use_default_node_log ) ) {
-		debug_printf(DEBUG_QUIET, "Unable to monitor user logfile for node %s\n",
-			job->GetJobName() );
-		debug_printf(DEBUG_QUIET, "Not running the POST script\n" );
-		return false;
-	}
 	if ( incrementRunCount ) {
 		_postRunNodeCount++;
 	}
@@ -1850,79 +1723,51 @@ Dag::PostScriptReaper( Job *job, int status )
 			job->GetJobName() );
 	}
 
-	PostScriptTerminatedEvent e;
+	PostScriptTerminatedEvent event;
 	
-	e.dagNodeName = strnewp( job->GetJobName() );
+	event.dagNodeName = strnewp( job->GetJobName() );
 
 	if( WIFSIGNALED( status ) ) {
 		debug_printf( DEBUG_QUIET, "POST script died on signal %d\n", status );
-		e.normal = false;
-		e.signalNumber = status;
-	} else {
-		e.normal = true;
-		e.returnValue = WEXITSTATUS( status );
-	}
-
-		// Note: after 6.7.15 is released, we'll be disabling the old-style
-		// Stork logs, so we should probably go ahead and write the POST
-		// script terminated events for Stork jobs here (although that
-		// could cause some backwards-compatibility problems).  wenger
-		// 2006-01-12.
-	if ( job->JobType() == Job::TYPE_STORK ) {
-			// Kludgey fix for Gnats PR 554 -- we are bypassing the whole
-			// writing of the ULOG_POST_SCRIPT_TERMINATED event because
-			// Stork doesn't use the UserLog code, and therefore presumably
-			// doesn't have the correct file locking on the log file.
-			// This means that in recovery mode we'll end up running this
-			// POST script again even if we successfully ran it already.
-			// wenger 2005-10-04.
-		e.cluster = job->GetCluster();
-		e.proc = job->GetProc();
-		e.subproc = job->GetSubProc();
-		ProcessPostTermEvent(&e, job, _recovery);
+		event.normal = false;
+		event.signalNumber = status;
 	} else {
-
-		e.cluster = job->GetCluster();
-		e.proc = job->GetProc();
-		e.subproc = job->GetSubProc();
-		WriteUserLog ulog;
-			// Disabling the global log (EventLog) fixes the main problem
-			// in gittrac #934 (if you can't write to the global log the
-			// write to the user log also fails, and DAGMan hangs
-			// waiting for the event that wasn't written).
-		ulog.setEnableGlobalLog( false );
-		ulog.setUseXML( !_use_default_node_log && job->GetLogFileIsXml() );
-			// For NOOP jobs, we need the proc and subproc values;
-			// for "real" jobs, they are not significant.
-		int procID = job->GetNoop() ? job->GetProc() : 0;
-		int subprocID = job->GetNoop() ? job->GetSubProc() : 0;
-		const char* s = _use_default_node_log ? DefaultNodeLog() :
-			job->GetLogFile();
-		if( !s ) { 
-				// User did not specify a log
-				// We specify one for him
-				// Default log is never in XML format
-			s = DefaultNodeLog();
-			ulog.setUseXML( false );
-		}
-		debug_printf( DEBUG_QUIET, "Initializing logfile %s, %d, %d, %d\n",
-			s, job->GetCluster(), procID, subprocID );
-		ulog.initialize( std::vector<const char*>(1,s), job->GetCluster(),
-			procID, subprocID, NULL );
-
-		for(int write_attempts = 0;;++write_attempts) {
-			if( !ulog.writeEvent( &e ) ) {
-				if( write_attempts >= 2 ) {
-					debug_printf( DEBUG_QUIET,
-							"Unable to log ULOG_POST_SCRIPT_TERMINATED event\n" );
-					// Exit here, because otherwise we'll wait forever to see
-					// the event that we just failed to write (see gittrac #934).
-					// wenger 2009-11-12.
-					main_shutdown_rescue( EXIT_ERROR, DAG_STATUS_ERROR );
-				}
-			} else {
-				break;
+		event.normal = true;
+		event.returnValue = WEXITSTATUS( status );
+	}
+
+		// For NOOP jobs, we need the proc and subproc values;
+		// for "real" jobs, they are not significant.
+	event.cluster = job->GetCluster();
+	event.proc = job->GetNoop() ? job->GetProc() : 0;
+	event.subproc = job->GetNoop() ? job->GetSubProc() : 0;
+
+	WriteUserLog ulog;
+		// Disabling the global log (EventLog) fixes the main problem
+		// in gittrac #934 (if you can't write to the global log the
+		// write to the user log also fails, and DAGMan hangs
+		// waiting for the event that wasn't written).
+	ulog.setEnableGlobalLog( false );
+	ulog.setUseXML( false );
+
+	debug_printf( DEBUG_QUIET,
+				"Initializing user log writer for %s, (%d.%d.%d)\n",
+				DefaultNodeLog(), event.cluster, event.proc, event.subproc );
+	ulog.initialize( DefaultNodeLog(), event.cluster, event.proc,
+				event.subproc, NULL );
+
+	for(int write_attempts = 0;;++write_attempts) {
+		if( !ulog.writeEvent( &event ) ) {
+			if( write_attempts >= 2 ) {
+				debug_printf( DEBUG_QUIET,
+						"Unable to log ULOG_POST_SCRIPT_TERMINATED event\n" );
+				// Exit here, because otherwise we'll wait forever to see
+				// the event that we just failed to write (see gittrac #934).
+				// wenger 2009-11-12.
+				main_shutdown_rescue( EXIT_ERROR, DAG_STATUS_ERROR );
 			}
+		} else {
+			break;
 		}
 	}
 	return true;
@@ -2088,27 +1933,15 @@ void Dag::RemoveRunningJobs ( const CondorID &dmJobId, bool removeCondorJobs,
 {
 	if ( bForce ) removeCondorJobs = true;
 
-	const char *conJobs = removeCondorJobs ? "Condor/" : "";
-	debug_printf( DEBUG_NORMAL, "Removing any/all submitted %s"
-				"Stork jobs...\n", conJobs );
-
-		// first, remove all Condor jobs submitted by this DAGMan
-		// Make sure we have at least one Condor (not Stork) job before
-		// we call condor_rm...
-	bool	haveCondorJob = bForce;
-    ListIterator<Job> jobList(_jobs);
-    Job * job;
-    while (jobList.Next(job)) {
-		ASSERT( job != NULL );
-		if ( job->JobType() == Job::TYPE_CONDOR ) {
-			haveCondorJob = true;
-			break;
-		}
-	}
+	// Hmm -- should we check here if we have jobs queued? wenger 2014-12-17
+	bool	haveCondorJob = true;
 
 	ArgList args;
 
 	if ( removeCondorJobs && haveCondorJob ) {
+		debug_printf( DEBUG_NORMAL, "Removing any/all submitted "
+					"Condor jobs...\n" );
+
 		MyString constraint;
 
 		args.Clear();
@@ -2123,26 +1956,6 @@ void Dag::RemoveRunningJobs ( const CondorID &dmJobId, bool removeCondorJobs,
 		}
 	}
 
-		// Okay, now remove any Stork jobs.
-    ListIterator<Job> iList(_jobs);
-    while (iList.Next(job)) {
-		ASSERT( job != NULL );
-			// if node has a Stork job that is presently submitted,
-			// remove it individually (this is necessary because
-			// DAGMan's job ID can't currently be inserted into the
-			// Stork job ad, and thus we can't do a "stork_rm -const..." 
-			// like we do with Condor; this should be fixed)
-		if( job->JobType() == Job::TYPE_STORK &&
-			job->GetStatus() == Job::STATUS_SUBMITTED ) {
-			args.Clear();
-			args.AppendArg( _storkRmExe );
-			args.AppendArg( job->GetCluster() );
-			if ( util_popen( args ) != 0 ) {
-				debug_printf( DEBUG_NORMAL, "Error removing Stork job\n");
-			}
-        }
-	}
-
 	return;
 }
 
@@ -2353,12 +2166,8 @@ Dag::WriteNodeToRescue( FILE *fp, Job *node, bool reset_retries_upon_rescue,
 	const char *keyword = "";
 	if ( node->GetFinal() ) {
 		keyword = "FINAL";
-	} else if ( node->JobType() == Job::TYPE_CONDOR ) {
-		keyword = node->GetDagFile() ? "SUBDAG EXTERNAL" : "JOB";
-	} else if( node->JobType() == Job::TYPE_STORK ) {
-		keyword = "DATA";
 	} else {
-		EXCEPT( "Illegal node type (%d)", node->JobType() );
+		keyword = node->GetDagFile() ? "SUBDAG EXTERNAL" : "JOB";
 	}
 
 	if ( !isPartial ) {
@@ -2523,12 +2332,7 @@ Dag::TerminateJob( Job* job, bool recovery, bool bootstrap )
 				// If we're bootstrapping, we don't want to do anything
 				// here.
 			if ( !bootstrap ) {
-				if ( recovery ) {
-						// We need to monitor the log file for the node that's
-						// newly ready.
-					(void)child->MonitorLogFile( _condorLogRdr, _storkLogRdr,
-								_nfsLogIsError, recovery, _defaultNodeLog, _use_default_node_log);
-				} else {
+				if ( !recovery ) {
 						// If child has no more parents in its waiting queue,
 						// submit it.
 					StartNode( child, false );
@@ -2604,10 +2408,8 @@ Dag::RestartNode( Job *node, bool recovery )
 			// Note: the if checking against the default condor ID
 			// should *always* be true here, but checking just to be safe.
 		if ( !(node->GetID() == _defaultCondorId) ) {
-			int logsource = node->JobType() == Job::TYPE_CONDOR ? CONDORLOG :
-						DAPLOG;
 			int id = GetIndexID( node->GetID() );
-			if ( GetEventIDHash( node->GetNoop(), logsource )->remove( id )
+			if ( GetEventIDHash( node->GetNoop() )->remove( id )
 						!= 0 ) {
 				EXCEPT( "Event ID hash table error!" );
 			}
@@ -2617,8 +2419,6 @@ Dag::RestartNode( Job *node, bool recovery )
 		// has retried nodes).  (See SubmitNodeJob() for where this
 		// gets done during "normal" running.)
 		node->SetCondorID( _defaultCondorId );
-		(void)node->MonitorLogFile( _condorLogRdr, _storkLogRdr,
-					_nfsLogIsError, recovery, _defaultNodeLog, _use_default_node_log );
 	}
 }
 
@@ -3086,7 +2886,6 @@ Dag::DumpNodeStatus( bool held, bool removed )
 				// ready to submit if it doesn't have any unfinished
 				// parents.
 			if ( !node->CanSubmit() ) {
-				// See Job::_job_type_names for other strings.
 				status = Job::STATUS_NOT_READY;
 			}
 
@@ -3208,6 +3007,48 @@ Dag::EscapeClassadString( const char* strIn )
 	return tmpStr.c_str();
 }
 
+//---------------------------------------------------------------------------
+bool
+Dag::MonitorLogFile()
+{
+	debug_printf( DEBUG_DEBUG_2,
+				"Attempting to monitor log file <%s>\n", _defaultNodeLog );
+
+	CondorError errstack;
+	if ( !_condorLogRdr.monitorLogFile( _defaultNodeLog, !_recovery,
+				errstack ) ) {
+		errstack.pushf( "DAGMan::Job", DAGMAN_ERR_LOG_FILE,
+					"ERROR: Unable to monitor log file <%s>\n",
+					_defaultNodeLog );
+		debug_printf( DEBUG_QUIET, "%s\n", errstack.getFullText().c_str() );
+		EXCEPT( "Fatal log file monitoring error!" );
+		return false;
+	}
+
+	return true;
+}
+
+//---------------------------------------------------------------------------
+bool
+Dag::UnmonitorLogFile()
+{
+	debug_printf( DEBUG_DEBUG_2, "Unmonitoring log file <%s>\n",
+				_defaultNodeLog );
+
+	CondorError errstack;
+	bool result = _condorLogRdr.unmonitorLogFile( _defaultNodeLog,
+				errstack );
+	if ( !result ) {
+		errstack.pushf( "DAGMan::Job", DAGMAN_ERR_LOG_FILE,
+					"ERROR: Unable to unmonitor log file <%s>\n",
+					_defaultNodeLog );
+		debug_printf( DEBUG_QUIET, "%s\n", errstack.getFullText().c_str() );
+		EXCEPT( "Fatal log file monitoring error!" );
+	}
+
+	return result;
+}
+
 //-------------------------------------------------------------------------
 void
 Dag::SetReject( const MyString &location )
@@ -3271,20 +3112,6 @@ Dag::CheckAllJobs()
 	} else {
 		debug_printf( DEBUG_DEBUG_1, "All Condor job events okay\n");
 	}
-
-	result = _checkStorkEvents.CheckAllJobs(jobError);
-	if ( result == CheckEvents::EVENT_ERROR ) {
-		debug_printf( DEBUG_QUIET, "Error checking Stork job events: %s\n",
-				jobError.Value() );
-		ASSERT( false );
-	} else if ( result == CheckEvents::EVENT_BAD_EVENT ||
-				result == CheckEvents::EVENT_WARNING ) {
-		debug_printf( DEBUG_NORMAL, "Warning checking Stork job events: %s\n",
-				jobError.Value() );
-		check_warning_strictness( DAG_STRICT_3 );
-	} else {
-		debug_printf( DEBUG_DEBUG_1, "All Stork job events okay\n");
-	}
 }
 
 //-------------------------------------------------------------------------
@@ -3744,7 +3571,7 @@ Dag::RemoveDependency( Job *parent, Job *child, MyString &whynot )
 
 //---------------------------------------------------------------------------
 Job*
-Dag::LogEventNodeLookup( int logsource, const ULogEvent* event,
+Dag::LogEventNodeLookup( const ULogEvent* event,
 			bool &submitEventIsSane )
 {
 	ASSERT( event );
@@ -3760,7 +3587,7 @@ Dag::LogEventNodeLookup( int logsource, const ULogEvent* event,
 	if ( event->eventNumber != ULOG_SUBMIT &&
 				event->eventNumber != ULOG_PRESKIP ) {
 		
-	  node = FindNodeByEventID( logsource, condorID );
+	  node = FindNodeByEventID( condorID );
 	  if( node ) {
 	    return node;
 	  }
@@ -3804,7 +3631,7 @@ Dag::LogEventNodeLookup( int logsource, const ULogEvent* event,
 					ASSERT( isNoop == node->GetNoop() );
 					int id = GetIndexID( condorID );
 					HashTable<int, Job *> *ht =
-								GetEventIDHash( isNoop, logsource );
+								GetEventIDHash( isNoop );
 					if ( ht->lookup(id, tmpNode) != 0 ) {
 							// Node not found.
 						int insertResult = ht->insert( id, node );
@@ -3848,7 +3675,7 @@ Dag::LogEventNodeLookup( int logsource, const ULogEvent* event,
 				bool isNoop = JobIsNoop( condorID );
 				int id = GetIndexID( condorID );
 				HashTable<int, Job *> *ht =
-							GetEventIDHash( isNoop, logsource );
+							GetEventIDHash( isNoop );
 				if ( ht->lookup(id, tmpNode) != 0 ) {
 						// Node not found.
 					int insertResult = ht->insert( id, node );
@@ -3890,7 +3717,7 @@ Dag::LogEventNodeLookup( int logsource, const ULogEvent* event,
 // (additionally sets *result=false if DAG should be aborted)
 
 bool
-Dag::EventSanityCheck( int logsource, const ULogEvent* event,
+Dag::EventSanityCheck( const ULogEvent* event,
 			const Job* node, bool* result )
 {
 	ASSERT( event );
@@ -3899,11 +3726,7 @@ Dag::EventSanityCheck( int logsource, const ULogEvent* event,
 	MyString eventError;
 	CheckEvents::check_event_result_t checkResult = CheckEvents::EVENT_OKAY;
 
-	if ( logsource == CONDORLOG ) {
-		checkResult = _checkCondorEvents.CheckAnEvent( event, eventError );
-	} else if ( logsource == DAPLOG ) {
-		checkResult = _checkStorkEvents.CheckAnEvent( event, eventError );
-	}
+	checkResult = _checkCondorEvents.CheckAnEvent( event, eventError );
 
 	if( checkResult == CheckEvents::EVENT_OKAY ) {
 		debug_printf( DEBUG_DEBUG_1, "Event is okay\n" );
@@ -3991,52 +3814,24 @@ Dag::SanityCheckSubmitEvent( const CondorID condorID, const Job* node )
 
 //---------------------------------------------------------------------------
 HashTable<int, Job *> *
-Dag::GetEventIDHash(bool isNoop, int jobType)
+Dag::GetEventIDHash(bool isNoop)
 {
 	if ( isNoop ) {
 		return &_noopIDHash;
 	}
 
-	switch (jobType) {
-	case Job::TYPE_CONDOR:
-		return &_condorIDHash;
-		break;
-
-	case Job::TYPE_STORK:
-		return &_storkIDHash;
-		break;
-
-	default:
-		EXCEPT( "Illegal job type (%d)", jobType );
-		break;
-	}
-
-	return NULL;
+	return &_condorIDHash;
 }
 
 //---------------------------------------------------------------------------
 const HashTable<int, Job *> *
-Dag::GetEventIDHash(bool isNoop, int jobType) const
+Dag::GetEventIDHash(bool isNoop) const
 {
 	if ( isNoop ) {
 		return &_noopIDHash;
 	}
 
-	switch (jobType) {
-	case Job::TYPE_CONDOR:
-		return &_condorIDHash;
-		break;
-
-	case Job::TYPE_STORK:
-		return &_storkIDHash;
-		break;
-
-	default:
-		EXCEPT( "Illegal job type (%d)", jobType );
-		break;
-	}
-
-	return NULL;
+	return &_condorIDHash;
 }
 
 // NOTE: dag addnode/removenode/adddep/removedep methods don't
@@ -4082,8 +3877,7 @@ Dag::SubmitNodeJob( const Dagman &dm, Job *node, CondorID &condorID )
 	if ( node->GetCluster() != _defaultCondorId._cluster ) {
 		ASSERT( JobIsNoop( condorID ) == node->GetNoop() );
 		int id = GetIndexID( node->GetID() );
-		int removeResult = GetEventIDHash( node->GetNoop(),
-					node->JobType() )->remove( id );
+		int removeResult = GetEventIDHash( node->GetNoop() )->remove( id );
 		ASSERT( removeResult == 0 );
 	}
 	node->SetCondorID( _defaultCondorId );
@@ -4099,7 +3893,7 @@ Dag::SubmitNodeJob( const Dagman &dm, Job *node, CondorID &condorID )
 		// Do condor_submit_dag -no_submit if this is a nested DAG node
 		// and lazy submit file generation is enabled (this must be
 		// done before we try to monitor the log file).
-   	if ( node->JobType() == Job::TYPE_CONDOR && !node->GetNoop() &&
+   	if ( !node->GetNoop() &&
 				node->GetDagFile() != NULL && _generateSubdagSubmits ) {
 		bool isRetry = node->GetRetries() > 0;
 		priority_swapper ps( node->_hasNodePriority, node->_nodePriority, _submitDagDeepOpts->priority);
@@ -4115,82 +3909,32 @@ Dag::SubmitNodeJob( const Dagman &dm, Job *node, CondorID &condorID )
 		}
 	}
 
-	if ( !node->MonitorLogFile( _condorLogRdr, _storkLogRdr, _nfsLogIsError,
-			_recovery, _defaultNodeLog, _use_default_node_log ) ) {
-		debug_printf( DEBUG_QUIET, "ERROR: Failed to monitor log for node %s.\n",
-			node->GetJobName() );
-		return SUBMIT_RESULT_NO_SUBMIT;
-	}
+	debug_printf( DEBUG_NORMAL, "Submitting %s Node %s job(s)...\n",
+			  	node->JobTypeString(), node->GetJobName() );
 
-		// We now only check for missing log files for Stork jobs because
-		// of the default log file feature; that doesn't work for Stork
-		// jobs because we can't specify the log file on the command
-		// line.  wenger 2009-08-14
-	if ( !_allowLogError && node->JobType() == Job::TYPE_STORK &&
-				!node->CheckForLogFile( false ) ) {
-		debug_printf( DEBUG_NORMAL, "ERROR: No 'log =' value found in "
-					"submit file %s for node %s\n", node->GetCmdFile(),
-					node->GetJobName() );
-		node->TerminateFailure();
-		snprintf( node->error_text, JOB_ERROR_TEXT_MAXLEN,
-					"No 'log =' value found in submit file %s",
-					node->GetCmdFile() );
-	  	_numNodesFailed++;
-		_metrics->NodeFinished( node->GetDagFile() != NULL, false );
-		if ( _dagStatus == DAG_STATUS_OK ) {
-			_dagStatus = DAG_STATUS_NODE_FAILED;
-		}
-		result = SUBMIT_RESULT_NO_SUBMIT;
+	bool submit_success = false;
 
+ 	node->_submitTries++;
+	if ( node->GetNoop() ) {
+   		submit_success = fake_condor_submit( condorID, 0,
+					node->GetJobName(), node->GetDirectory(),
+					_defaultNodeLog );
 	} else {
-		debug_printf( DEBUG_NORMAL, "Submitting %s Node %s job(s)...\n",
-				  	node->JobTypeString(), node->GetJobName() );
-
-		bool submit_success = false;
-
-    	if( node->JobType() == Job::TYPE_CONDOR ) {
-	  		node->_submitTries++;
-			if ( node->GetNoop() ) {
-      			submit_success = fake_condor_submit( condorID, 0,
-							node->GetJobName(), node->GetDirectory(),
-							node->GetLogFile() ,
-							node->GetLogFileIsXml() );
-			} else {
-				const char *logFile = node->UsingDefaultLog() ?
-							node->GetLogFile() : NULL;
-					// Note: assigning the ParentListString() return value
-					// to a variable here, instead of just passing it directly
-					// to condor_submit(), fixes a memory leak(!).
-					// wenger 2008-12-18
-				MyString parents = ParentListString( node );
-      			submit_success = condor_submit( dm, node->GetCmdFile(), condorID,
-							node->GetJobName(), parents,
-							node->varsFromDag, node->GetRetries(),
-							node->GetDirectory(), logFile,
-							ProhibitMultiJobs(),
-							node->NumChildren() > 0 && dm._claim_hold_time > 0 );
-			}
-    	} else if( node->JobType() == Job::TYPE_STORK ) {
-	  		node->_submitTries++;
-			if ( node->GetNoop() ) {
-      			submit_success = fake_condor_submit( condorID, 0,
-							node->GetJobName(), node->GetDirectory(),
-							node->GetLogFile(),
-							node->GetLogFileIsXml() );
-
-			} else {
-      			submit_success = stork_submit( dm, node->GetCmdFile(),
-						condorID, node->GetJobName(), node->GetDirectory() );
-			}
-    	} else {
-	    	debug_printf( DEBUG_QUIET, "Illegal job type: %d\n",
-						node->JobType() );
-			ASSERT(false);
-		}
-
-		result = submit_success ? SUBMIT_RESULT_OK : SUBMIT_RESULT_FAILED;
+			// Note: assigning the ParentListString() return value
+			// to a variable here, instead of just passing it directly
+			// to condor_submit(), fixes a memory leak(!).
+			// wenger 2008-12-18
+		MyString parents = ParentListString( node );
+   		submit_success = condor_submit( dm, node->GetCmdFile(), condorID,
+					node->GetJobName(), parents,
+					node->varsFromDag, node->GetRetries(),
+					node->GetDirectory(), _defaultNodeLog,
+					ProhibitMultiJobs(),
+					node->NumChildren() > 0 && dm._claim_hold_time > 0 );
 	}
 
+	result = submit_success ? SUBMIT_RESULT_OK : SUBMIT_RESULT_FAILED;
+
 	return result;
 }
 
@@ -4225,8 +3969,7 @@ Dag::ProcessSuccessfulSubmit( Job *node, const CondorID &condorID )
 	node->SetCondorID( condorID );
 	ASSERT( JobIsNoop( node->GetID() ) == node->GetNoop() );
 	int id = GetIndexID( node->GetID() );
-	int insertResult = GetEventIDHash( node->GetNoop(), node->JobType() )->
-				insert( id, node );
+	int insertResult = GetEventIDHash( node->GetNoop() )->insert( id, node );
 	ASSERT( insertResult == 0 );
 
 	debug_printf( DEBUG_VERBOSE, "\tassigned %s ID (%d.%d.%d)\n",
@@ -4249,8 +3992,6 @@ Dag::ProcessFailedSubmit( Job *node, int max_submit_attempts )
 	_nextSubmitTime = time(NULL) + thisSubmitDelay;
 	_nextSubmitDelay *= 2;
 
-	(void)node->UnmonitorLogFile( _condorLogRdr, _storkLogRdr );
-
 	if ( node->_submitTries >= max_submit_attempts ) {
 			// We're out of submit attempts, treat this as a submit failure.
 
diff --git a/src/condor_dagman/dag.h b/src/condor_dagman/dag.h
index 7dc4d5c..82800c2 100644
--- a/src/condor_dagman/dag.h
+++ b/src/condor_dagman/dag.h
@@ -38,12 +38,6 @@
 #include "dagman_recursive_submit.h"
 #include "jobstate_log.h"
 
-// NOTE: must be kept in sync with Job::job_type_t
-enum Log_source{
-  CONDORLOG = Job::TYPE_CONDOR,
-  DAPLOG = Job::TYPE_STORK
-};
-
 // Which layer of splices do we want to lift?
 enum SpliceLayer {
 	SELF,
@@ -115,14 +109,12 @@ class Dag {
 		@param retryNodeFirst whether, when a node fails and has retries,
 			   to put the node at the head of the ready queue
 		@param condorRmExe executable to remove Condor jobs
-		@param storkRmExe executable to remove Stork jobs
 		@param DAGManJobId Condor ID of this DAGMan process
 		@param prohibitMultiJobs whether submit files queueing multiple
 			   job procs are prohibited
 		@param submitDepthFirst whether ready nodes should be submitted
 			   in depth-first (as opposed to breadth-first) order
-		@param The user log file to be used for nodes whose submit files do
-				not specify a log file.
+		@param defaultNodeLog The user log file to be used for node jobs.
 		@param isSplice is a boolean which lets the dag object know whether
 				of not it is a splicing dag, or the toplevel dag. We don't
 				wan't to allocate some regulated resources we won't need
@@ -139,7 +131,7 @@ class Dag {
 		 bool allowLogError,
 		 bool useDagDir, int maxIdleJobProcs, bool retrySubmitFirst,
 		 bool retryNodeFirst, const char *condorRmExe,
-		 const char *storkRmExe, const CondorID *DAGManJobId,
+		 const CondorID *DAGManJobId,
 		 bool prohibitMultiJobs, bool submitDepthFirst,
 		 const char *defaultNodeLog, bool generateSubdagSubmits,
 		 SubmitDagDeepOptions *submitDagDeepOpts,
@@ -188,24 +180,19 @@ class Dag {
     /** Blocks until the Condor Log file grows.
         @return true: log file grew, false: timeout or shrinkage
     */
-
-    
     bool DetectCondorLogGrowth();
-    bool DetectDaPLogGrowth();            //<--DAP
 
     /** Force the Dag to process all new events in the condor log file.
         This may cause the state of some jobs to change.
 
-		@param logsource The type of log from which events should be read.
         @param recover Process Log in Recover Mode, from beginning to end
         @return true on success, false on failure
     */
-    bool ProcessLogEvents (int logsource, bool recovery = false); //<--DAP
+    bool ProcessLogEvents (bool recovery = false);
 
 	/** Process a single event.  Note that this is called every time
 			we attempt to read the user log, so we may or may not have
 			a valid event here.
-		@param The type of log which is the source of the event.
 		@param The outcome from the attempt to read the user log.
 	    @param The event.
 		@param Whether we're in recovery mode.
@@ -213,7 +200,7 @@ class Dag {
 			function).
 		@return True if the DAG should continue, false if we should abort.
 	*/
-	bool ProcessOneEvent (int logsource, ULogEventOutcome outcome, const ULogEvent *event,
+	bool ProcessOneEvent (ULogEventOutcome outcome, const ULogEvent *event,
 			bool recovery, bool &done);
 
 	/** Process an abort or executable error event.
@@ -298,12 +285,11 @@ class Dag {
     */
     Job * FindNodeByName (const char * jobName) const;
 
-    /** Get pointer to job with condor or stork ID condorID
-		@param logsource The type of log from which events should be read.
+    /** Get pointer to job with condor ID condorID
         @param condorID the CondorID of the job in the DAG
         @return address of Job object, or NULL if not found
     */
-    Job * FindNodeByEventID (int logsource, const CondorID condorID ) const;
+    Job * FindNodeByEventID ( const CondorID condorID ) const;
 
     /** Ask whether a node name exists in the DAG
         @param nodeName the name of the node in the DAG
@@ -639,9 +625,6 @@ class Dag {
 	// do not free this pointer
 	const char* CondorRmExe(void) { return _condorRmExe; }
 
-	// do not free this pointer
-	const char* StorkRmExe(void) { return _storkRmExe; }
-
 	const CondorID* DAGManJobId(void) { return _DAGManJobId; }
 
 	bool SubmitDepthFirst(void) { return _submitDepthFirst; }
@@ -773,7 +756,6 @@ class Dag {
 	*/
 	inline bool Recovery() const { return _recovery; }
 
-	inline void UseDefaultNodeLog(bool useit) { _use_default_node_log = useit; }
   private:
 
 	// If this DAG is a splice, then this is what the DIR was set to, it 
@@ -830,7 +812,7 @@ class Dag {
 		SUBMIT_RESULT_NO_SUBMIT,
 	} submit_result_t;
 
-	/** Submit the Condor or Stork job for a node, including doing
+	/** Submit the Condor job for a node, including doing
 		some higher-level work such as sleeping before the actual submit
 		if necessary.
 		@param the appropriate Dagman object
@@ -841,15 +823,13 @@ class Dag {
 	submit_result_t SubmitNodeJob( const Dagman &dm, Job *node,
 				CondorID &condorID );
 
-	/** Do the post-processing of a successful submit of a Condor or
-		Stork job.
+	/** Do the post-processing of a successful submit of a Condor job.
 		@param the node for which the job was just submitted
 		@param the Condor ID of the associated job
 	*/	
 	void ProcessSuccessfulSubmit( Job *node, const CondorID &condorID );
 
-	/** Do the post-processing of a failed submit of a Condor or
-		Stork job.
+	/** Do the post-processing of a failed submit of a Condor job.
 		@param the node for which the job was just submitted
 		@param the maximum number of submit attempts allowed for a job.
 	*/
@@ -899,12 +879,12 @@ class Dag {
 	bool CheckForDagAbort(Job *job, const char *type);
 
 		// takes a userlog event and returns the corresponding node
-	Job* LogEventNodeLookup( int logsource, const ULogEvent* event,
+	Job* LogEventNodeLookup( const ULogEvent* event,
 				bool &submitEventIsSane );
 
 		// check whether a userlog event is sane, or "impossible"
 
-	bool EventSanityCheck( int logsource, const ULogEvent* event,
+	bool EventSanityCheck( const ULogEvent* event,
 						const Job* node, bool* result );
 
 		// compares a submit event's job ID with the one that appeared
@@ -913,24 +893,17 @@ class Dag {
 
 	bool SanityCheckSubmitEvent( const CondorID condorID, const Job* node );
 
-		/** Get the appropriate hash table for event ID->node mapping,
-			according to whether this is a Condor or Stork node.
+		/** Get the appropriate hash table for event ID->node mapping.
 			@param whether the node is a NOOP node
-			@param the node type/logsource (Condor or Stork) (see
-				Log_source and Job::job_type_t)
 			@return a pointer to the appropriate hash table
 		*/
-	HashTable<int, Job *> *		GetEventIDHash(bool isNoop, int jobType);
+	HashTable<int, Job *> *		GetEventIDHash(bool isNoop);
 
-		/** Get the appropriate hash table for event ID->node mapping,
-			according to whether this is a Condor or Stork node.
+		/** Get the appropriate hash table for event ID->node mapping.
 			@param whether the node is a NOOP node
-			@param the node type/logsource (Condor or Stork) (see
-				Log_source and Job::job_type_t)
 			@return a pointer to the appropriate hash table
 		*/
-	const HashTable<int, Job *> *		GetEventIDHash(bool isNoop,
-				int jobType) const;
+	const HashTable<int, Job *> *		GetEventIDHash(bool isNoop) const;
 
 	// run DAGs in directories from DAG file paths if true
 	bool _useDagDir;
@@ -938,20 +911,14 @@ class Dag {
     // Documentation on ReadUserLog is present in condor_utils
 	ReadMultipleUserLogs _condorLogRdr;
 
-		// Object to read events from Stork logs.
-	ReadMultipleUserLogs	_storkLogRdr;
-
 		/** Get the total number of node job user log files we'll be
 			accessing.
 			@return The total number of log files.
 		*/
-	int TotalLogFileCount() { return CondorLogFileCount() +
-				StorkLogFileCount(); }
-
+	int TotalLogFileCount() { return CondorLogFileCount(); }
+				
 	int CondorLogFileCount() { return _condorLogRdr.totalLogFileCount(); }
 
-	int StorkLogFileCount() { return _storkLogRdr.totalLogFileCount(); }
-
 		/** Write information for the given node to a rescue DAG.
 			@param fp: file pointer to the rescue DAG file
 			@param node: the node for which to write info
@@ -977,6 +944,16 @@ class Dag {
 	*/
 	const char *EscapeClassadString( const char* strIn );
 
+	/** Monitor the workflow log file for this DAG.
+		@return:  true if successful, false otherwise
+	*/
+	bool MonitorLogFile();
+
+	/** Unmonitor the workflow log file for this DAG.
+		@return:  true if successful, false otherwise
+	*/
+	bool UnmonitorLogFile();
+
 protected:
     /// List of Job objects
     List<Job>     _jobs;
@@ -994,10 +971,6 @@ private:
 	// procs in the same cluster map to the same node).
 	HashTable<int, Job *>			_condorIDHash;
 
-	// Hash by StorkID (really just by the cluster ID because all
-	// procs in the same cluster map to the same node).
-	HashTable<int, Job *>			_storkIDHash;
-
 	// NOOP nodes are indexed by subprocID.
 	HashTable<int, Job *>			_noopIDHash;
 
@@ -1044,9 +1017,6 @@ private:
 		// Executable to remove Condor jobs.
 	const char *	_condorRmExe;
 
-		// Executable to remove Stork jobs.
-	const char *	_storkRmExe;
-
 		// Condor ID of this DAGMan process.
 	const CondorID *	_DAGManJobId;
 
@@ -1100,10 +1070,7 @@ private:
 		// Last time the status file was written.
 	time_t _lastStatusUpdateTimestamp;
 
-		// Separate event checkers for Condor and Stork here because
-		// IDs could collide.
 	CheckEvents	_checkCondorEvents;
-	CheckEvents	_checkStorkEvents;
 
 		// Total count of jobs deferred because of MaxJobs limit (note
 		// that a single job getting deferred multiple times is counted
@@ -1162,12 +1129,7 @@ private:
 		// retry.
 	static const CondorID	_defaultCondorId;
 
-		// Whether having node job log files on NFS is an error (vs.
-		// just a warning).
-	bool	_nfsLogIsError;
-
-		// The user log file to be used for nodes whose submit files do
-		// not specify a log file.
+		// The user log file to be used for nodes jobs.
 	const char *_defaultNodeLog;
 
 		// Whether to generate the .condor.sub files for sub-DAGs
@@ -1230,12 +1192,6 @@ private:
 		// The name of the halt file (we halt the DAG if that file exists).
 	MyString _haltFile;
 	
-		// Whether to use the default node log as *the* log
-		// for writing and reading events.  If false, use the user log
-		// This must be false if dagman is communicating with a pre-7.9.0
-		// schedd/shadow or submit.
-	bool _use_default_node_log;
-
 		// Object to deal with reporting DAGMan metrics (to Pegasus).
 	DagmanMetrics *_metrics;
 };
diff --git a/src/condor_dagman/dagman_commands.cpp b/src/condor_dagman/dagman_commands.cpp
index 3e87782..d758aa4 100644
--- a/src/condor_dagman/dagman_commands.cpp
+++ b/src/condor_dagman/dagman_commands.cpp
@@ -60,7 +60,7 @@ ResumeDag(Dagman &dm)
 }
 
 bool
-AddNode( Dag *dag, Job::job_type_t type, const char *name,
+AddNode( Dag *dag, const char *name,
 		 const char* directory,
 		 const char* submitFile,
 		 const char *precmd, const char *postcmd, bool noop,
@@ -83,7 +83,7 @@ AddNode( Dag *dag, Job::job_type_t type, const char *name,
 		(void)check_warning_strictness( DAG_STRICT_1, false );
 		done = false;
 	}
-	Job* node = new Job( type, name, directory, submitFile );
+	Job* node = new Job( name, directory, submitFile );
 	if( !node ) {
 		dprintf( D_ALWAYS, "ERROR: out of memory!\n" );
 			// we already know we're out of memory, so filling in
diff --git a/src/condor_dagman/dagman_commands.h b/src/condor_dagman/dagman_commands.h
index bce5bd4..03077fa 100644
--- a/src/condor_dagman/dagman_commands.h
+++ b/src/condor_dagman/dagman_commands.h
@@ -31,7 +31,7 @@ bool PauseDag(Dagman &dm);
 // resume DAGMan's normal event-processing
 bool ResumeDag(Dagman &dm);
 
-bool AddNode( Dag *dag, Job::job_type_t type, const char *name,
+bool AddNode( Dag *dag, const char *name,
 			  const char* directory,
 			  const char* submitFile,
 			  const char *precmd, const char *postcmd, bool noop,
diff --git a/src/condor_dagman/dagman_main.cpp b/src/condor_dagman/dagman_main.cpp
index 846ee05..9328603 100644
--- a/src/condor_dagman/dagman_main.cpp
+++ b/src/condor_dagman/dagman_main.cpp
@@ -106,8 +106,6 @@ Dagman::Dagman() :
 	paused (false),
 	condorSubmitExe (NULL),
 	condorRmExe (NULL),
-	storkSubmitExe (NULL),
-	storkRmExe (NULL),
 	submit_delay (0),
 	max_submit_attempts (6),
 	max_submits_per_interval (5), // so Coverity is happy
@@ -240,9 +238,11 @@ Dagman::Config()
 	debug_printf( DEBUG_NORMAL, "DAGMAN_DEFAULT_PRIORITY setting: %d\n",
 				_defaultPriority );
 
-	_submitDagDeepOpts.always_use_node_log = param_boolean( "DAGMAN_ALWAYS_USE_NODE_LOG", true);
-	debug_printf( DEBUG_NORMAL, "DAGMAN_ALWAYS_USE_NODE_LOG setting: %s\n",
-				_submitDagDeepOpts.always_use_node_log ? "True" : "False" );
+	if ( !param_boolean( "DAGMAN_ALWAYS_USE_NODE_LOG", true ) ) {
+       	debug_printf( DEBUG_QUIET,
+					"Error: setting DAGMAN_ALWAYS_USE_NODE_LOG to false is no longer allowed\n" );
+		DC_Exit( EXIT_ERROR );
+	}
 
 	_submitDagDeepOpts.suppress_notification = param_boolean(
 		"DAGMAN_SUPPRESS_NOTIFICATION",
@@ -358,18 +358,16 @@ Dagman::Config()
 		ASSERT( condorRmExe );
 	}
 
-	free( storkSubmitExe );
-	storkSubmitExe = param( "DAGMAN_STORK_SUBMIT_EXE" );
-	if( !storkSubmitExe ) {
-		storkSubmitExe = strdup( "stork_submit" );
-		ASSERT( storkSubmitExe );
+	if ( param_boolean( "DAGMAN_STORK_SUBMIT_EXE", false ) ) {
+		debug_printf( DEBUG_NORMAL, "Warning: DAGMAN_STORK_SUBMIT_EXE is "
+					"no longer supported\n" );
+		check_warning_strictness( DAG_STRICT_1 );
 	}
 
-	free( storkRmExe );
-	storkRmExe = param( "DAGMAN_STORK_RM_EXE" );
-	if( !storkRmExe ) {
-		storkRmExe = strdup( "stork_rm" );
-		ASSERT( storkRmExe );
+	if ( param_boolean( "DAGMAN_STORK_RM_EXE", false ) ) {
+		debug_printf( DEBUG_NORMAL, "Warning: DAGMAN_STORK_RM_EXE is "
+					"no longer supported\n" );
+		check_warning_strictness( DAG_STRICT_1 );
 	}
 
 	abortDuplicates = param_boolean( "DAGMAN_ABORT_DUPLICATES",
@@ -820,7 +818,9 @@ void main_init (int argc, char ** const argv) {
 			dagman._submitDagDeepOpts.priority = atoi(argv[i]);
 
 		} else if( !strcasecmp( "-dont_use_default_node_log", argv[i] ) ) {
-			dagman._submitDagDeepOpts.always_use_node_log = false;
+       		debug_printf( DEBUG_QUIET,
+						"Error: -dont_use_default_node_log is no longer allowed\n" );
+			DC_Exit( EXIT_ERROR );
 
 		} else if ( !strcasecmp( "-dorecov", argv[i] ) ) {
 			dagman._doRecovery = true;
@@ -933,11 +933,6 @@ void main_init (int argc, char ** const argv) {
         Usage();
     }
 
-	if ( !dagman._submitDagDeepOpts.always_use_node_log ) {
-        debug_printf( DEBUG_QUIET, "Error: setting DAGMAN_ALWAYS_USE_NODE_LOG to false is no longer allowed\n" );
-		DC_Exit( EXIT_ERROR );
-	}
-
 	//
 	// ...done checking arguments.
 	//
@@ -1021,7 +1016,7 @@ void main_init (int argc, char ** const argv) {
 						  dagman.allowLogError, dagman.useDagDir,
 						  dagman.maxIdle, dagman.retrySubmitFirst,
 						  dagman.retryNodeFirst, dagman.condorRmExe,
-						  dagman.storkRmExe, &dagman.DAGManJobId,
+						  &dagman.DAGManJobId,
 						  dagman.prohibitMultiJobs, dagman.submitDepthFirst,
 						  dagman._defaultNodeLog.Value(),
 						  dagman._generateSubdagSubmits,
@@ -1233,13 +1228,7 @@ void main_init (int argc, char ** const argv) {
 		}
 
         if ( recovery ) {
-				// Not using the default node log is the backward
-				// compatible thing to do, so if using the default
-				// log file is already disabled, we don't have to
-				// do any checking.
-			if ( dagman._submitDagDeepOpts.always_use_node_log ) { 
-				dagman.CheckLogFileMode( submitFileVersion );
-			}
+			dagman.CheckLogFileMode( submitFileVersion );
 		}
 
 			//
@@ -1287,8 +1276,8 @@ Dagman::CheckLogFileMode( const CondorVersionInfo &submitFileVersion )
 				// Pre-7.9.0 -- default log wasn't implemented yet, so
 				// we need to use individual logs from submit files.
 			debug_printf( DEBUG_QUIET, "Submit file version indicates submit is too old. "
-				"Falling back to 7.8 behavior of not using the default node log\n");
-			DisableDefaultLog();
+				"DAGMan no longer supports individual per-job log files.\n" );
+			DC_Exit( EXIT_ERROR );
 		}
 
 	} else {
@@ -1309,27 +1298,14 @@ Dagman::CheckLogFileMode( const CondorVersionInfo &submitFileVersion )
 				// We are in recovery, but the default log does not exist.
 				// Fall back to 7.8 behavior
 			debug_printf( DEBUG_QUIET, "Default node log does not exist. "
-						"Falling back to 7.8 behavior of not using the default node log\n");
-			DisableDefaultLog();
+				"DAGMan no longer supports individual per-job log files.\n" );
+			DC_Exit( EXIT_ERROR );
 		}
 	}
 }
 
 //---------------------------------------------------------------------------
 void
-Dagman::DisableDefaultLog()
-{
-	dagman._submitDagDeepOpts.always_use_node_log = false;
-		// Note:  we have to explicitly turn off the default
-		// log file here because
-		// _submitDagDeepOpts.always_use_node_log is
-		// referenced in the Dag constructor, so just
-		// changing that here won't do us any good.
-	dagman.dag->UseDefaultNodeLog(false);
-}
-
-//---------------------------------------------------------------------------
-void
 Dagman::ResolveDefaultLog()
 {
 	char *dagDir = condor_dirname( primaryDagFile.Value() );
@@ -1352,8 +1328,7 @@ Dagman::ResolveDefaultLog()
 					"default node log file %s contains an '@' character -- "
 					"unresolved macro substituion?\n",
 					_defaultNodeLog.Value() );
-		check_warning_strictness( _submitDagDeepOpts.always_use_node_log ?
-					DAG_STRICT_1 : DAG_STRICT_2 );
+		check_warning_strictness( DAG_STRICT_1 );
 	}
 
 		// Force default log file path to be absolute so it works
@@ -1371,8 +1346,29 @@ Dagman::ResolveDefaultLog()
 		debug_printf( DEBUG_QUIET, "Warning: "
 					"default node log file %s is in /tmp\n",
 					_defaultNodeLog.Value() );
-		check_warning_strictness( _submitDagDeepOpts.always_use_node_log ?
-					DAG_STRICT_1 : DAG_STRICT_2 );
+		check_warning_strictness( DAG_STRICT_1 );
+	}
+
+	bool nfsLogIsError = param_boolean( "DAGMAN_LOG_ON_NFS_IS_ERROR", true );
+	if ( nfsLogIsError ) {
+		bool userlog_locking = param_boolean( "ENABLE_USERLOG_LOCKING", true );
+		if ( userlog_locking ) {
+			bool locks_on_local = param_boolean( "CREATE_LOCKS_ON_LOCAL_DISK", true);
+			if ( locks_on_local ) {
+				debug_printf( DEBUG_QUIET, "Ignoring value of DAGMAN_LOG_ON_NFS_IS_ERROR because ENABLE_USERLOG_LOCKING and CREATE_LOCKS_ON_LOCAL_DISK are true.\n");
+				nfsLogIsError = false;
+			}
+		}
+	}
+
+		// This function returns true if the log file is on NFS and
+		// that is an error.  If the log file is on NFS, but nfsIsError
+		// is false, it prints a warning but returns false.
+	if ( MultiLogFiles::logFileNFSError( _defaultNodeLog.Value(),
+				nfsLogIsError ) ) {
+		debug_printf( DEBUG_QUIET, "Error: log file %s on NFS\n",
+					_defaultNodeLog.Value() );
+		DC_Exit( EXIT_ERROR );
 	}
 
 	debug_printf( DEBUG_NORMAL, "Default node log file is: <%s>\n",
@@ -1452,19 +1448,9 @@ void condor_event_timer () {
 
 	// If the log has grown
 	if( dagman.dag->DetectCondorLogGrowth() ) {
-		if( dagman.dag->ProcessLogEvents( CONDORLOG ) == false ) {
-			debug_printf( DEBUG_NORMAL,
-						"ProcessLogEvents(CONDORLOG) returned false\n" );
-			dagman.dag->PrintReadyQ( DEBUG_DEBUG_1 );
-			main_shutdown_rescue( EXIT_ERROR, Dag::DAG_STATUS_ERROR );
-			return;
-		}
-	}
-
-	if( dagman.dag->DetectDaPLogGrowth() ) {
-		if( dagman.dag->ProcessLogEvents( DAPLOG ) == false ) {
+		if( dagman.dag->ProcessLogEvents() == false ) {
 			debug_printf( DEBUG_NORMAL,
-						"ProcessLogEvents(DAPLOG) returned false\n" );
+						"ProcessLogEvents() returned false\n" );
 			dagman.dag->PrintReadyQ( DEBUG_DEBUG_1 );
 			main_shutdown_rescue( EXIT_ERROR, Dag::DAG_STATUS_ERROR );
 			return;
diff --git a/src/condor_dagman/dagman_main.h b/src/condor_dagman/dagman_main.h
index 0ce4732..99b1e61 100644
--- a/src/condor_dagman/dagman_main.h
+++ b/src/condor_dagman/dagman_main.h
@@ -55,10 +55,6 @@ class Dagman {
 		// whether we should fall back to non-default log mode.
 	void CheckLogFileMode( const CondorVersionInfo &submitFileVersion );
 
-		// Disable use of the default node log (use the log files from
-		// the submit files instead).
-	void DisableDefaultLog();
-
 		// Resolve macro substitutions in _defaultNodeLog.  Also check
 		// for some errors/warnings.
 	void ResolveDefaultLog();
@@ -72,8 +68,6 @@ class Dagman {
 
 	char* condorSubmitExe;
 	char* condorRmExe;
-	char* storkSubmitExe;
-	char* storkRmExe;
 
 	// number of seconds to wait before consecutive calls to
 	// condor_submit (or dap_submit, etc.)
diff --git a/src/condor_dagman/dagman_recursive_submit.cpp b/src/condor_dagman/dagman_recursive_submit.cpp
index b01b46f..a3f210a 100644
--- a/src/condor_dagman/dagman_recursive_submit.cpp
+++ b/src/condor_dagman/dagman_recursive_submit.cpp
@@ -127,10 +127,6 @@ runSubmitDag( const SubmitDagDeepOptions &deepOpts,
 		args.AppendArg( deepOpts.priority );
 	}
 
-	if( !deepOpts.always_use_node_log ) {
-		args.AppendArg( "-dont_use_default_node_log" );
-	}
-
 	if( deepOpts.suppress_notification ) {
 		args.AppendArg( "-suppress_notification" );
 	} else {
diff --git a/src/condor_dagman/dagman_recursive_submit.h b/src/condor_dagman/dagman_recursive_submit.h
index 1a3fa66..0395bf3 100644
--- a/src/condor_dagman/dagman_recursive_submit.h
+++ b/src/condor_dagman/dagman_recursive_submit.h
@@ -57,8 +57,8 @@ struct SubmitDagShallowOptions
 	// non-command line options
 	MyString strLibOut;
 	MyString strLibErr;
-	MyString strDebugLog;
-	MyString strSchedLog;
+	MyString strDebugLog; // the dagman.out file
+	MyString strSchedLog; // the user log of condor_dagman's events
 	MyString strSubFile;
 	MyString strRescueFile;
 	MyString strLockFile;
@@ -112,11 +112,6 @@ struct SubmitDagDeepOptions
 	bool importEnv; // explicitly import environment into .condor.sub file
 	int priority; // Priority of parent of DAG node
 
-		// Use the default node log (<DAGfile>.nodes.log) for events
-		// Defaults to true
-		// Set to false if this dagman is going to be communicating
-		// with pre-7.9.0 schedd/shadow/submit
-	bool always_use_node_log;		 	
 	bool suppress_notification;
 
 	SubmitDagDeepOptions() 
@@ -133,7 +128,6 @@ struct SubmitDagDeepOptions
 		updateSubmit = false;
 		importEnv = false;
 		priority = 0;
-		always_use_node_log = true;
 		suppress_notification = true;
 	}
 };
diff --git a/src/condor_dagman/dagman_submit.cpp b/src/condor_dagman/dagman_submit.cpp
index 860b4f7..1669383 100644
--- a/src/condor_dagman/dagman_submit.cpp
+++ b/src/condor_dagman/dagman_submit.cpp
@@ -65,34 +65,8 @@ parse_condor_submit( const char *buffer, int &jobProcCount, int &cluster )
   return true;
 }
 
-//-------------------------------------------------------------------------
-/** Parse output from stork_submit, determine the number of job procs
-    and the cluster.
-	@param buffer containing the line to be parsed
-	@param integer to be filled in with the number of job procs generated
-	       by the stork_submit
-	@param integer to be filled in with the cluster ID
-	@return true iff the line was correctly parsed
-*/
 static bool
-parse_stork_submit( const char *buffer, int &jobProcCount, int &cluster )
-{
-  // The initial space is to make the sscanf match zero or more leading
-  // whitespace that may exist in the buffer.
-  if ( 1 != sscanf( buffer, " Request assigned id: %d", &cluster) ) {
-	debug_printf( DEBUG_QUIET, "ERROR: parse_stork_submit failed:\n\t%s\n",
-				buffer );
-    return false;
-  }
-
-  jobProcCount = 1;
-  return true;
-}
-
-//-------------------------------------------------------------------------
-static bool
-submit_try( ArgList &args, CondorID &condorID, Job::job_type_t type,
-			bool prohibitMultiJobs )
+submit_try( ArgList &args, CondorID &condorID, bool prohibitMultiJobs )
 {
   MyString cmd; // for debug output
   args.GetArgsStringForDisplay( &cmd );
@@ -106,8 +80,8 @@ submit_try( ArgList &args, CondorID &condorID, Job::job_type_t type,
   }
   
   //----------------------------------------------------------------------
-  // Parse submit command output for a Condor/Stork job ID.  This
-  // desperately needs to be replaced by Condor/Stork submit APIs.
+  // Parse submit command output for a Condor job ID.  This
+  // desperately needs to be replaced by Condor submit APIs.
   //
   // Typical condor_submit output for Condor v6 looks like:
   //
@@ -124,30 +98,22 @@ submit_try( ArgList &args, CondorID &condorID, Job::job_type_t type,
   const char *marker = NULL;
   parse_submit_fnc parseFnc = NULL;
 
-  if ( type == Job::TYPE_CONDOR ) {
-    marker = " submitted to cluster ";
-
-	  // Note: we *could* check how many jobs got submitted here, and
-	  // correlate that with how many submit events we see later on.
-	  // I'm not worrying about that for now...  wenger 2006-02-07.
-	  // We also have to check the number of jobs to get an accurate
-	  // count of submitted jobs to report in the dagman.out file.
-
-	  // We should also check whether we got more than one cluster, and
-	  // either deal with it correctly or generate an error message.
-	parseFnc = parse_condor_submit;
-  } else if ( type == Job::TYPE_STORK ) {
-    marker = "assigned id";
-	parseFnc = parse_stork_submit;
-  } else {
-	debug_printf( DEBUG_QUIET, "Illegal job type: %d\n", type );
-	ASSERT(false);
-  }
+  marker = " submitted to cluster ";
+
+  // Note: we *could* check how many jobs got submitted here, and
+  // correlate that with how many submit events we see later on.
+  // I'm not worrying about that for now...  wenger 2006-02-07.
+  // We also have to check the number of jobs to get an accurate
+  // count of submitted jobs to report in the dagman.out file.
+
+  // We should also check whether we got more than one cluster, and
+  // either deal with it correctly or generate an error message.
+  parseFnc = parse_condor_submit;
   
-  // Take all of the output (both stdout and stderr) from condor_submit
-  // or stork_submit, and echo it to the dagman.out file.  Look for
-  // the line (if any) containing the word "cluster" (Condor) or
-  // "assigned id" (Stork).  If we don't find such a line, something
+  // Take all of the output (both stdout and stderr) from condor_submit,
+  // and echo it to the dagman.out file.  Look for
+  // the line (if any) containing the word "cluster" (Condor).
+  // If we don't find such a line, something
   // went wrong with the submit, so we return false.  The caller of this
   // function can retry the submit by repeatedly calling this function.
 
@@ -201,13 +167,6 @@ submit_try( ArgList &args, CondorID &condorID, Job::job_type_t type,
   	  return true;
   }
 
-  // Stork job specs have only 1 dimension.  The Stork user log forces the proc
-  // and sub-proc ids to "-1", so do the same here for the returned submit id.
-  if ( type == Job::TYPE_STORK ) {
-	  condorID._proc = -1;
-	  condorID._subproc = -1;
-  }
-
   	// Check for multiple job procs if configured to disallow that.
   if ( prohibitMultiJobs && (jobProcCount > 1) ) {
 	debug_printf( DEBUG_NORMAL, "Submit generated %d job procs; "
@@ -223,8 +182,7 @@ submit_try( ArgList &args, CondorID &condorID, Job::job_type_t type,
 // NOTE: this and submit_try should probably be merged into a single
 // method -- submit_batch_job or something like that.  wenger/pfc 2006-04-05.
 static bool
-do_submit( ArgList &args, CondorID &condorID, Job::job_type_t jobType,
-			bool prohibitMultiJobs )
+do_submit( ArgList &args, CondorID &condorID, bool prohibitMultiJobs )
 {
 	MyString cmd; // for debug output
 	args.GetArgsStringForDisplay( &cmd );
@@ -232,7 +190,7 @@ do_submit( ArgList &args, CondorID &condorID, Job::job_type_t jobType,
   
 	bool success = false;
 
-	success = submit_try( args, condorID, jobType, prohibitMultiJobs );
+	success = submit_try( args, condorID, prohibitMultiJobs );
 
 	if( !success ) {
 	    debug_printf( DEBUG_QUIET, "ERROR: submit attempt failed\n" );
@@ -324,31 +282,29 @@ condor_submit( const Dagman &dm, const char* cmdFile, CondorID& condorID,
 				"submit_event_notes = DAG Node: " ) + DAGNodeName;
 	args.AppendArg( submitEventNotes.Value() );
 
-		// workflowLogFile is non-null here if we need to tell the schedd to
-		// use that file as the default/workflow log for this node.
-	if ( workflowLogFile ) {
-			// We need to append the DAGman default log file to
-			// the log file list
-		args.AppendArg( "-a" );
-		std::string dlog( "dagman_log = " );
-		dlog += workflowLogFile;
-		args.AppendArg( dlog.c_str() );
-		debug_printf( DEBUG_VERBOSE, "Adding a DAGMan workflow log %s\n",
-					workflowLogFile );
-
-			// Now append the mask
-		debug_printf( DEBUG_VERBOSE, "Masking the events recorded in the DAGMAN workflow log\n" );
-		args.AppendArg( "-a" );
-		std::string dmask("+");
-		dmask += ATTR_DAGMAN_WORKFLOW_MASK;
-		dmask += " = \"";
-		const char *eventMask = getEventMask();
-		debug_printf( DEBUG_VERBOSE, "Mask for workflow log is %s\n",
-					eventMask );
-		dmask += eventMask;
-		dmask += "\"";
-		args.AppendArg( dmask.c_str() );
-	}
+	ASSERT( workflowLogFile );
+
+		// We need to append the DAGman default log file to
+		// the log file list
+	args.AppendArg( "-a" );
+	std::string dlog( "dagman_log = " );
+	dlog += workflowLogFile;
+	args.AppendArg( dlog.c_str() );
+	debug_printf( DEBUG_VERBOSE, "Adding a DAGMan workflow log %s\n",
+				workflowLogFile );
+
+		// Now append the mask
+	debug_printf( DEBUG_VERBOSE, "Masking the events recorded in the DAGMAN workflow log\n" );
+	args.AppendArg( "-a" );
+	std::string dmask("+");
+	dmask += ATTR_DAGMAN_WORKFLOW_MASK;
+	dmask += " = \"";
+	const char *eventMask = getEventMask();
+	debug_printf( DEBUG_VERBOSE, "Mask for workflow log is %s\n",
+				eventMask );
+	dmask += eventMask;
+	dmask += "\"";
+	args.AppendArg( dmask.c_str() );
 
 		// Suppress the job's log file if that option is enabled.
 	if ( workflowLogFile && dm._suppressJobLogs ) {
@@ -435,8 +391,7 @@ condor_submit( const Dagman &dm, const char* cmdFile, CondorID& condorID,
 
 	args.AppendArg( cmdFile );
 
-	bool success = do_submit( args, condorID, Job::TYPE_CONDOR,
-				dm.prohibitMultiJobs );
+	bool success = do_submit( args, condorID, dm.prohibitMultiJobs );
 
 	if ( !tmpDir.Cd2MainDir( errMsg ) ) {
 		debug_printf( DEBUG_QUIET,
@@ -448,44 +403,6 @@ condor_submit( const Dagman &dm, const char* cmdFile, CondorID& condorID,
 	return success;
 }
 
-//-------------------------------------------------------------------------
-bool
-stork_submit( const Dagman &dm, const char* cmdFile, CondorID& condorID,
-			   const char* DAGNodeName, const char* directory )
-{
-	TmpDir		tmpDir;
-	MyString	errMsg;
-	if ( !tmpDir.Cd2TmpDir( directory, errMsg ) ) {
-		debug_printf( DEBUG_QUIET,
-				"Could not change to DAG directory %s: %s\n",
-				directory, errMsg.Value() );
-		return false;
-	}
-
-  ArgList args;
-
-  args.AppendArg( dm.storkSubmitExe );
-  args.AppendArg( "-lognotes" );
-
-  MyString logNotes = MyString( "DAG Node: " ) + DAGNodeName;
-  args.AppendArg( logNotes.Value() );
-
-  args.AppendArg( cmdFile );
-
-  bool success = do_submit( args, condorID, Job::TYPE_STORK,
-  			dm.prohibitMultiJobs );
-
-	if ( !tmpDir.Cd2MainDir( errMsg ) ) {
-		debug_printf( DEBUG_QUIET,
-				"Could not change to original directory: %s\n",
-				errMsg.Value() );
-		success = false;
-	}
-
-  return success;
-}
-
-//-------------------------------------------------------------------------
 // Subproc ID for "fake" events (for NOOP jobs).
 static int _subprocID = 0;
 
@@ -504,7 +421,7 @@ get_fake_condorID()
 //-------------------------------------------------------------------------
 bool
 fake_condor_submit( CondorID& condorID, Job* job, const char* DAGNodeName, 
-			   const char* directory, const char *logFile, bool logIsXml )
+			   const char* directory, const char *logFile )
 {
 	TmpDir		tmpDir;
 	MyString	errMsg;
@@ -529,8 +446,8 @@ fake_condor_submit( CondorID& condorID, Job* job, const char* DAGNodeName,
 
 	WriteUserLog ulog;
 	ulog.setEnableGlobalLog( false );
-	ulog.setUseXML( logIsXml );
-	ulog.initialize( std::vector<const char*>(1,logFile), condorID._cluster,
+	ulog.setUseXML( false );
+	ulog.initialize( logFile, condorID._cluster,
 		condorID._proc, condorID._subproc, NULL );
 
 	SubmitEvent subEvent;
@@ -570,7 +487,7 @@ fake_condor_submit( CondorID& condorID, Job* job, const char* DAGNodeName,
 }
 
 bool writePreSkipEvent( CondorID& condorID, Job* job, const char* DAGNodeName, 
-			   const char* directory, const char *logFile, bool logIsXml )
+			   const char* directory, const char *logFile )
 {
 	TmpDir tmpDir;
 	MyString	errMsg;
@@ -596,7 +513,7 @@ bool writePreSkipEvent( CondorID& condorID, Job* job, const char* DAGNodeName,
 
 	WriteUserLog ulog;
 	ulog.setEnableGlobalLog( false );
-	ulog.setUseXML( logIsXml );
+	ulog.setUseXML( false );
 	ulog.initialize( std::vector<const char*>(1,logFile), condorID._cluster,
 		condorID._proc, condorID._subproc, NULL );
 
diff --git a/src/condor_dagman/job.cpp b/src/condor_dagman/job.cpp
index d2d8dec..af6f768 100644
--- a/src/condor_dagman/job.cpp
+++ b/src/condor_dagman/job.cpp
@@ -58,14 +58,6 @@ const char * Job::status_t_names[] = {
 };
 
 //---------------------------------------------------------------------------
-// NOTE: must be kept in sync with the job_type_t enum
-const char* Job::_job_type_names[] = {
-    "Condor",
-    "Stork",
-    "No-Op",
-};
-
-//---------------------------------------------------------------------------
 Job::~Job() {
 	delete [] _directory;
 	delete [] _cmdFile;
@@ -76,7 +68,6 @@ Job::~Job() {
     // as of 6/2004 we don't yet use that.  For details, see:
     // http://support.microsoft.com/support/kb/articles/Q131/3/22.asp
 	delete [] _jobName;
-	delete [] _logFile;
 
 	varsFromDag->Rewind();
 	NodeVar *var;
@@ -92,9 +83,9 @@ Job::~Job() {
 }
 
 //---------------------------------------------------------------------------
-Job::Job( const job_type_t jobType, const char* jobName,
+Job::Job( const char* jobName,
 			const char *directory, const char* cmdFile ) :
-	_jobType( jobType ), _preskip( PRE_SKIP_INVALID ), _final( false )
+	_preskip( PRE_SKIP_INVALID ), _final( false )
 {
 	ASSERT( jobName != NULL );
 	ASSERT( cmdFile != NULL );
@@ -113,8 +104,6 @@ Job::Job( const job_type_t jobType, const char* jobName,
 	_cmdFile = strnewp (cmdFile);
 	_dagFile = NULL;
 	_throttleInfo = NULL;
-	_logIsMonitored = false;
-	_useDefaultLog = false;
 
     // _condorID struct initializes itself
 
@@ -139,9 +128,6 @@ Job::Job( const job_type_t jobType, const char* jobName,
 	_hasNodePriority = false;
 	_nodePriority = 0;
 
-	_logFile = NULL;
-	_logFileIsXml = false;
-
 	_noop = false;
 
 	_jobTag = NULL;
@@ -195,16 +181,6 @@ bool Job::Remove (const queue_t queue, const JobID_t jobID)
 }  
 
 //---------------------------------------------------------------------------
-bool
-Job::CheckForLogFile( bool usingDefault ) const
-{
-	bool tmpLogFileIsXml;
-	MyString logFile = MultiLogFiles::loadLogFileNameFromSubFile( _cmdFile,
-				_directory, tmpLogFileIsXml, usingDefault );
-	return (logFile != "");
-}
-
-//---------------------------------------------------------------------------
 void Job::Dump ( const Dag *dag ) const {
     dprintf( D_ALWAYS, "---------------------- Job ----------------------\n");
     dprintf( D_ALWAYS, "      Node Name: %s\n", _jobName );
@@ -246,6 +222,7 @@ void Job::Dump ( const Dag *dag ) const {
     }
 }
 
+#if 0 // not used -- wenger 2015-02-17
 //---------------------------------------------------------------------------
 void Job::Print (bool condorID) const {
     dprintf( D_ALWAYS, "ID: %4d Name: %s", _jobID, _jobName);
@@ -263,6 +240,7 @@ void job_print (Job * job, bool condorID) {
 		job->Print(condorID);
 	}
 }
+#endif
 
 const char*
 Job::GetPreScriptName() const
@@ -670,29 +648,6 @@ Job::RemoveDependency( queue_t queue, JobID_t job, MyString &whynot )
 	return true;
 }
 
-
-Job::job_type_t
-Job::JobType() const
-{
-    return _jobType;
-}
-
-
-const char*
-Job::JobTypeString() const
-{
-    return _job_type_names[_jobType];
-}
-
-
-/*
-const char* Job::JobIdString() const
-{
-
-}
-*/
-
-
 int
 Job::NumParents() const
 {
@@ -741,9 +696,7 @@ Job::SetCategory( const char *categoryName, ThrottleByCategory &catThrottles )
 void
 Job::PrefixName(const MyString &prefix)
 {
-	MyString tmp = _jobName;
-
-	tmp = prefix + tmp;
+	MyString tmp = prefix + _jobName;
 
 	delete[] _jobName;
 
@@ -776,138 +729,6 @@ Job::SetDagFile(const char *dagFile)
 }
 
 //---------------------------------------------------------------------------
-bool
-Job::MonitorLogFile( ReadMultipleUserLogs &condorLogReader,
-			ReadMultipleUserLogs &storkLogReader, bool nfsIsError,
-			bool recovery, const char *defaultNodeLog, bool usingDefault )
-{
-	debug_printf( DEBUG_DEBUG_2,
-				"Attempting to monitor log file for node %s; using default?: %d\n",
-				GetJobName(), usingDefault );
-
-	if ( _logIsMonitored ) {
-		debug_printf( DEBUG_DEBUG_1, "Warning: log file for node "
-					"%s is already monitored\n", GetJobName() );
-		return true;
-	}
-
-	ReadMultipleUserLogs &logReader = (_jobType == TYPE_CONDOR) ?
-				condorLogReader : storkLogReader;
-
-	MyString logFile;
-	if ( !FindLogFile( usingDefault, logFile ) ) {
-		LogMonitorFailed();
-		return false;
-	}
-	// Note:  logFile is "" here if usingDefault is true and this node
-	// is an HTCondor node (not Stork).
-
-		// Warn the user if the node's log file is in /tmp.
-	if ( logFile.find( "/tmp" ) == 0 ) {
-		debug_printf( DEBUG_QUIET, "Warning: "
-					"Log file %s for node %s is in /tmp\n",
-					logFile.Value(), GetJobName() );
-			// If we're using the workflow log, we'll only ever get here
-			// for Stork nodes, because they can't use the workflow log.
-        check_warning_strictness( DAG_STRICT_1 );
-	}
-
-	if ( logFile == "" ) {
-			// Using the workflow/default log file for this node.
-		logFile = defaultNodeLog;
-		_useDefaultLog = true;
-			// Default User log is never XML
-		_logFileIsXml = false;
-	}
-
-		// This function returns true if the log file is on NFS and
-		// that is an error.  If the log file is on NFS, but nfsIsError
-		// is false, it prints a warning but returns false.
-	if ( MultiLogFiles::logFileNFSError( logFile.Value(),
-				nfsIsError ) ) {
-		debug_printf( DEBUG_QUIET, "Error: log file %s on NFS\n",
-					logFile.Value() );
-		LogMonitorFailed();
-		return false;
-	}
-
-	delete [] _logFile;
-		// Saving log file here in case submit file gets changed.
-	_logFile = strnewp( logFile.Value() );
-	debug_printf( DEBUG_DEBUG_2, "Monitoring log file <%s> for node %s\n",
-				GetLogFile(), GetJobName() );
-	CondorError errstack;
-	if ( !logReader.monitorLogFile( GetLogFile(), !recovery, errstack ) ) {
-		errstack.pushf( "DAGMan::Job", DAGMAN_ERR_LOG_FILE,
-					"ERROR: Unable to monitor log file for node %s",
-					GetJobName() );
-		debug_printf( DEBUG_QUIET, "%s\n", errstack.getFullText().c_str() );
-		LogMonitorFailed();
-		EXCEPT( "Fatal log file monitoring error!" );
-		return false;
-	}
-
-	_logIsMonitored = true;
-
-	return true;
-}
-
-//---------------------------------------------------------------------------
-bool
-Job::UnmonitorLogFile( ReadMultipleUserLogs &condorLogReader,
-			ReadMultipleUserLogs &storkLogReader )
-{
-	debug_printf( DEBUG_DEBUG_2, "Unmonitoring log file <%s> for node %s\n",
-				GetLogFile(), GetJobName() );
-
-	if ( !_logIsMonitored ) {
-		debug_printf( DEBUG_DEBUG_1, "Warning: log file for node "
-					"%s is already unmonitored\n", GetJobName() );
-		return true;
-	}
-
-	ReadMultipleUserLogs &logReader = (_jobType == TYPE_CONDOR) ?
-				condorLogReader : storkLogReader;
-
-	debug_printf( DEBUG_DEBUG_1, "Unmonitoring log file <%s> for node %s\n",
-				GetLogFile(), GetJobName() );
-
-	CondorError errstack;
-	bool result = logReader.unmonitorLogFile( GetLogFile(), errstack );
-	if ( !result ) {
-		errstack.pushf( "DAGMan::Job", DAGMAN_ERR_LOG_FILE,
-					"ERROR: Unable to unmonitor log " "file for node %s",
-					GetJobName() );
-		debug_printf( DEBUG_QUIET, "%s\n", errstack.getFullText().c_str() );
-		EXCEPT( "Fatal log file monitoring error!" );
-	}
-
-	if ( result ) {
-		delete [] _logFile;
-		_logFile = NULL;
-		_logIsMonitored = false;
-	}
-
-	return result;
-}
-
-//---------------------------------------------------------------------------
-void
-Job::LogMonitorFailed()
-{
-	if ( _Status != Job::STATUS_ERROR ) {
-		SetStatus( Job::STATUS_ERROR );
-		snprintf( error_text, JOB_ERROR_TEXT_MAXLEN,
-					"Unable to monitor node job log file" );
-		retval = Dag::DAG_ERROR_LOG_MONITOR_ERROR;
-		if ( _scriptPost != NULL) {
-				// let the script know the job's exit status
-			_scriptPost->_retValJob = retval;
-		}
-	}
-}
-
-//---------------------------------------------------------------------------
 const char *
 Job::GetJobstateJobTag()
 {
@@ -1113,55 +934,3 @@ Job::Cleanup()
 	std::vector<unsigned char> s2;
 	_gotEvents.swap(s2); // Free memory in _gotEvents
 }
-
-//---------------------------------------------------------------------------
-bool
-Job::FindLogFile( bool usingWorkflowLog, MyString &logFile )
-{
-	if ( _jobType == TYPE_CONDOR ) {
-		if ( usingWorkflowLog ) {
-				// Now, if we're using the workflow log file, we don't
-				// even look at the node's submit file.  (See gittrac
-				// #3843.)
-			logFile = "";
-
-		} else {
-				// We're not in workflow/default log mode, so get the
-				// log file (if any) from the submit file.
-    		logFile = MultiLogFiles::loadLogFileNameFromSubFile(
-						_cmdFile, _directory, _logFileIsXml, false );
-			if ( logFile == "" ) {
-				debug_printf( DEBUG_NORMAL, "Unable to get log file from "
-							"submit file %s (node %s); using default/workflow log\n",
-							_cmdFile, GetJobName() );
-				// Don't return false here, because not specifying the
-				// log file is not an error.
-			}
-		}
-
-	} else {
-			// Workflow/default log file mode is not supported for Stork
-			// nodes, so we always have to get the log file for a Stork
-			// node.
-		StringList logFiles;
-		MyString tmpResult = MultiLogFiles::loadLogFileNamesFromStorkSubFile(
-					_cmdFile, _directory, logFiles );
-		if ( tmpResult != "" ) {
-			debug_printf( DEBUG_QUIET, "Error getting Stork log file: %s\n",
-						tmpResult.Value() );
-			return false;
-
-		} else if ( logFiles.number() != 1 ) {
-			debug_printf( DEBUG_QUIET, "Error: %d Stork log files found "
-						"in submit file %s; we want 1\n",
-						logFiles.number(), _cmdFile );
-			return false;
-
-		} else {
-			logFiles.rewind();
-			logFile = logFiles.next();
-		}
-	}
-
-	return true;
-}
diff --git a/src/condor_dagman/job.h b/src/condor_dagman/job.h
index 4bbf9e5..15926c2 100644
--- a/src/condor_dagman/job.h
+++ b/src/condor_dagman/job.h
@@ -75,14 +75,6 @@ typedef int JobID_t;
      queue) are put on the DAG's ready list.  */
 class Job {
   public:
-
-        // possible kinds of job (e.g., Condor, Stork, etc.)
-        // NOTE: must be kept in sync with _job_type_strings[]
-        // NOTE: must be kept in sync with enum Log_source
-	typedef enum {
-		TYPE_CONDOR,
-		TYPE_STORK,
-	 } job_type_t;
   
     /** Enumeration for specifying which queue for Add() and Remove().
         If you change this enum, you *must* also update queue_t_names
@@ -157,13 +149,12 @@ class Job {
 	static int NOOP_NODE_PROCID;
   
     /** Constructor
-        @param jobType Type of job in dag file.
         @param jobName Name of job in dag file.  String is deep copied.
 		@param directory Directory to run the node in, "" if current
 		       directory.  String is deep copied.
         @param cmdFile Path to condor cmd file.  String is deep copied.
     */
-    Job( const job_type_t jobType, const char* jobName,
+    Job( const char* jobName,
 				const char* directory, const char* cmdFile ); 
   
     ~Job();
@@ -177,8 +168,7 @@ class Job {
 	inline int GetRetries() const { return retries; }
 	const char* GetPreScriptName() const;
 	const char* GetPostScriptName() const;
-	const char* JobTypeString() const;
-	job_type_t JobType() const;
+	static const char* JobTypeString() { return "Condor"; }
 
 	bool AddPreScript( const char *cmd, MyString &whynot );
 	bool AddPreSkip( int exitCode, MyString &whynot );
@@ -221,14 +211,6 @@ class Job {
     */
     bool Remove (const queue_t queue, const JobID_t jobID);
 
-	/** Check whether the submit file for this job has a log file
-	    defined.
-		@param usingDefault is true if DAGman is watching the
-			default node log
-		@return true iff the submit file defines a log file
-	*/
-	bool CheckForLogFile( bool usingDefault ) const;
-
     /** Returns true if a queue is empty (has no jobs)
         @param queue Selects which queue to look at
         @return true: queue is empty, false: otherwise
@@ -267,6 +249,7 @@ class Job {
 			@return true: specified node is our child, false: otherwise
 		*/
 	bool HasChild( Job* child );
+
 		/** Is the specified node a parent of this node?
 			@param child Pointer to the node to check for parenthood.
 			@return true: specified node is our parent, false: otherwise
@@ -284,10 +267,12 @@ class Job {
 	*/
     void Dump ( const Dag *dag ) const;
   
+#if 0 // not used -- wenger 2015-02-17
     /** Print the identification info for this Job to stdout.
         @param condorID If true, also print the job's CondorID
      */
     void Print (bool condorID = false) const;
+#endif
   
 		// double-check internal data structures for consistency
 	bool SanityCheck() const;
@@ -333,7 +318,7 @@ class Job {
 		@param prefix: the prefix to be joined to the directory using "/"
 		@return void
 	*/
-	void PrefixDirectory(MyString &prefix);
+	void PrefixDirectory( MyString &prefix );
 
 	/** Set the DAG file (if any) for this node.  (This is set for nested
 			DAGs defined with the "SUBDAG" keyword.)
@@ -349,44 +334,6 @@ class Job {
 		return _dagFile;
 	}
 
-	/** Monitor this node's Condor or Stork log file with the
-		multiple log reader.  (Must be called before this node's
-		job is submitted.)
-		@param logReader: the multiple log reader
-		@param recovery: whether we're in recovery mode
-		@param defaultNodeLog: the default log file to be used if the
-			node's submit file doesn't define a log file
-		@param usingDefault: whether we're using the default/workflow
-			log at the DAG level
-		@return true if successful, false if failed
-	*/
-	bool MonitorLogFile( ReadMultipleUserLogs &condorLogReader,
-				ReadMultipleUserLogs &storkLogReader, bool nfsIsError,
-				bool recovery, const char *defaultNodeLog, bool usingDefault );
-
-	/** Unmonitor this node's Condor or Stork log file with the
-		multiple log reader.  (Must be called after everything is done
-		for this node.)
-		@param logReader: the multiple log reader
-		@return true if successful, false if failed
-	*/
-	bool UnmonitorLogFile( ReadMultipleUserLogs &logReader,
-				ReadMultipleUserLogs &storkLogReader );
-
-		// Whether this node is using the default node log file.
-	bool UsingDefaultLog() const { return _useDefaultLog; }
-
-	/** Get the log file for this node.
-		@return the name of this node's log file.
-	*/
-	const char *GetLogFile() const { return _logFile; }
-
-	/** Get whether this node's log file is XML (versus "standard"
-		format).
-		@return true iff the log file is XML.
-	*/
-	bool GetLogFileIsXml() const { return _logFileIsXml; }
-
 	/** Get the jobstate.log job tag for this node.
 		@return The job tag (can be "local"; if no tag is specified,
 			the value will be "-").
@@ -541,33 +488,12 @@ private:
 		*/
 	void Cleanup();
 
-		/** Get the log file specified in the given submit file, if
-			any.  Note that if the job is an HTCondor job and
-			usingWorkflowLog is true, this method will return "" for
-			the log file name.
-			@param usingWorkflowLog: true iff we're using the workflow
-				log file to monitor jobs
-			@param logFile: a MyString to hold the log file name
-			@return true on success, false otherwise
-		*/
-	bool FindLogFile( bool usingWorkflowLog, MyString &logFile );
-
 		/** _onHold[proc] is nonzero if the condor job 
  			with ProcId == proc is on hold, and zero
 			otherwise
 		*/
 	std::vector<unsigned char> _onHold;	
 
-		// Mark this node as failed because of an error in monitoring
-		// the log file.
-  	void LogMonitorFailed();
-
-        // strings for job_type_t (e.g., "Condor, "Stork", etc.)
-    static const char* _job_type_names[];
-
-		// type of job (e.g., Condor, Stork, etc.)
-	job_type_t _jobType;
-
 		// Directory to cd to before running the job or the PRE and POST
 		// scripts.
 	char * _directory;
@@ -621,20 +547,6 @@ private:
 		// ThrottleByCategory object.
 	ThrottleByCategory::ThrottleInfo *_throttleInfo;
 
-		// Whether this node's log file is currently being monitored.
-	bool _logIsMonitored;
-
-		// Whether this node uses the default user log file.
-	bool _useDefaultLog;
-
-		// The log file for this job -- it will be assigned the default
-		// log file name if no log file is specified in the submit file.
-	char *_logFile;
-
-		// Whether the log file is XML.
-	bool _logFileIsXml;
-
-
 		// Whether this is a noop job (shouldn't actually be submitted
 		// to Condor).
 	bool _noop;
@@ -683,11 +595,13 @@ private:
 	std::vector<unsigned char> _gotEvents;	
 };
 
+#if 0 // not used -- wenger 2015-02-17
 /** A wrapper function for Job::Print which allows a NULL job pointer.
     @param job Pointer to job object, if NULL then "(UNKNOWN)" is printed
     @param condorID If true, also print the job's CondorID
 */
 void job_print (Job * job, bool condorID = false);
+#endif
 
 
 #endif /* ifndef JOB_H */
diff --git a/src/condor_dagman/parse.cpp b/src/condor_dagman/parse.cpp
index f77b1d7..7358a22 100644
--- a/src/condor_dagman/parse.cpp
+++ b/src/condor_dagman/parse.cpp
@@ -52,12 +52,12 @@ static bool _useDagDir = false;
 static int _thisDagNum = -1;
 static bool _mungeNames = true;
 
-static bool parse_subdag( Dag *dag, Job::job_type_t nodeType,
+static bool parse_subdag( Dag *dag,
 						const char* nodeTypeKeyword,
 						const char* dagFile, int lineNum,
 						const char *directory);
 
-static bool parse_node( Dag *dag, Job::job_type_t nodeType,
+static bool parse_node( Dag *dag,
 						const char* nodeTypeKeyword,
 						const char* dagFile, int lineNum,
 						const char *directory, const char *inlineOrExt,
@@ -221,7 +221,7 @@ bool parse (Dag *dag, const char *filename, bool useDagDir) {
 		//
 		if(strcasecmp(token, "JOB") == 0) {
 			parsed_line_successfully = parse_node( dag, 
-					   Job::TYPE_CONDOR, token,
+					   token,
 					   filename, lineNumber, tmpDirectory.Value(), "",
 					   "submitfile" );
 		}
@@ -230,35 +230,29 @@ bool parse (Dag *dag, const char *filename, bool useDagDir) {
 		// Example Syntax is:  DATA j1 j1.dapsubmit [DONE]
 		//
 		else if	(strcasecmp(token, "DAP") == 0) {	// DEPRECATED!
-			parsed_line_successfully = parse_node( dag,
-					   Job::TYPE_STORK, token,
-					   filename, lineNumber, tmpDirectory.Value(), "",
-					   "submitfile" );
 			debug_printf( DEBUG_QUIET, "%s (line %d): "
-				"Warning: the DAP token is deprecated and may be unsupported "
-				"in a future release.  Use the DATA token\n",
+				"ERROR: the DAP token is no longer supported\n",
 				filename, lineNumber );
-			check_warning_strictness( DAG_STRICT_2 );
+			parsed_line_successfully = false;
 		}
 
 		else if	(strcasecmp(token, "DATA") == 0) {
-			parsed_line_successfully = parse_node( dag,
-					   Job::TYPE_STORK, token,
-					   filename, lineNumber, tmpDirectory.Value(), "",
-					   "submitfile");
+			debug_printf( DEBUG_QUIET, "%s (line %d): "
+				"ERROR: the DATA token is no longer supported\n",
+				filename, lineNumber );
+			parsed_line_successfully = false;
 		}
 
 		// Handle a SUBDAG spec
 		else if	(strcasecmp(token, "SUBDAG") == 0) {
 			parsed_line_successfully = parse_subdag( dag, 
-						Job::TYPE_CONDOR,
 						token, filename, lineNumber, tmpDirectory.Value() );
 		}
 
 		// Handle a FINAL spec
 		else if(strcasecmp(token, "FINAL") == 0) {
 			parsed_line_successfully = parse_node( dag, 
-					   Job::TYPE_CONDOR, token,
+					   token,
 					   filename, lineNumber, tmpDirectory.Value(), "",
 					   "submitfile" );
 		}
@@ -423,7 +417,7 @@ bool parse (Dag *dag, const char *filename, bool useDagDir) {
 }
 
 static bool 
-parse_subdag( Dag *dag, Job::job_type_t nodeType,
+parse_subdag( Dag *dag, 
 			const char* nodeTypeKeyword,
 			const char* dagFile, int lineNum, const char *directory )
 {
@@ -434,7 +428,7 @@ parse_subdag( Dag *dag, Job::job_type_t nodeType,
 		return false;
 	}
 	if ( !strcasecmp( inlineOrExt, "EXTERNAL" ) ) {
-		return parse_node( dag, nodeType, nodeTypeKeyword, dagFile,
+		return parse_node( dag, nodeTypeKeyword, dagFile,
 					lineNum, directory, " EXTERNAL", "dagfile" );
 	}
 
@@ -444,7 +438,7 @@ parse_subdag( Dag *dag, Job::job_type_t nodeType,
 }
 
 static bool 
-parse_node( Dag *dag, Job::job_type_t nodeType,
+parse_node( Dag *dag, 
 			const char* nodeTypeKeyword,
 			const char* dagFile, int lineNum, const char *directory,
 			const char *inlineOrExt, const char *submitOrDagFile)
@@ -583,7 +577,7 @@ parse_node( Dag *dag, Job::job_type_t nodeType,
 
 	// looks ok, so add it
 	bool isFinal = strcasecmp( nodeTypeKeyword, "FINAL" ) == MATCH;
-	if( !AddNode( dag, nodeType, nodeName, directory,
+	if( !AddNode( dag, nodeName, directory,
 				submitFile, NULL, NULL, noop, done, isFinal, whynot ) )
 	{
 		debug_printf( DEBUG_QUIET, "ERROR: %s (line %d): %s\n",
@@ -1662,7 +1656,6 @@ parse_splice(
 							dag->RetrySubmitFirst(),
 							dag->RetryNodeFirst(),
 							dag->CondorRmExe(),
-							dag->StorkRmExe(),
 							dag->DAGManJobId(),
 							dag->ProhibitMultiJobs(),
 							dag->SubmitDepthFirst(),
diff --git a/src/condor_dagman/submit.h b/src/condor_dagman/submit.h
index df8b79a..b9b27f5 100644
--- a/src/condor_dagman/submit.h
+++ b/src/condor_dagman/submit.h
@@ -58,18 +58,14 @@ bool condor_submit( const Dagman &dm, const char* cmdFile, CondorID& condorID,
 					const char* directory, const char *worflowLogFile,
 					bool prohibitMultiJobs, bool hold_claim );
 
-bool stork_submit( const Dagman &dm, const char* cmdFile, CondorID& condorID,
-				   const char* DAGNodeName, const char* directory );
-
 void set_fake_condorID( int subprocID );
 
 bool fake_condor_submit( CondorID& condorID, Job* job, const char* DAGNodeName,
-					const char* directory, const char *logFile,
-					bool logIsXml );
+					const char* directory, const char *logFile );
 
 int get_fake_condorID();
 
 bool writePreSkipEvent( CondorID& condorID, Job* job, const char* DAGNodeName, 
-			   const char* directory, const char *logFile, bool logIsXml );
+			   const char* directory, const char *logFile );
 
 #endif /* #ifndef CONDOR_SUBMIT_H */
diff --git a/src/condor_tests/job_dagman_prepost.dag b/src/condor_tests/job_dagman_prepost.dag
index 0d162f0..9e27608 100644
--- a/src/condor_tests/job_dagman_prepost.dag
+++ b/src/condor_tests/job_dagman_prepost.dag
@@ -1,4 +1,5 @@
-Job     NODE0         job_dagman_prepost-node.cmd
+# Make sure PRE and POST scripts work right with NOOP node.
+Job     NODE0         job_dagman_prepost-node.cmd NOOP
 Job     NODE1         job_dagman_prepost-node.cmd
 Job     NODE2         job_dagman_prepost-node.cmd
 Job     NODE3         job_dagman_prepost-node.cmd
diff --git a/src/condor_tests/job_dagman_rescue_recov.run b/src/condor_tests/job_dagman_rescue_recov.run
index 232a670..9b6289d 100755
--- a/src/condor_tests/job_dagman_rescue_recov.run
+++ b/src/condor_tests/job_dagman_rescue_recov.run
@@ -54,12 +54,8 @@ foreach $name (@logfiles) {
 	}
 }
 
-# Get rid of the rescue DAGs we should have at the end.
-foreach $name (@rescuedags) {
-	if (-e $name) {
-		runcmd("rm -f $name");
-	}
-}
+# Get rid of all related rescue DAGs.
+runcmd("rm -f job_dagman_rescue_recov*rescue???");
 
 $abnormal = sub 
 {

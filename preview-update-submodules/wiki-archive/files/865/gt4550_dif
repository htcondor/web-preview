diff --git a/doc/admin-man/configure.tex b/doc/admin-man/configure.tex
index 6d3dde1..ee563eb 100644
--- a/doc/admin-man/configure.tex
+++ b/doc/admin-man/configure.tex
@@ -10228,6 +10228,13 @@ in section~\ref{sec:DAG-configuration}.
 
 \label{param:DAGManStorkSubmitExe}
 \item[\Macro{DAGMAN\_STORK\_SUBMIT\_EXE}]
+  This configuration variable is no longer used, because as of
+  version 8.3.4 \Condor{dagman} no longer supports Stork jobs.
+  Setting this configuration variable will result in a warning
+  from \Condor{dagman} (which will be turned into a fatal error
+  if \MacroNI{DAGMAN\_USE\_STRICT} is set to 1 or above).
+
+  For completeness, here is the definition for historical purposes: 
   The executable that \Condor{dagman} will use to submit Stork jobs.
   If not defined, \Condor{dagman} looks for \Prog{stork\_submit} in the path.
 
@@ -10238,6 +10245,13 @@ in section~\ref{sec:DAG-configuration}.
 
 \label{param:DAGManStorkRmExe}
 \item[\Macro{DAGMAN\_STORK\_RM\_EXE}]
+  This configuration variable is no longer used, because as of
+  version 8.3.4 \Condor{dagman} no longer supports Stork jobs.
+  Setting this configuration variable will result in a warning
+  from \Condor{dagman} (which will be turned into a fatal error
+  if \MacroNI{DAGMAN\_USE\_STRICT} is set to 1 or above).
+
+  For completeness, here is the definition for historical purposes: 
   The executable that \Condor{dagman} will use to remove Stork jobs.
   If not defined, \Condor{dagman} looks for \Prog{stork\_rm} in the path.
 
diff --git a/doc/user-man/dagman.tex b/doc/user-man/dagman.tex
index aad07a5..eff8484 100644
--- a/doc/user-man/dagman.tex
+++ b/doc/user-man/dagman.tex
@@ -1,3 +1,4 @@
+%TEMPTEMP -- remove all references to Stork once this gets merged w/ Karen's update
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \section{\label{sec:DAGMan}DAGMan Applications}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@@ -78,7 +79,7 @@ The input file used by DAGMan is called a DAG input file.
 It specifies the nodes of the DAG as well as the dependencies
 that order the DAG.
 All items are optional, except that there must be at least one \Arg{JOB}
-or \Arg{DATA} item.
+item.
 
 Comments may be placed in the DAG input file.
 The pound character (\verb@#@) as the first character on a
@@ -199,43 +200,7 @@ and the HTCondor job will not be submitted.
 \index{DAGMan input file!DATA key word}
 \item \Bold{DATA}
 
-The \Arg{DATA} key word specifies a job to be managed by the Stork data
-placement server.  
-Stork software is provided by the Stork project.
-Please refer to their website: 
-\URL{http://www.cct.lsu.edu/~kosar/stork/index.php}.
-
-The syntax used for each \Arg{DATA} entry is
-
-\Opt{DATA} \Arg{JobName} \Arg{SubmitDescriptionFileName}
-\oOptArg{DIR}{directory} \oOpt{NOOP} \oOpt{DONE}
-
-A \Arg{DATA} entry maps a \Arg{JobName} to a Stork submit description file.
-In all other respects, the \Arg{DATA} key word is identical to the
-\Arg{JOB} key word.
-
-The keywords \Arg{DIR}, \Arg{NOOP} and \Arg{DONE} 
-follow the same rules and restrictions, and they have the same effect
-for \Opt{DATA} nodes as they do for \Opt{JOB} nodes.
-
-Here is an example of a simple DAG that stages in data using Stork,
-processes the data using HTCondor, 
-and stages the processed data out using Stork.
-Depending upon the implementation, multiple data jobs to stage in data
-or to stage out data
-may be run in parallel.
-
-\footnotesize
-\begin{verbatim}
-    DATA    STAGE_IN1  stage_in1.stork
-    DATA    STAGE_IN2  stage_in2.stork
-    JOB     PROCESS    process.condor 
-    DATA    STAGE_OUT1 stage_out1.stork
-    DATA    STAGE_OUT2 stage_out2.stork
-    PARENT  STAGE_IN1 STAGE_IN2 CHILD PROCESS
-    PARENT  PROCESS CHILD STAGE_OUT1 STAGE_OUT2
-\end{verbatim}
-\normalsize
+As of version 8.3.4, \Condor{dagman} no longer supports DATA nodes.
 
 %----------------------------
 \label{dagman:ParentChild}
diff --git a/doc/version-history/8-3.history.tex b/doc/version-history/8-3.history.tex
index 7283869..5672b6e 100644
--- a/doc/version-history/8-3.history.tex
+++ b/doc/version-history/8-3.history.tex
@@ -33,6 +33,10 @@ The details of each version are described below.
 file when node jobs go on hold.
 \Ticket{4766}
 
+%TEMPTEMP -- this needs to get moved into the 8.3.4 section once things are merged
+\item \Condor{dagman} no longer supports Stork jobs as DAG nodes.
+\Ticket{4550}
+
 \end{itemize}
 
 \noindent Bugs Fixed:
diff --git a/src/condor_dagman/dag.cpp b/src/condor_dagman/dag.cpp
index 82bbec1..1d3b83f 100644
--- a/src/condor_dagman/dag.cpp
+++ b/src/condor_dagman/dag.cpp
@@ -22,7 +22,7 @@
 // We are calling a *node* the combination of pre-script, job, and
 // post-script.
 // We are calling a *job* essentially what results from one invocation
-// of condor_submit or stork_submit.
+// of condor_submit.
 // We are calling a *job proc* an individual batch system process within
 // a cluster.
 // So nodes-to-jobs is 1-to-1; jobs to job procs is 1-to-n.
@@ -88,7 +88,7 @@ Dag::Dag( /* const */ StringList &dagFiles,
 		  bool allowLogError,
 		  bool useDagDir, int maxIdleJobProcs, bool retrySubmitFirst,
 		  bool retryNodeFirst, const char *condorRmExe,
-		  const char *storkRmExe, const CondorID *DAGManJobID,
+		  const CondorID *DAGManJobID,
 		  bool prohibitMultiJobs, bool submitDepthFirst,
 		  const char *defaultNodeLog, bool generateSubdagSubmits,
 		  SubmitDagDeepOptions *submitDagDeepOpts, bool isSplice,
@@ -103,7 +103,6 @@ Dag::Dag( /* const */ StringList &dagFiles,
 	_nodeNameHash		  (NODE_HASH_SIZE, MyStringHash, rejectDuplicateKeys),
 	_nodeIDHash			  (NODE_HASH_SIZE, hashFuncInt, rejectDuplicateKeys),
 	_condorIDHash		  (NODE_HASH_SIZE, hashFuncInt, rejectDuplicateKeys),
-	_storkIDHash		  (NODE_HASH_SIZE, hashFuncInt, rejectDuplicateKeys),
 	_noopIDHash			  (NODE_HASH_SIZE, hashFuncInt, rejectDuplicateKeys),
     _numNodesDone         (0),
     _numNodesFailed       (0),
@@ -116,12 +115,10 @@ Dag::Dag( /* const */ StringList &dagFiles,
 	m_retrySubmitFirst	  (retrySubmitFirst),
 	m_retryNodeFirst	  (retryNodeFirst),
 	_condorRmExe		  (condorRmExe),
-	_storkRmExe			  (storkRmExe),
 	_DAGManJobId		  (DAGManJobID),
 	_preRunNodeCount	  (0),
 	_postRunNodeCount	  (0),
 	_checkCondorEvents    (),
-	_checkStorkEvents     (),
 	_maxJobsDeferredCount (0),
 	_maxIdleDeferredCount (0),
 	_catThrottleDeferredCount (0),
@@ -316,8 +313,9 @@ bool Dag::Bootstrap (bool recovery)
    		jobs.ToBeforeFirst();
    		while( jobs.Next( job ) ) {
 			if ( job->CanSubmit() ) {
-				if ( !job->MonitorLogFile( _condorLogRdr, _storkLogRdr,
-							_nfsLogIsError, recovery, _defaultNodeLog, _use_default_node_log ) ) {
+				if ( !job->MonitorLogFile( _condorLogRdr, _nfsLogIsError,
+							recovery, _defaultNodeLog,
+							_use_default_node_log ) ) {
 					debug_cache_stop_caching();
 					_jobstateLog.WriteRecoveryFailure();
 					return false;
@@ -325,19 +323,8 @@ bool Dag::Bootstrap (bool recovery)
 			}
 		}
 
-			// Note: I just realized that this will almost certainly fail
-			// on a combination of Condor and Stork events -- we probably
-			// need a loop around the event processing.  wenger 2009-06-18.
 		if( CondorLogFileCount() > 0 ) {
-			if( !ProcessLogEvents( CONDORLOG, recovery ) ) {
-				_recovery = false;
-				debug_cache_stop_caching();
-				_jobstateLog.WriteRecoveryFailure();
-				return false;
-			}
-		}
-		if( StorkLogFileCount() > 0 ) {
-			if( !ProcessLogEvents( DAPLOG, recovery ) ) {
+			if( !ProcessLogEvents( recovery ) ) {
 				_recovery = false;
 				debug_cache_stop_caching();
 				_jobstateLog.WriteRecoveryFailure();
@@ -467,29 +454,11 @@ Dag::DetectCondorLogGrowth () {
 }
 
 //-------------------------------------------------------------------------
-bool Dag::DetectDaPLogGrowth () {
-
-	if( StorkLogFileCount() <= 0 ) {
-		return false;
-	}
-
-	bool growth = _storkLogRdr.detectLogGrowth();
-    debug_printf( DEBUG_DEBUG_4, "%s\n",
-				  growth ? "Log GREW!" : "No log growth..." );
-    return growth;
-}
-
-//-------------------------------------------------------------------------
 // Developer's Note: returning false tells main_timer to abort the DAG
-bool Dag::ProcessLogEvents (int logsource, bool recovery) {
+bool Dag::ProcessLogEvents (bool recovery) {
 
-	if ( logsource == CONDORLOG ) {
-		debug_printf( DEBUG_VERBOSE, "Currently monitoring %d Condor "
-					"log file(s)\n", _condorLogRdr.activeLogFileCount() );
-	} else if ( logsource == DAPLOG ) {
-		debug_printf( DEBUG_VERBOSE, "Currently monitoring %d Stork "
-					"log file(s)\n", _storkLogRdr.activeLogFileCount() );
-	}
+	debug_printf( DEBUG_VERBOSE, "Currently monitoring %d Condor "
+				"log file(s)\n", _condorLogRdr.activeLogFileCount() );
 
 	bool done = false;  // Keep scanning until ULOG_NO_EVENT
 	bool result = true;
@@ -498,13 +467,9 @@ bool Dag::ProcessLogEvents (int logsource, bool recovery) {
 		ULogEvent* e = NULL;
 		ULogEventOutcome outcome = ULOG_NO_EVENT;
 
-		if ( logsource == CONDORLOG ) {
-			outcome = _condorLogRdr.readEvent(e);
-		} else if ( logsource == DAPLOG ){
-			outcome = _storkLogRdr.readEvent(e);
-		}
+		outcome = _condorLogRdr.readEvent(e);
 
-		bool tmpResult = ProcessOneEvent( logsource, outcome, e, recovery,
+		bool tmpResult = ProcessOneEvent( outcome, e, recovery,
 					done );
 			// If ProcessOneEvent returns false, the result here must
 			// be false.
@@ -517,7 +482,7 @@ bool Dag::ProcessLogEvents (int logsource, bool recovery) {
 	}
 
 	if (DEBUG_LEVEL(DEBUG_VERBOSE) && recovery) {
-		const char *name = (logsource == CONDORLOG) ? "Condor" : "Stork";
+		const char *name = "Condor";
 		debug_printf( DEBUG_NORMAL, "    ------------------------------\n");
 		debug_printf( DEBUG_NORMAL, "       %s Recovery Complete\n", name);
 		debug_printf( DEBUG_NORMAL, "    ------------------------------\n");
@@ -536,7 +501,7 @@ bool Dag::ProcessLogEvents (int logsource, bool recovery) {
 
 //---------------------------------------------------------------------------
 // Developer's Note: returning false tells main_timer to abort the DAG
-bool Dag::ProcessOneEvent (int logsource, ULogEventOutcome outcome,
+bool Dag::ProcessOneEvent (ULogEventOutcome outcome,
 		const ULogEvent *event, bool recovery, bool &done) {
 
 	bool result = true;
@@ -592,14 +557,14 @@ bool Dag::ProcessOneEvent (int logsource, ULogEventOutcome outcome,
 		{
 			ASSERT( event != NULL );
 			bool submitEventIsSane;
-			Job *job = LogEventNodeLookup( logsource, event,
+			Job *job = LogEventNodeLookup( event,
 						submitEventIsSane );
 			PrintEvent( DEBUG_VERBOSE, event, job, recovery );
 			if( !job ) {
 					// event is for a job outside this DAG; ignore it
 				break;
 			}
-			if( !EventSanityCheck( logsource, event, job, &result ) ) {
+			if( !EventSanityCheck( event, job, &result ) ) {
 					// this event is "impossible"; we will either
 					// abort the DAG (if result was set to false) or
 					// ignore it and hope for the best...
@@ -687,7 +652,7 @@ bool Dag::ProcessOneEvent (int logsource, ULogEventOutcome outcome,
 
 			case ULOG_PRESKIP:
 				TerminateJob( job, recovery );
-				job->UnmonitorLogFile( _condorLogRdr, _storkLogRdr );
+				job->UnmonitorLogFile( _condorLogRdr );
 				if(!recovery) {
 					--_preRunNodeCount;
 				}
@@ -876,30 +841,16 @@ Dag::RemoveBatchJob(Job *node) {
 	ArgList args;
 	MyString constraint;
 
-	switch ( node->JobType() ) {
-	case Job::TYPE_CONDOR:
-		args.AppendArg( _condorRmExe );
-		args.AppendArg( "-const" );
-
-			// Adding this DAGMan's cluster ID as a constraint to
-			// be extra-careful to avoid removing someone else's
-			// job.
-		constraint.formatstr( "%s =?= %d && %s =?= %d",
-					ATTR_DAGMAN_JOB_ID, _DAGManJobId->_cluster,
-					ATTR_CLUSTER_ID, node->GetCluster() );
-		args.AppendArg( constraint.Value() );
-		break;
-
-	case Job::TYPE_STORK:
-		args.AppendArg( _storkRmExe );
-		args.AppendArg( node->GetCluster() );
-		break;
+	args.AppendArg( _condorRmExe );
+	args.AppendArg( "-const" );
 
-	default:
-		EXCEPT( "Illegal job (%d) type for node %s", node->JobType(),
-					node->GetJobName() );
-		break;
-	}
+		// Adding this DAGMan's cluster ID as a constraint to
+		// be extra-careful to avoid removing someone else's
+		// job.
+	constraint.formatstr( "%s =?= %d && %s =?= %d",
+				ATTR_DAGMAN_JOB_ID, _DAGManJobId->_cluster,
+				ATTR_CLUSTER_ID, node->GetCluster() );
+	args.AppendArg( constraint.Value() );
 	
 	MyString display;
 	args.GetArgsStringForDisplay( &display );
@@ -923,7 +874,7 @@ Dag::ProcessJobProcEnd(Job *job, bool recovery, bool failed) {
 	ASSERT ( _isSplice == false );
 
 	if ( job->_queuedNodeJobProcs == 0 ) {
-		(void)job->UnmonitorLogFile( _condorLogRdr, _storkLogRdr );
+		(void)job->UnmonitorLogFile( _condorLogRdr );
 
 			// Log job success or failure if necessary.
 		_jobstateLog.WriteJobSuccessOrFailure( job );
@@ -969,8 +920,9 @@ Dag::ProcessJobProcEnd(Job *job, bool recovery, bool failed) {
 			if ( recovery ) {
 				job->SetStatus( Job::STATUS_POSTRUN );
 				_postRunNodeCount++;
-				(void)job->MonitorLogFile( _condorLogRdr, _storkLogRdr,
-						_nfsLogIsError, _recovery, _defaultNodeLog, _use_default_node_log );
+				(void)job->MonitorLogFile( _condorLogRdr, _nfsLogIsError,
+							_recovery, _defaultNodeLog,
+							_use_default_node_log );
 			} else {
 				(void)RunPostScript( job, _alwaysRunPost, 0 );
 			}
@@ -988,7 +940,7 @@ Dag::ProcessPostTermEvent(const ULogEvent *event, Job *job,
 		bool recovery) {
 
 	if( job ) {
-		(void)job->UnmonitorLogFile( _condorLogRdr, _storkLogRdr );
+		(void)job->UnmonitorLogFile( _condorLogRdr );
 
 			// Note: "|| recovery" below is somewhat of a "quick and dirty"
 			// fix to Gnats PR 357.  The first part of the assert can fail
@@ -1367,7 +1319,7 @@ Dag::NodeExists( const char* nodeName ) const
 }
 
 //---------------------------------------------------------------------------
-Job * Dag::FindNodeByEventID ( int logsource, const CondorID condorID ) const {
+Job * Dag::FindNodeByEventID ( const CondorID condorID ) const {
 	if ( condorID._cluster == -1 ) {
 		return NULL;
 	}
@@ -1375,7 +1327,7 @@ Job * Dag::FindNodeByEventID ( int logsource, const CondorID condorID ) const {
 	Job *	node = NULL;
 	bool isNoop = JobIsNoop( condorID );
 	int id = GetIndexID( condorID );
-	if ( GetEventIDHash( isNoop, logsource )->lookup(id, node) != 0 ) {
+	if ( GetEventIDHash( isNoop )->lookup(id, node) != 0 ) {
 			// Note: eventually get rid of the "(might be because of
 			// node retries)" message here, and have code that explicitly
 			// figures out whether the node was not found because of a
@@ -1415,7 +1367,6 @@ void
 Dag::SetAllowEvents( int allowEvents)
 {
 	_checkCondorEvents.SetAllowEvents( allowEvents );
-	_checkStorkEvents.SetAllowEvents( allowEvents );
 }
 
 //-------------------------------------------------------------------------
@@ -1736,7 +1687,7 @@ Dag::PreScriptReaper( Job *job, int status )
 			CondorID id;
 				// This might be the first time we watch the file, so we
 				// monitor it.
-			if ( !job->MonitorLogFile( _condorLogRdr, _storkLogRdr, _nfsLogIsError,
+			if ( !job->MonitorLogFile( _condorLogRdr, _nfsLogIsError,
 					_recovery, _defaultNodeLog, _use_default_node_log ) ) {
 				return 0;
 			}
@@ -1822,7 +1773,7 @@ bool Dag::RunPostScript( Job *job, bool ignore_status, int status,
 	// a POST script is specified for the job, so run it
 	// We are told to ignore the result of the PRE script
 	job->SetStatus( Job::STATUS_POSTRUN );
-	if ( !job->MonitorLogFile( _condorLogRdr, _storkLogRdr,
+	if ( !job->MonitorLogFile( _condorLogRdr, 
 			_nfsLogIsError, _recovery, _defaultNodeLog, _use_default_node_log ) ) {
 		debug_printf(DEBUG_QUIET, "Unable to monitor user logfile for node %s\n",
 			job->GetJobName() );
@@ -1863,66 +1814,46 @@ Dag::PostScriptReaper( Job *job, int status )
 		e.returnValue = WEXITSTATUS( status );
 	}
 
-		// Note: after 6.7.15 is released, we'll be disabling the old-style
-		// Stork logs, so we should probably go ahead and write the POST
-		// script terminated events for Stork jobs here (although that
-		// could cause some backwards-compatibility problems).  wenger
-		// 2006-01-12.
-	if ( job->JobType() == Job::TYPE_STORK ) {
-			// Kludgey fix for Gnats PR 554 -- we are bypassing the whole
-			// writing of the ULOG_POST_SCRIPT_TERMINATED event because
-			// Stork doesn't use the UserLog code, and therefore presumably
-			// doesn't have the correct file locking on the log file.
-			// This means that in recovery mode we'll end up running this
-			// POST script again even if we successfully ran it already.
-			// wenger 2005-10-04.
-		e.cluster = job->GetCluster();
-		e.proc = job->GetProc();
-		e.subproc = job->GetSubProc();
-		ProcessPostTermEvent(&e, job, _recovery);
-	} else {
-
-		e.cluster = job->GetCluster();
-		e.proc = job->GetProc();
-		e.subproc = job->GetSubProc();
-		WriteUserLog ulog;
-			// Disabling the global log (EventLog) fixes the main problem
-			// in gittrac #934 (if you can't write to the global log the
-			// write to the user log also fails, and DAGMan hangs
-			// waiting for the event that wasn't written).
-		ulog.setEnableGlobalLog( false );
-		ulog.setUseXML( !_use_default_node_log && job->GetLogFileIsXml() );
-			// For NOOP jobs, we need the proc and subproc values;
-			// for "real" jobs, they are not significant.
-		int procID = job->GetNoop() ? job->GetProc() : 0;
-		int subprocID = job->GetNoop() ? job->GetSubProc() : 0;
-		const char* s = _use_default_node_log ? DefaultNodeLog() :
-			job->GetLogFile();
-		if( !s ) { 
-				// User did not specify a log
-				// We specify one for him
-				// Default log is never in XML format
-			s = DefaultNodeLog();
-			ulog.setUseXML( false );
-		}
-		debug_printf( DEBUG_QUIET, "Initializing logfile %s, %d, %d, %d\n",
-			s, job->GetCluster(), procID, subprocID );
-		ulog.initialize( std::vector<const char*>(1,s), job->GetCluster(),
-			procID, subprocID, NULL );
-
-		for(int write_attempts = 0;;++write_attempts) {
-			if( !ulog.writeEvent( &e ) ) {
-				if( write_attempts >= 2 ) {
-					debug_printf( DEBUG_QUIET,
-							"Unable to log ULOG_POST_SCRIPT_TERMINATED event\n" );
-					// Exit here, because otherwise we'll wait forever to see
-					// the event that we just failed to write (see gittrac #934).
-					// wenger 2009-11-12.
-					main_shutdown_rescue( EXIT_ERROR, DAG_STATUS_ERROR );
-				}
-			} else {
-				break;
+	e.cluster = job->GetCluster();
+	e.proc = job->GetProc();
+	e.subproc = job->GetSubProc();
+	WriteUserLog ulog;
+		// Disabling the global log (EventLog) fixes the main problem
+		// in gittrac #934 (if you can't write to the global log the
+		// write to the user log also fails, and DAGMan hangs
+		// waiting for the event that wasn't written).
+	ulog.setEnableGlobalLog( false );
+	ulog.setUseXML( !_use_default_node_log && job->GetLogFileIsXml() );
+		// For NOOP jobs, we need the proc and subproc values;
+		// for "real" jobs, they are not significant.
+	int procID = job->GetNoop() ? job->GetProc() : 0;
+	int subprocID = job->GetNoop() ? job->GetSubProc() : 0;
+	const char* s = _use_default_node_log ? DefaultNodeLog() :
+		job->GetLogFile();
+	if( !s ) { 
+			// User did not specify a log
+			// We specify one for him
+			// Default log is never in XML format
+		s = DefaultNodeLog();
+		ulog.setUseXML( false );
+	}
+	debug_printf( DEBUG_QUIET, "Initializing logfile %s, %d, %d, %d\n",
+		s, job->GetCluster(), procID, subprocID );
+	ulog.initialize( std::vector<const char*>(1,s), job->GetCluster(),
+		procID, subprocID, NULL );
+
+	for(int write_attempts = 0;;++write_attempts) {
+		if( !ulog.writeEvent( &e ) ) {
+			if( write_attempts >= 2 ) {
+				debug_printf( DEBUG_QUIET,
+						"Unable to log ULOG_POST_SCRIPT_TERMINATED event\n" );
+				// Exit here, because otherwise we'll wait forever to see
+				// the event that we just failed to write (see gittrac #934).
+				// wenger 2009-11-12.
+				main_shutdown_rescue( EXIT_ERROR, DAG_STATUS_ERROR );
 			}
+		} else {
+			break;
 		}
 	}
 	return true;
@@ -2088,27 +2019,15 @@ void Dag::RemoveRunningJobs ( const CondorID &dmJobId, bool removeCondorJobs,
 {
 	if ( bForce ) removeCondorJobs = true;
 
-	const char *conJobs = removeCondorJobs ? "Condor/" : "";
-	debug_printf( DEBUG_NORMAL, "Removing any/all submitted %s"
-				"Stork jobs...\n", conJobs );
-
-		// first, remove all Condor jobs submitted by this DAGMan
-		// Make sure we have at least one Condor (not Stork) job before
-		// we call condor_rm...
-	bool	haveCondorJob = bForce;
-    ListIterator<Job> jobList(_jobs);
-    Job * job;
-    while (jobList.Next(job)) {
-		ASSERT( job != NULL );
-		if ( job->JobType() == Job::TYPE_CONDOR ) {
-			haveCondorJob = true;
-			break;
-		}
-	}
+	// Hmm -- should we check here if we have jobs queued? wenger 2014-12-17
+	bool	haveCondorJob = true;
 
 	ArgList args;
 
 	if ( removeCondorJobs && haveCondorJob ) {
+		debug_printf( DEBUG_NORMAL, "Removing any/all submitted "
+					"Condor jobs...\n" );
+
 		MyString constraint;
 
 		args.Clear();
@@ -2123,26 +2042,6 @@ void Dag::RemoveRunningJobs ( const CondorID &dmJobId, bool removeCondorJobs,
 		}
 	}
 
-		// Okay, now remove any Stork jobs.
-    ListIterator<Job> iList(_jobs);
-    while (iList.Next(job)) {
-		ASSERT( job != NULL );
-			// if node has a Stork job that is presently submitted,
-			// remove it individually (this is necessary because
-			// DAGMan's job ID can't currently be inserted into the
-			// Stork job ad, and thus we can't do a "stork_rm -const..." 
-			// like we do with Condor; this should be fixed)
-		if( job->JobType() == Job::TYPE_STORK &&
-			job->GetStatus() == Job::STATUS_SUBMITTED ) {
-			args.Clear();
-			args.AppendArg( _storkRmExe );
-			args.AppendArg( job->GetCluster() );
-			if ( util_popen( args ) != 0 ) {
-				debug_printf( DEBUG_NORMAL, "Error removing Stork job\n");
-			}
-        }
-	}
-
 	return;
 }
 
@@ -2353,12 +2252,8 @@ Dag::WriteNodeToRescue( FILE *fp, Job *node, bool reset_retries_upon_rescue,
 	const char *keyword = "";
 	if ( node->GetFinal() ) {
 		keyword = "FINAL";
-	} else if ( node->JobType() == Job::TYPE_CONDOR ) {
-		keyword = node->GetDagFile() ? "SUBDAG EXTERNAL" : "JOB";
-	} else if( node->JobType() == Job::TYPE_STORK ) {
-		keyword = "DATA";
 	} else {
-		EXCEPT( "Illegal node type (%d)", node->JobType() );
+		keyword = node->GetDagFile() ? "SUBDAG EXTERNAL" : "JOB";
 	}
 
 	if ( !isPartial ) {
@@ -2526,7 +2421,7 @@ Dag::TerminateJob( Job* job, bool recovery, bool bootstrap )
 				if ( recovery ) {
 						// We need to monitor the log file for the node that's
 						// newly ready.
-					(void)child->MonitorLogFile( _condorLogRdr, _storkLogRdr,
+					(void)child->MonitorLogFile( _condorLogRdr, 
 								_nfsLogIsError, recovery, _defaultNodeLog, _use_default_node_log);
 				} else {
 						// If child has no more parents in its waiting queue,
@@ -2604,10 +2499,8 @@ Dag::RestartNode( Job *node, bool recovery )
 			// Note: the if checking against the default condor ID
 			// should *always* be true here, but checking just to be safe.
 		if ( !(node->GetID() == _defaultCondorId) ) {
-			int logsource = node->JobType() == Job::TYPE_CONDOR ? CONDORLOG :
-						DAPLOG;
 			int id = GetIndexID( node->GetID() );
-			if ( GetEventIDHash( node->GetNoop(), logsource )->remove( id )
+			if ( GetEventIDHash( node->GetNoop() )->remove( id )
 						!= 0 ) {
 				EXCEPT( "Event ID hash table error!" );
 			}
@@ -2617,7 +2510,7 @@ Dag::RestartNode( Job *node, bool recovery )
 		// has retried nodes).  (See SubmitNodeJob() for where this
 		// gets done during "normal" running.)
 		node->SetCondorID( _defaultCondorId );
-		(void)node->MonitorLogFile( _condorLogRdr, _storkLogRdr,
+		(void)node->MonitorLogFile( _condorLogRdr, 
 					_nfsLogIsError, recovery, _defaultNodeLog, _use_default_node_log );
 	}
 }
@@ -3086,7 +2979,6 @@ Dag::DumpNodeStatus( bool held, bool removed )
 				// ready to submit if it doesn't have any unfinished
 				// parents.
 			if ( !node->CanSubmit() ) {
-				// See Job::_job_type_names for other strings.
 				status = Job::STATUS_NOT_READY;
 			}
 
@@ -3271,20 +3163,6 @@ Dag::CheckAllJobs()
 	} else {
 		debug_printf( DEBUG_DEBUG_1, "All Condor job events okay\n");
 	}
-
-	result = _checkStorkEvents.CheckAllJobs(jobError);
-	if ( result == CheckEvents::EVENT_ERROR ) {
-		debug_printf( DEBUG_QUIET, "Error checking Stork job events: %s\n",
-				jobError.Value() );
-		ASSERT( false );
-	} else if ( result == CheckEvents::EVENT_BAD_EVENT ||
-				result == CheckEvents::EVENT_WARNING ) {
-		debug_printf( DEBUG_NORMAL, "Warning checking Stork job events: %s\n",
-				jobError.Value() );
-		check_warning_strictness( DAG_STRICT_3 );
-	} else {
-		debug_printf( DEBUG_DEBUG_1, "All Stork job events okay\n");
-	}
 }
 
 //-------------------------------------------------------------------------
@@ -3744,7 +3622,7 @@ Dag::RemoveDependency( Job *parent, Job *child, MyString &whynot )
 
 //---------------------------------------------------------------------------
 Job*
-Dag::LogEventNodeLookup( int logsource, const ULogEvent* event,
+Dag::LogEventNodeLookup( const ULogEvent* event,
 			bool &submitEventIsSane )
 {
 	ASSERT( event );
@@ -3760,7 +3638,7 @@ Dag::LogEventNodeLookup( int logsource, const ULogEvent* event,
 	if ( event->eventNumber != ULOG_SUBMIT &&
 				event->eventNumber != ULOG_PRESKIP ) {
 		
-	  node = FindNodeByEventID( logsource, condorID );
+	  node = FindNodeByEventID( condorID );
 	  if( node ) {
 	    return node;
 	  }
@@ -3804,7 +3682,7 @@ Dag::LogEventNodeLookup( int logsource, const ULogEvent* event,
 					ASSERT( isNoop == node->GetNoop() );
 					int id = GetIndexID( condorID );
 					HashTable<int, Job *> *ht =
-								GetEventIDHash( isNoop, logsource );
+								GetEventIDHash( isNoop );
 					if ( ht->lookup(id, tmpNode) != 0 ) {
 							// Node not found.
 						int insertResult = ht->insert( id, node );
@@ -3848,7 +3726,7 @@ Dag::LogEventNodeLookup( int logsource, const ULogEvent* event,
 				bool isNoop = JobIsNoop( condorID );
 				int id = GetIndexID( condorID );
 				HashTable<int, Job *> *ht =
-							GetEventIDHash( isNoop, logsource );
+							GetEventIDHash( isNoop );
 				if ( ht->lookup(id, tmpNode) != 0 ) {
 						// Node not found.
 					int insertResult = ht->insert( id, node );
@@ -3890,7 +3768,7 @@ Dag::LogEventNodeLookup( int logsource, const ULogEvent* event,
 // (additionally sets *result=false if DAG should be aborted)
 
 bool
-Dag::EventSanityCheck( int logsource, const ULogEvent* event,
+Dag::EventSanityCheck( const ULogEvent* event,
 			const Job* node, bool* result )
 {
 	ASSERT( event );
@@ -3899,11 +3777,7 @@ Dag::EventSanityCheck( int logsource, const ULogEvent* event,
 	MyString eventError;
 	CheckEvents::check_event_result_t checkResult = CheckEvents::EVENT_OKAY;
 
-	if ( logsource == CONDORLOG ) {
-		checkResult = _checkCondorEvents.CheckAnEvent( event, eventError );
-	} else if ( logsource == DAPLOG ) {
-		checkResult = _checkStorkEvents.CheckAnEvent( event, eventError );
-	}
+	checkResult = _checkCondorEvents.CheckAnEvent( event, eventError );
 
 	if( checkResult == CheckEvents::EVENT_OKAY ) {
 		debug_printf( DEBUG_DEBUG_1, "Event is okay\n" );
@@ -3991,52 +3865,24 @@ Dag::SanityCheckSubmitEvent( const CondorID condorID, const Job* node )
 
 //---------------------------------------------------------------------------
 HashTable<int, Job *> *
-Dag::GetEventIDHash(bool isNoop, int jobType)
+Dag::GetEventIDHash(bool isNoop)
 {
 	if ( isNoop ) {
 		return &_noopIDHash;
 	}
 
-	switch (jobType) {
-	case Job::TYPE_CONDOR:
-		return &_condorIDHash;
-		break;
-
-	case Job::TYPE_STORK:
-		return &_storkIDHash;
-		break;
-
-	default:
-		EXCEPT( "Illegal job type (%d)", jobType );
-		break;
-	}
-
-	return NULL;
+	return &_condorIDHash;
 }
 
 //---------------------------------------------------------------------------
 const HashTable<int, Job *> *
-Dag::GetEventIDHash(bool isNoop, int jobType) const
+Dag::GetEventIDHash(bool isNoop) const
 {
 	if ( isNoop ) {
 		return &_noopIDHash;
 	}
 
-	switch (jobType) {
-	case Job::TYPE_CONDOR:
-		return &_condorIDHash;
-		break;
-
-	case Job::TYPE_STORK:
-		return &_storkIDHash;
-		break;
-
-	default:
-		EXCEPT( "Illegal job type (%d)", jobType );
-		break;
-	}
-
-	return NULL;
+	return &_condorIDHash;
 }
 
 // NOTE: dag addnode/removenode/adddep/removedep methods don't
@@ -4082,8 +3928,7 @@ Dag::SubmitNodeJob( const Dagman &dm, Job *node, CondorID &condorID )
 	if ( node->GetCluster() != _defaultCondorId._cluster ) {
 		ASSERT( JobIsNoop( condorID ) == node->GetNoop() );
 		int id = GetIndexID( node->GetID() );
-		int removeResult = GetEventIDHash( node->GetNoop(),
-					node->JobType() )->remove( id );
+		int removeResult = GetEventIDHash( node->GetNoop() )->remove( id );
 		ASSERT( removeResult == 0 );
 	}
 	node->SetCondorID( _defaultCondorId );
@@ -4099,7 +3944,7 @@ Dag::SubmitNodeJob( const Dagman &dm, Job *node, CondorID &condorID )
 		// Do condor_submit_dag -no_submit if this is a nested DAG node
 		// and lazy submit file generation is enabled (this must be
 		// done before we try to monitor the log file).
-   	if ( node->JobType() == Job::TYPE_CONDOR && !node->GetNoop() &&
+   	if ( !node->GetNoop() &&
 				node->GetDagFile() != NULL && _generateSubdagSubmits ) {
 		bool isRetry = node->GetRetries() > 0;
 		priority_swapper ps( node->_hasNodePriority, node->_nodePriority, _submitDagDeepOpts->priority);
@@ -4115,81 +3960,41 @@ Dag::SubmitNodeJob( const Dagman &dm, Job *node, CondorID &condorID )
 		}
 	}
 
-	if ( !node->MonitorLogFile( _condorLogRdr, _storkLogRdr, _nfsLogIsError,
+	if ( !node->MonitorLogFile( _condorLogRdr, _nfsLogIsError,
 			_recovery, _defaultNodeLog, _use_default_node_log ) ) {
 		debug_printf( DEBUG_QUIET, "ERROR: Failed to monitor log for node %s.\n",
 			node->GetJobName() );
 		return SUBMIT_RESULT_NO_SUBMIT;
 	}
 
-		// We now only check for missing log files for Stork jobs because
-		// of the default log file feature; that doesn't work for Stork
-		// jobs because we can't specify the log file on the command
-		// line.  wenger 2009-08-14
-	if ( !_allowLogError && node->JobType() == Job::TYPE_STORK &&
-				!node->CheckForLogFile( false ) ) {
-		debug_printf( DEBUG_NORMAL, "ERROR: No 'log =' value found in "
-					"submit file %s for node %s\n", node->GetCmdFile(),
-					node->GetJobName() );
-		node->TerminateFailure();
-		snprintf( node->error_text, JOB_ERROR_TEXT_MAXLEN,
-					"No 'log =' value found in submit file %s",
-					node->GetCmdFile() );
-	  	_numNodesFailed++;
-		_metrics->NodeFinished( node->GetDagFile() != NULL, false );
-		if ( _dagStatus == DAG_STATUS_OK ) {
-			_dagStatus = DAG_STATUS_NODE_FAILED;
-		}
-		result = SUBMIT_RESULT_NO_SUBMIT;
-
-	} else {
-		debug_printf( DEBUG_NORMAL, "Submitting %s Node %s job(s)...\n",
-				  	node->JobTypeString(), node->GetJobName() );
-
-		bool submit_success = false;
-
-    	if( node->JobType() == Job::TYPE_CONDOR ) {
-	  		node->_submitTries++;
-			if ( node->GetNoop() ) {
-      			submit_success = fake_condor_submit( condorID, 0,
-							node->GetJobName(), node->GetDirectory(),
-							node->GetLogFile() ,
-							node->GetLogFileIsXml() );
-			} else {
-				const char *logFile = node->UsingDefaultLog() ?
-							node->GetLogFile() : NULL;
-					// Note: assigning the ParentListString() return value
-					// to a variable here, instead of just passing it directly
-					// to condor_submit(), fixes a memory leak(!).
-					// wenger 2008-12-18
-				MyString parents = ParentListString( node );
-      			submit_success = condor_submit( dm, node->GetCmdFile(), condorID,
-							node->GetJobName(), parents,
-							node->varsFromDag, node->GetRetries(),
-							node->GetDirectory(), logFile,
-							ProhibitMultiJobs(),
-							node->NumChildren() > 0 && dm._claim_hold_time > 0 );
-			}
-    	} else if( node->JobType() == Job::TYPE_STORK ) {
-	  		node->_submitTries++;
-			if ( node->GetNoop() ) {
-      			submit_success = fake_condor_submit( condorID, 0,
-							node->GetJobName(), node->GetDirectory(),
-							node->GetLogFile(),
-							node->GetLogFileIsXml() );
+	debug_printf( DEBUG_NORMAL, "Submitting %s Node %s job(s)...\n",
+			  	node->JobTypeString(), node->GetJobName() );
 
-			} else {
-      			submit_success = stork_submit( dm, node->GetCmdFile(),
-						condorID, node->GetJobName(), node->GetDirectory() );
-			}
-    	} else {
-	    	debug_printf( DEBUG_QUIET, "Illegal job type: %d\n",
-						node->JobType() );
-			ASSERT(false);
-		}
+	bool submit_success = false;
 
-		result = submit_success ? SUBMIT_RESULT_OK : SUBMIT_RESULT_FAILED;
-	}
+ 	node->_submitTries++;
+	if ( node->GetNoop() ) {
+   		submit_success = fake_condor_submit( condorID, 0,
+					node->GetJobName(), node->GetDirectory(),
+					node->GetLogFile() ,
+					node->GetLogFileIsXml() );
+	} else {
+		const char *logFile = node->UsingDefaultLog() ?
+					node->GetLogFile() : NULL;
+			// Note: assigning the ParentListString() return value
+			// to a variable here, instead of just passing it directly
+			// to condor_submit(), fixes a memory leak(!).
+			// wenger 2008-12-18
+		MyString parents = ParentListString( node );
+   		submit_success = condor_submit( dm, node->GetCmdFile(), condorID,
+					node->GetJobName(), parents,
+					node->varsFromDag, node->GetRetries(),
+					node->GetDirectory(), logFile,
+					ProhibitMultiJobs(),
+					node->NumChildren() > 0 && dm._claim_hold_time > 0 );
+	}
+
+	result = submit_success ? SUBMIT_RESULT_OK : SUBMIT_RESULT_FAILED;
 
 	return result;
 }
@@ -4225,8 +4030,7 @@ Dag::ProcessSuccessfulSubmit( Job *node, const CondorID &condorID )
 	node->SetCondorID( condorID );
 	ASSERT( JobIsNoop( node->GetID() ) == node->GetNoop() );
 	int id = GetIndexID( node->GetID() );
-	int insertResult = GetEventIDHash( node->GetNoop(), node->JobType() )->
-				insert( id, node );
+	int insertResult = GetEventIDHash( node->GetNoop() )->insert( id, node );
 	ASSERT( insertResult == 0 );
 
 	debug_printf( DEBUG_VERBOSE, "\tassigned %s ID (%d.%d.%d)\n",
@@ -4249,7 +4053,7 @@ Dag::ProcessFailedSubmit( Job *node, int max_submit_attempts )
 	_nextSubmitTime = time(NULL) + thisSubmitDelay;
 	_nextSubmitDelay *= 2;
 
-	(void)node->UnmonitorLogFile( _condorLogRdr, _storkLogRdr );
+	(void)node->UnmonitorLogFile( _condorLogRdr );
 
 	if ( node->_submitTries >= max_submit_attempts ) {
 			// We're out of submit attempts, treat this as a submit failure.
diff --git a/src/condor_dagman/dag.h b/src/condor_dagman/dag.h
index 7dc4d5c..c91e4c6 100644
--- a/src/condor_dagman/dag.h
+++ b/src/condor_dagman/dag.h
@@ -38,12 +38,6 @@
 #include "dagman_recursive_submit.h"
 #include "jobstate_log.h"
 
-// NOTE: must be kept in sync with Job::job_type_t
-enum Log_source{
-  CONDORLOG = Job::TYPE_CONDOR,
-  DAPLOG = Job::TYPE_STORK
-};
-
 // Which layer of splices do we want to lift?
 enum SpliceLayer {
 	SELF,
@@ -115,7 +109,6 @@ class Dag {
 		@param retryNodeFirst whether, when a node fails and has retries,
 			   to put the node at the head of the ready queue
 		@param condorRmExe executable to remove Condor jobs
-		@param storkRmExe executable to remove Stork jobs
 		@param DAGManJobId Condor ID of this DAGMan process
 		@param prohibitMultiJobs whether submit files queueing multiple
 			   job procs are prohibited
@@ -139,7 +132,7 @@ class Dag {
 		 bool allowLogError,
 		 bool useDagDir, int maxIdleJobProcs, bool retrySubmitFirst,
 		 bool retryNodeFirst, const char *condorRmExe,
-		 const char *storkRmExe, const CondorID *DAGManJobId,
+		 const CondorID *DAGManJobId,
 		 bool prohibitMultiJobs, bool submitDepthFirst,
 		 const char *defaultNodeLog, bool generateSubdagSubmits,
 		 SubmitDagDeepOptions *submitDagDeepOpts,
@@ -191,16 +184,14 @@ class Dag {
 
     
     bool DetectCondorLogGrowth();
-    bool DetectDaPLogGrowth();            //<--DAP
 
     /** Force the Dag to process all new events in the condor log file.
         This may cause the state of some jobs to change.
 
-		@param logsource The type of log from which events should be read.
         @param recover Process Log in Recover Mode, from beginning to end
         @return true on success, false on failure
     */
-    bool ProcessLogEvents (int logsource, bool recovery = false); //<--DAP
+    bool ProcessLogEvents (bool recovery = false);
 
 	/** Process a single event.  Note that this is called every time
 			we attempt to read the user log, so we may or may not have
@@ -213,7 +204,7 @@ class Dag {
 			function).
 		@return True if the DAG should continue, false if we should abort.
 	*/
-	bool ProcessOneEvent (int logsource, ULogEventOutcome outcome, const ULogEvent *event,
+	bool ProcessOneEvent (ULogEventOutcome outcome, const ULogEvent *event,
 			bool recovery, bool &done);
 
 	/** Process an abort or executable error event.
@@ -298,12 +289,11 @@ class Dag {
     */
     Job * FindNodeByName (const char * jobName) const;
 
-    /** Get pointer to job with condor or stork ID condorID
-		@param logsource The type of log from which events should be read.
+    /** Get pointer to job with condor ID condorID
         @param condorID the CondorID of the job in the DAG
         @return address of Job object, or NULL if not found
     */
-    Job * FindNodeByEventID (int logsource, const CondorID condorID ) const;
+    Job * FindNodeByEventID ( const CondorID condorID ) const;
 
     /** Ask whether a node name exists in the DAG
         @param nodeName the name of the node in the DAG
@@ -639,9 +629,6 @@ class Dag {
 	// do not free this pointer
 	const char* CondorRmExe(void) { return _condorRmExe; }
 
-	// do not free this pointer
-	const char* StorkRmExe(void) { return _storkRmExe; }
-
 	const CondorID* DAGManJobId(void) { return _DAGManJobId; }
 
 	bool SubmitDepthFirst(void) { return _submitDepthFirst; }
@@ -830,7 +817,7 @@ class Dag {
 		SUBMIT_RESULT_NO_SUBMIT,
 	} submit_result_t;
 
-	/** Submit the Condor or Stork job for a node, including doing
+	/** Submit the Condor job for a node, including doing
 		some higher-level work such as sleeping before the actual submit
 		if necessary.
 		@param the appropriate Dagman object
@@ -841,15 +828,13 @@ class Dag {
 	submit_result_t SubmitNodeJob( const Dagman &dm, Job *node,
 				CondorID &condorID );
 
-	/** Do the post-processing of a successful submit of a Condor or
-		Stork job.
+	/** Do the post-processing of a successful submit of a Condor job.
 		@param the node for which the job was just submitted
 		@param the Condor ID of the associated job
 	*/	
 	void ProcessSuccessfulSubmit( Job *node, const CondorID &condorID );
 
-	/** Do the post-processing of a failed submit of a Condor or
-		Stork job.
+	/** Do the post-processing of a failed submit of a Condor job.
 		@param the node for which the job was just submitted
 		@param the maximum number of submit attempts allowed for a job.
 	*/
@@ -899,12 +884,12 @@ class Dag {
 	bool CheckForDagAbort(Job *job, const char *type);
 
 		// takes a userlog event and returns the corresponding node
-	Job* LogEventNodeLookup( int logsource, const ULogEvent* event,
+	Job* LogEventNodeLookup( const ULogEvent* event,
 				bool &submitEventIsSane );
 
 		// check whether a userlog event is sane, or "impossible"
 
-	bool EventSanityCheck( int logsource, const ULogEvent* event,
+	bool EventSanityCheck( const ULogEvent* event,
 						const Job* node, bool* result );
 
 		// compares a submit event's job ID with the one that appeared
@@ -913,24 +898,17 @@ class Dag {
 
 	bool SanityCheckSubmitEvent( const CondorID condorID, const Job* node );
 
-		/** Get the appropriate hash table for event ID->node mapping,
-			according to whether this is a Condor or Stork node.
+		/** Get the appropriate hash table for event ID->node mapping.
 			@param whether the node is a NOOP node
-			@param the node type/logsource (Condor or Stork) (see
-				Log_source and Job::job_type_t)
 			@return a pointer to the appropriate hash table
 		*/
-	HashTable<int, Job *> *		GetEventIDHash(bool isNoop, int jobType);
+	HashTable<int, Job *> *		GetEventIDHash(bool isNoop);
 
-		/** Get the appropriate hash table for event ID->node mapping,
-			according to whether this is a Condor or Stork node.
+		/** Get the appropriate hash table for event ID->node mapping.
 			@param whether the node is a NOOP node
-			@param the node type/logsource (Condor or Stork) (see
-				Log_source and Job::job_type_t)
 			@return a pointer to the appropriate hash table
 		*/
-	const HashTable<int, Job *> *		GetEventIDHash(bool isNoop,
-				int jobType) const;
+	const HashTable<int, Job *> *		GetEventIDHash(bool isNoop) const;
 
 	// run DAGs in directories from DAG file paths if true
 	bool _useDagDir;
@@ -938,20 +916,15 @@ class Dag {
     // Documentation on ReadUserLog is present in condor_utils
 	ReadMultipleUserLogs _condorLogRdr;
 
-		// Object to read events from Stork logs.
-	ReadMultipleUserLogs	_storkLogRdr;
-
 		/** Get the total number of node job user log files we'll be
 			accessing.
 			@return The total number of log files.
 		*/
-	int TotalLogFileCount() { return CondorLogFileCount() +
-				StorkLogFileCount(); }
+	int TotalLogFileCount() { return CondorLogFileCount(); }
+				
 
 	int CondorLogFileCount() { return _condorLogRdr.totalLogFileCount(); }
 
-	int StorkLogFileCount() { return _storkLogRdr.totalLogFileCount(); }
-
 		/** Write information for the given node to a rescue DAG.
 			@param fp: file pointer to the rescue DAG file
 			@param node: the node for which to write info
@@ -994,10 +967,6 @@ private:
 	// procs in the same cluster map to the same node).
 	HashTable<int, Job *>			_condorIDHash;
 
-	// Hash by StorkID (really just by the cluster ID because all
-	// procs in the same cluster map to the same node).
-	HashTable<int, Job *>			_storkIDHash;
-
 	// NOOP nodes are indexed by subprocID.
 	HashTable<int, Job *>			_noopIDHash;
 
@@ -1044,9 +1013,6 @@ private:
 		// Executable to remove Condor jobs.
 	const char *	_condorRmExe;
 
-		// Executable to remove Stork jobs.
-	const char *	_storkRmExe;
-
 		// Condor ID of this DAGMan process.
 	const CondorID *	_DAGManJobId;
 
@@ -1100,10 +1066,7 @@ private:
 		// Last time the status file was written.
 	time_t _lastStatusUpdateTimestamp;
 
-		// Separate event checkers for Condor and Stork here because
-		// IDs could collide.
 	CheckEvents	_checkCondorEvents;
-	CheckEvents	_checkStorkEvents;
 
 		// Total count of jobs deferred because of MaxJobs limit (note
 		// that a single job getting deferred multiple times is counted
diff --git a/src/condor_dagman/dagman_commands.cpp b/src/condor_dagman/dagman_commands.cpp
index 3e87782..d758aa4 100644
--- a/src/condor_dagman/dagman_commands.cpp
+++ b/src/condor_dagman/dagman_commands.cpp
@@ -60,7 +60,7 @@ ResumeDag(Dagman &dm)
 }
 
 bool
-AddNode( Dag *dag, Job::job_type_t type, const char *name,
+AddNode( Dag *dag, const char *name,
 		 const char* directory,
 		 const char* submitFile,
 		 const char *precmd, const char *postcmd, bool noop,
@@ -83,7 +83,7 @@ AddNode( Dag *dag, Job::job_type_t type, const char *name,
 		(void)check_warning_strictness( DAG_STRICT_1, false );
 		done = false;
 	}
-	Job* node = new Job( type, name, directory, submitFile );
+	Job* node = new Job( name, directory, submitFile );
 	if( !node ) {
 		dprintf( D_ALWAYS, "ERROR: out of memory!\n" );
 			// we already know we're out of memory, so filling in
diff --git a/src/condor_dagman/dagman_commands.h b/src/condor_dagman/dagman_commands.h
index bce5bd4..03077fa 100644
--- a/src/condor_dagman/dagman_commands.h
+++ b/src/condor_dagman/dagman_commands.h
@@ -31,7 +31,7 @@ bool PauseDag(Dagman &dm);
 // resume DAGMan's normal event-processing
 bool ResumeDag(Dagman &dm);
 
-bool AddNode( Dag *dag, Job::job_type_t type, const char *name,
+bool AddNode( Dag *dag, const char *name,
 			  const char* directory,
 			  const char* submitFile,
 			  const char *precmd, const char *postcmd, bool noop,
diff --git a/src/condor_dagman/dagman_main.cpp b/src/condor_dagman/dagman_main.cpp
index 846ee05..3749ce2 100644
--- a/src/condor_dagman/dagman_main.cpp
+++ b/src/condor_dagman/dagman_main.cpp
@@ -106,8 +106,6 @@ Dagman::Dagman() :
 	paused (false),
 	condorSubmitExe (NULL),
 	condorRmExe (NULL),
-	storkSubmitExe (NULL),
-	storkRmExe (NULL),
 	submit_delay (0),
 	max_submit_attempts (6),
 	max_submits_per_interval (5), // so Coverity is happy
@@ -358,18 +356,16 @@ Dagman::Config()
 		ASSERT( condorRmExe );
 	}
 
-	free( storkSubmitExe );
-	storkSubmitExe = param( "DAGMAN_STORK_SUBMIT_EXE" );
-	if( !storkSubmitExe ) {
-		storkSubmitExe = strdup( "stork_submit" );
-		ASSERT( storkSubmitExe );
+	if ( param_boolean( "DAGMAN_STORK_SUBMIT_EXE", false ) ) {
+		debug_printf( DEBUG_NORMAL, "Warning: DAGMAN_STORK_SUBMIT_EXE is "
+					"no longer supported\n" );
+		check_warning_strictness( DAG_STRICT_1 );
 	}
 
-	free( storkRmExe );
-	storkRmExe = param( "DAGMAN_STORK_RM_EXE" );
-	if( !storkRmExe ) {
-		storkRmExe = strdup( "stork_rm" );
-		ASSERT( storkRmExe );
+	if ( param_boolean( "DAGMAN_STORK_RM_EXE", false ) ) {
+		debug_printf( DEBUG_NORMAL, "Warning: DAGMAN_STORK_RM_EXE is "
+					"no longer supported\n" );
+		check_warning_strictness( DAG_STRICT_1 );
 	}
 
 	abortDuplicates = param_boolean( "DAGMAN_ABORT_DUPLICATES",
@@ -1021,7 +1017,7 @@ void main_init (int argc, char ** const argv) {
 						  dagman.allowLogError, dagman.useDagDir,
 						  dagman.maxIdle, dagman.retrySubmitFirst,
 						  dagman.retryNodeFirst, dagman.condorRmExe,
-						  dagman.storkRmExe, &dagman.DAGManJobId,
+						  &dagman.DAGManJobId,
 						  dagman.prohibitMultiJobs, dagman.submitDepthFirst,
 						  dagman._defaultNodeLog.Value(),
 						  dagman._generateSubdagSubmits,
@@ -1452,19 +1448,9 @@ void condor_event_timer () {
 
 	// If the log has grown
 	if( dagman.dag->DetectCondorLogGrowth() ) {
-		if( dagman.dag->ProcessLogEvents( CONDORLOG ) == false ) {
-			debug_printf( DEBUG_NORMAL,
-						"ProcessLogEvents(CONDORLOG) returned false\n" );
-			dagman.dag->PrintReadyQ( DEBUG_DEBUG_1 );
-			main_shutdown_rescue( EXIT_ERROR, Dag::DAG_STATUS_ERROR );
-			return;
-		}
-	}
-
-	if( dagman.dag->DetectDaPLogGrowth() ) {
-		if( dagman.dag->ProcessLogEvents( DAPLOG ) == false ) {
+		if( dagman.dag->ProcessLogEvents() == false ) {
 			debug_printf( DEBUG_NORMAL,
-						"ProcessLogEvents(DAPLOG) returned false\n" );
+						"ProcessLogEvents() returned false\n" );
 			dagman.dag->PrintReadyQ( DEBUG_DEBUG_1 );
 			main_shutdown_rescue( EXIT_ERROR, Dag::DAG_STATUS_ERROR );
 			return;
diff --git a/src/condor_dagman/dagman_main.h b/src/condor_dagman/dagman_main.h
index 0ce4732..722bd24 100644
--- a/src/condor_dagman/dagman_main.h
+++ b/src/condor_dagman/dagman_main.h
@@ -72,8 +72,6 @@ class Dagman {
 
 	char* condorSubmitExe;
 	char* condorRmExe;
-	char* storkSubmitExe;
-	char* storkRmExe;
 
 	// number of seconds to wait before consecutive calls to
 	// condor_submit (or dap_submit, etc.)
diff --git a/src/condor_dagman/dagman_submit.cpp b/src/condor_dagman/dagman_submit.cpp
index 860b4f7..5913fca 100644
--- a/src/condor_dagman/dagman_submit.cpp
+++ b/src/condor_dagman/dagman_submit.cpp
@@ -66,33 +66,8 @@ parse_condor_submit( const char *buffer, int &jobProcCount, int &cluster )
 }
 
 //-------------------------------------------------------------------------
-/** Parse output from stork_submit, determine the number of job procs
-    and the cluster.
-	@param buffer containing the line to be parsed
-	@param integer to be filled in with the number of job procs generated
-	       by the stork_submit
-	@param integer to be filled in with the cluster ID
-	@return true iff the line was correctly parsed
-*/
-static bool
-parse_stork_submit( const char *buffer, int &jobProcCount, int &cluster )
-{
-  // The initial space is to make the sscanf match zero or more leading
-  // whitespace that may exist in the buffer.
-  if ( 1 != sscanf( buffer, " Request assigned id: %d", &cluster) ) {
-	debug_printf( DEBUG_QUIET, "ERROR: parse_stork_submit failed:\n\t%s\n",
-				buffer );
-    return false;
-  }
-
-  jobProcCount = 1;
-  return true;
-}
-
-//-------------------------------------------------------------------------
 static bool
-submit_try( ArgList &args, CondorID &condorID, Job::job_type_t type,
-			bool prohibitMultiJobs )
+submit_try( ArgList &args, CondorID &condorID, bool prohibitMultiJobs )
 {
   MyString cmd; // for debug output
   args.GetArgsStringForDisplay( &cmd );
@@ -106,8 +81,8 @@ submit_try( ArgList &args, CondorID &condorID, Job::job_type_t type,
   }
   
   //----------------------------------------------------------------------
-  // Parse submit command output for a Condor/Stork job ID.  This
-  // desperately needs to be replaced by Condor/Stork submit APIs.
+  // Parse submit command output for a Condor job ID.  This
+  // desperately needs to be replaced by Condor submit APIs.
   //
   // Typical condor_submit output for Condor v6 looks like:
   //
@@ -124,30 +99,22 @@ submit_try( ArgList &args, CondorID &condorID, Job::job_type_t type,
   const char *marker = NULL;
   parse_submit_fnc parseFnc = NULL;
 
-  if ( type == Job::TYPE_CONDOR ) {
-    marker = " submitted to cluster ";
-
-	  // Note: we *could* check how many jobs got submitted here, and
-	  // correlate that with how many submit events we see later on.
-	  // I'm not worrying about that for now...  wenger 2006-02-07.
-	  // We also have to check the number of jobs to get an accurate
-	  // count of submitted jobs to report in the dagman.out file.
-
-	  // We should also check whether we got more than one cluster, and
-	  // either deal with it correctly or generate an error message.
-	parseFnc = parse_condor_submit;
-  } else if ( type == Job::TYPE_STORK ) {
-    marker = "assigned id";
-	parseFnc = parse_stork_submit;
-  } else {
-	debug_printf( DEBUG_QUIET, "Illegal job type: %d\n", type );
-	ASSERT(false);
-  }
+  marker = " submitted to cluster ";
+
+  // Note: we *could* check how many jobs got submitted here, and
+  // correlate that with how many submit events we see later on.
+  // I'm not worrying about that for now...  wenger 2006-02-07.
+  // We also have to check the number of jobs to get an accurate
+  // count of submitted jobs to report in the dagman.out file.
+
+  // We should also check whether we got more than one cluster, and
+  // either deal with it correctly or generate an error message.
+  parseFnc = parse_condor_submit;
   
-  // Take all of the output (both stdout and stderr) from condor_submit
-  // or stork_submit, and echo it to the dagman.out file.  Look for
-  // the line (if any) containing the word "cluster" (Condor) or
-  // "assigned id" (Stork).  If we don't find such a line, something
+  // Take all of the output (both stdout and stderr) from condor_submit,
+  // and echo it to the dagman.out file.  Look for
+  // the line (if any) containing the word "cluster" (Condor).
+  // If we don't find such a line, something
   // went wrong with the submit, so we return false.  The caller of this
   // function can retry the submit by repeatedly calling this function.
 
@@ -201,13 +168,6 @@ submit_try( ArgList &args, CondorID &condorID, Job::job_type_t type,
   	  return true;
   }
 
-  // Stork job specs have only 1 dimension.  The Stork user log forces the proc
-  // and sub-proc ids to "-1", so do the same here for the returned submit id.
-  if ( type == Job::TYPE_STORK ) {
-	  condorID._proc = -1;
-	  condorID._subproc = -1;
-  }
-
   	// Check for multiple job procs if configured to disallow that.
   if ( prohibitMultiJobs && (jobProcCount > 1) ) {
 	debug_printf( DEBUG_NORMAL, "Submit generated %d job procs; "
@@ -223,8 +183,7 @@ submit_try( ArgList &args, CondorID &condorID, Job::job_type_t type,
 // NOTE: this and submit_try should probably be merged into a single
 // method -- submit_batch_job or something like that.  wenger/pfc 2006-04-05.
 static bool
-do_submit( ArgList &args, CondorID &condorID, Job::job_type_t jobType,
-			bool prohibitMultiJobs )
+do_submit( ArgList &args, CondorID &condorID, bool prohibitMultiJobs )
 {
 	MyString cmd; // for debug output
 	args.GetArgsStringForDisplay( &cmd );
@@ -232,7 +191,7 @@ do_submit( ArgList &args, CondorID &condorID, Job::job_type_t jobType,
   
 	bool success = false;
 
-	success = submit_try( args, condorID, jobType, prohibitMultiJobs );
+	success = submit_try( args, condorID, prohibitMultiJobs );
 
 	if( !success ) {
 	    debug_printf( DEBUG_QUIET, "ERROR: submit attempt failed\n" );
@@ -435,8 +394,7 @@ condor_submit( const Dagman &dm, const char* cmdFile, CondorID& condorID,
 
 	args.AppendArg( cmdFile );
 
-	bool success = do_submit( args, condorID, Job::TYPE_CONDOR,
-				dm.prohibitMultiJobs );
+	bool success = do_submit( args, condorID, dm.prohibitMultiJobs );
 
 	if ( !tmpDir.Cd2MainDir( errMsg ) ) {
 		debug_printf( DEBUG_QUIET,
@@ -449,43 +407,6 @@ condor_submit( const Dagman &dm, const char* cmdFile, CondorID& condorID,
 }
 
 //-------------------------------------------------------------------------
-bool
-stork_submit( const Dagman &dm, const char* cmdFile, CondorID& condorID,
-			   const char* DAGNodeName, const char* directory )
-{
-	TmpDir		tmpDir;
-	MyString	errMsg;
-	if ( !tmpDir.Cd2TmpDir( directory, errMsg ) ) {
-		debug_printf( DEBUG_QUIET,
-				"Could not change to DAG directory %s: %s\n",
-				directory, errMsg.Value() );
-		return false;
-	}
-
-  ArgList args;
-
-  args.AppendArg( dm.storkSubmitExe );
-  args.AppendArg( "-lognotes" );
-
-  MyString logNotes = MyString( "DAG Node: " ) + DAGNodeName;
-  args.AppendArg( logNotes.Value() );
-
-  args.AppendArg( cmdFile );
-
-  bool success = do_submit( args, condorID, Job::TYPE_STORK,
-  			dm.prohibitMultiJobs );
-
-	if ( !tmpDir.Cd2MainDir( errMsg ) ) {
-		debug_printf( DEBUG_QUIET,
-				"Could not change to original directory: %s\n",
-				errMsg.Value() );
-		success = false;
-	}
-
-  return success;
-}
-
-//-------------------------------------------------------------------------
 // Subproc ID for "fake" events (for NOOP jobs).
 static int _subprocID = 0;
 
diff --git a/src/condor_dagman/job.cpp b/src/condor_dagman/job.cpp
index d2d8dec..b371307 100644
--- a/src/condor_dagman/job.cpp
+++ b/src/condor_dagman/job.cpp
@@ -58,14 +58,6 @@ const char * Job::status_t_names[] = {
 };
 
 //---------------------------------------------------------------------------
-// NOTE: must be kept in sync with the job_type_t enum
-const char* Job::_job_type_names[] = {
-    "Condor",
-    "Stork",
-    "No-Op",
-};
-
-//---------------------------------------------------------------------------
 Job::~Job() {
 	delete [] _directory;
 	delete [] _cmdFile;
@@ -92,9 +84,9 @@ Job::~Job() {
 }
 
 //---------------------------------------------------------------------------
-Job::Job( const job_type_t jobType, const char* jobName,
+Job::Job( const char* jobName,
 			const char *directory, const char* cmdFile ) :
-	_jobType( jobType ), _preskip( PRE_SKIP_INVALID ), _final( false )
+	_preskip( PRE_SKIP_INVALID ), _final( false )
 {
 	ASSERT( jobName != NULL );
 	ASSERT( cmdFile != NULL );
@@ -195,16 +187,6 @@ bool Job::Remove (const queue_t queue, const JobID_t jobID)
 }  
 
 //---------------------------------------------------------------------------
-bool
-Job::CheckForLogFile( bool usingDefault ) const
-{
-	bool tmpLogFileIsXml;
-	MyString logFile = MultiLogFiles::loadLogFileNameFromSubFile( _cmdFile,
-				_directory, tmpLogFileIsXml, usingDefault );
-	return (logFile != "");
-}
-
-//---------------------------------------------------------------------------
 void Job::Dump ( const Dag *dag ) const {
     dprintf( D_ALWAYS, "---------------------- Job ----------------------\n");
     dprintf( D_ALWAYS, "      Node Name: %s\n", _jobName );
@@ -670,29 +652,6 @@ Job::RemoveDependency( queue_t queue, JobID_t job, MyString &whynot )
 	return true;
 }
 
-
-Job::job_type_t
-Job::JobType() const
-{
-    return _jobType;
-}
-
-
-const char*
-Job::JobTypeString() const
-{
-    return _job_type_names[_jobType];
-}
-
-
-/*
-const char* Job::JobIdString() const
-{
-
-}
-*/
-
-
 int
 Job::NumParents() const
 {
@@ -778,7 +737,7 @@ Job::SetDagFile(const char *dagFile)
 //---------------------------------------------------------------------------
 bool
 Job::MonitorLogFile( ReadMultipleUserLogs &condorLogReader,
-			ReadMultipleUserLogs &storkLogReader, bool nfsIsError,
+			bool nfsIsError,
 			bool recovery, const char *defaultNodeLog, bool usingDefault )
 {
 	debug_printf( DEBUG_DEBUG_2,
@@ -791,24 +750,20 @@ Job::MonitorLogFile( ReadMultipleUserLogs &condorLogReader,
 		return true;
 	}
 
-	ReadMultipleUserLogs &logReader = (_jobType == TYPE_CONDOR) ?
-				condorLogReader : storkLogReader;
+	ReadMultipleUserLogs &logReader = condorLogReader;
 
 	MyString logFile;
 	if ( !FindLogFile( usingDefault, logFile ) ) {
 		LogMonitorFailed();
 		return false;
 	}
-	// Note:  logFile is "" here if usingDefault is true and this node
-	// is an HTCondor node (not Stork).
+	// Note:  logFile is "" here if usingDefault is true.
 
 		// Warn the user if the node's log file is in /tmp.
 	if ( logFile.find( "/tmp" ) == 0 ) {
 		debug_printf( DEBUG_QUIET, "Warning: "
 					"Log file %s for node %s is in /tmp\n",
 					logFile.Value(), GetJobName() );
-			// If we're using the workflow log, we'll only ever get here
-			// for Stork nodes, because they can't use the workflow log.
         check_warning_strictness( DAG_STRICT_1 );
 	}
 
@@ -854,8 +809,7 @@ Job::MonitorLogFile( ReadMultipleUserLogs &condorLogReader,
 
 //---------------------------------------------------------------------------
 bool
-Job::UnmonitorLogFile( ReadMultipleUserLogs &condorLogReader,
-			ReadMultipleUserLogs &storkLogReader )
+Job::UnmonitorLogFile( ReadMultipleUserLogs &condorLogReader )
 {
 	debug_printf( DEBUG_DEBUG_2, "Unmonitoring log file <%s> for node %s\n",
 				GetLogFile(), GetJobName() );
@@ -866,8 +820,7 @@ Job::UnmonitorLogFile( ReadMultipleUserLogs &condorLogReader,
 		return true;
 	}
 
-	ReadMultipleUserLogs &logReader = (_jobType == TYPE_CONDOR) ?
-				condorLogReader : storkLogReader;
+	ReadMultipleUserLogs &logReader = condorLogReader;
 
 	debug_printf( DEBUG_DEBUG_1, "Unmonitoring log file <%s> for node %s\n",
 				GetLogFile(), GetJobName() );
@@ -1118,48 +1071,23 @@ Job::Cleanup()
 bool
 Job::FindLogFile( bool usingWorkflowLog, MyString &logFile )
 {
-	if ( _jobType == TYPE_CONDOR ) {
-		if ( usingWorkflowLog ) {
-				// Now, if we're using the workflow log file, we don't
-				// even look at the node's submit file.  (See gittrac
-				// #3843.)
-			logFile = "";
-
-		} else {
-				// We're not in workflow/default log mode, so get the
-				// log file (if any) from the submit file.
-    		logFile = MultiLogFiles::loadLogFileNameFromSubFile(
-						_cmdFile, _directory, _logFileIsXml, false );
-			if ( logFile == "" ) {
-				debug_printf( DEBUG_NORMAL, "Unable to get log file from "
-							"submit file %s (node %s); using default/workflow log\n",
-							_cmdFile, GetJobName() );
-				// Don't return false here, because not specifying the
-				// log file is not an error.
-			}
-		}
+	if ( usingWorkflowLog ) {
+			// Now, if we're using the workflow log file, we don't
+			// even look at the node's submit file.  (See gittrac
+			// #3843.)
+		logFile = "";
 
 	} else {
-			// Workflow/default log file mode is not supported for Stork
-			// nodes, so we always have to get the log file for a Stork
-			// node.
-		StringList logFiles;
-		MyString tmpResult = MultiLogFiles::loadLogFileNamesFromStorkSubFile(
-					_cmdFile, _directory, logFiles );
-		if ( tmpResult != "" ) {
-			debug_printf( DEBUG_QUIET, "Error getting Stork log file: %s\n",
-						tmpResult.Value() );
-			return false;
-
-		} else if ( logFiles.number() != 1 ) {
-			debug_printf( DEBUG_QUIET, "Error: %d Stork log files found "
-						"in submit file %s; we want 1\n",
-						logFiles.number(), _cmdFile );
-			return false;
-
-		} else {
-			logFiles.rewind();
-			logFile = logFiles.next();
+			// We're not in workflow/default log mode, so get the
+			// log file (if any) from the submit file.
+   		logFile = MultiLogFiles::loadLogFileNameFromSubFile(
+					_cmdFile, _directory, _logFileIsXml, false );
+		if ( logFile == "" ) {
+			debug_printf( DEBUG_NORMAL, "Unable to get log file from "
+						"submit file %s (node %s); using default/workflow log\n",
+						_cmdFile, GetJobName() );
+			// Don't return false here, because not specifying the
+			// log file is not an error.
 		}
 	}
 
diff --git a/src/condor_dagman/job.h b/src/condor_dagman/job.h
index 4bbf9e5..7a7a866 100644
--- a/src/condor_dagman/job.h
+++ b/src/condor_dagman/job.h
@@ -75,14 +75,6 @@ typedef int JobID_t;
      queue) are put on the DAG's ready list.  */
 class Job {
   public:
-
-        // possible kinds of job (e.g., Condor, Stork, etc.)
-        // NOTE: must be kept in sync with _job_type_strings[]
-        // NOTE: must be kept in sync with enum Log_source
-	typedef enum {
-		TYPE_CONDOR,
-		TYPE_STORK,
-	 } job_type_t;
   
     /** Enumeration for specifying which queue for Add() and Remove().
         If you change this enum, you *must* also update queue_t_names
@@ -157,13 +149,12 @@ class Job {
 	static int NOOP_NODE_PROCID;
   
     /** Constructor
-        @param jobType Type of job in dag file.
         @param jobName Name of job in dag file.  String is deep copied.
 		@param directory Directory to run the node in, "" if current
 		       directory.  String is deep copied.
         @param cmdFile Path to condor cmd file.  String is deep copied.
     */
-    Job( const job_type_t jobType, const char* jobName,
+    Job( const char* jobName,
 				const char* directory, const char* cmdFile ); 
   
     ~Job();
@@ -177,8 +168,7 @@ class Job {
 	inline int GetRetries() const { return retries; }
 	const char* GetPreScriptName() const;
 	const char* GetPostScriptName() const;
-	const char* JobTypeString() const;
-	job_type_t JobType() const;
+	static const char* JobTypeString() { return "Condor"; }
 
 	bool AddPreScript( const char *cmd, MyString &whynot );
 	bool AddPreSkip( int exitCode, MyString &whynot );
@@ -221,14 +211,6 @@ class Job {
     */
     bool Remove (const queue_t queue, const JobID_t jobID);
 
-	/** Check whether the submit file for this job has a log file
-	    defined.
-		@param usingDefault is true if DAGman is watching the
-			default node log
-		@return true iff the submit file defines a log file
-	*/
-	bool CheckForLogFile( bool usingDefault ) const;
-
     /** Returns true if a queue is empty (has no jobs)
         @param queue Selects which queue to look at
         @return true: queue is empty, false: otherwise
@@ -267,6 +249,7 @@ class Job {
 			@return true: specified node is our child, false: otherwise
 		*/
 	bool HasChild( Job* child );
+
 		/** Is the specified node a parent of this node?
 			@param child Pointer to the node to check for parenthood.
 			@return true: specified node is our parent, false: otherwise
@@ -349,7 +332,7 @@ class Job {
 		return _dagFile;
 	}
 
-	/** Monitor this node's Condor or Stork log file with the
+	/** Monitor this node's Condor or file with the
 		multiple log reader.  (Must be called before this node's
 		job is submitted.)
 		@param logReader: the multiple log reader
@@ -361,17 +344,16 @@ class Job {
 		@return true if successful, false if failed
 	*/
 	bool MonitorLogFile( ReadMultipleUserLogs &condorLogReader,
-				ReadMultipleUserLogs &storkLogReader, bool nfsIsError,
+				bool nfsIsError,
 				bool recovery, const char *defaultNodeLog, bool usingDefault );
 
-	/** Unmonitor this node's Condor or Stork log file with the
+	/** Unmonitor this node's Condor log file with the
 		multiple log reader.  (Must be called after everything is done
 		for this node.)
 		@param logReader: the multiple log reader
 		@return true if successful, false if failed
 	*/
-	bool UnmonitorLogFile( ReadMultipleUserLogs &logReader,
-				ReadMultipleUserLogs &storkLogReader );
+	bool UnmonitorLogFile( ReadMultipleUserLogs &logReader );
 
 		// Whether this node is using the default node log file.
 	bool UsingDefaultLog() const { return _useDefaultLog; }
@@ -562,12 +544,6 @@ private:
 		// the log file.
   	void LogMonitorFailed();
 
-        // strings for job_type_t (e.g., "Condor, "Stork", etc.)
-    static const char* _job_type_names[];
-
-		// type of job (e.g., Condor, Stork, etc.)
-	job_type_t _jobType;
-
 		// Directory to cd to before running the job or the PRE and POST
 		// scripts.
 	char * _directory;
diff --git a/src/condor_dagman/parse.cpp b/src/condor_dagman/parse.cpp
index f77b1d7..7358a22 100644
--- a/src/condor_dagman/parse.cpp
+++ b/src/condor_dagman/parse.cpp
@@ -52,12 +52,12 @@ static bool _useDagDir = false;
 static int _thisDagNum = -1;
 static bool _mungeNames = true;
 
-static bool parse_subdag( Dag *dag, Job::job_type_t nodeType,
+static bool parse_subdag( Dag *dag,
 						const char* nodeTypeKeyword,
 						const char* dagFile, int lineNum,
 						const char *directory);
 
-static bool parse_node( Dag *dag, Job::job_type_t nodeType,
+static bool parse_node( Dag *dag,
 						const char* nodeTypeKeyword,
 						const char* dagFile, int lineNum,
 						const char *directory, const char *inlineOrExt,
@@ -221,7 +221,7 @@ bool parse (Dag *dag, const char *filename, bool useDagDir) {
 		//
 		if(strcasecmp(token, "JOB") == 0) {
 			parsed_line_successfully = parse_node( dag, 
-					   Job::TYPE_CONDOR, token,
+					   token,
 					   filename, lineNumber, tmpDirectory.Value(), "",
 					   "submitfile" );
 		}
@@ -230,35 +230,29 @@ bool parse (Dag *dag, const char *filename, bool useDagDir) {
 		// Example Syntax is:  DATA j1 j1.dapsubmit [DONE]
 		//
 		else if	(strcasecmp(token, "DAP") == 0) {	// DEPRECATED!
-			parsed_line_successfully = parse_node( dag,
-					   Job::TYPE_STORK, token,
-					   filename, lineNumber, tmpDirectory.Value(), "",
-					   "submitfile" );
 			debug_printf( DEBUG_QUIET, "%s (line %d): "
-				"Warning: the DAP token is deprecated and may be unsupported "
-				"in a future release.  Use the DATA token\n",
+				"ERROR: the DAP token is no longer supported\n",
 				filename, lineNumber );
-			check_warning_strictness( DAG_STRICT_2 );
+			parsed_line_successfully = false;
 		}
 
 		else if	(strcasecmp(token, "DATA") == 0) {
-			parsed_line_successfully = parse_node( dag,
-					   Job::TYPE_STORK, token,
-					   filename, lineNumber, tmpDirectory.Value(), "",
-					   "submitfile");
+			debug_printf( DEBUG_QUIET, "%s (line %d): "
+				"ERROR: the DATA token is no longer supported\n",
+				filename, lineNumber );
+			parsed_line_successfully = false;
 		}
 
 		// Handle a SUBDAG spec
 		else if	(strcasecmp(token, "SUBDAG") == 0) {
 			parsed_line_successfully = parse_subdag( dag, 
-						Job::TYPE_CONDOR,
 						token, filename, lineNumber, tmpDirectory.Value() );
 		}
 
 		// Handle a FINAL spec
 		else if(strcasecmp(token, "FINAL") == 0) {
 			parsed_line_successfully = parse_node( dag, 
-					   Job::TYPE_CONDOR, token,
+					   token,
 					   filename, lineNumber, tmpDirectory.Value(), "",
 					   "submitfile" );
 		}
@@ -423,7 +417,7 @@ bool parse (Dag *dag, const char *filename, bool useDagDir) {
 }
 
 static bool 
-parse_subdag( Dag *dag, Job::job_type_t nodeType,
+parse_subdag( Dag *dag, 
 			const char* nodeTypeKeyword,
 			const char* dagFile, int lineNum, const char *directory )
 {
@@ -434,7 +428,7 @@ parse_subdag( Dag *dag, Job::job_type_t nodeType,
 		return false;
 	}
 	if ( !strcasecmp( inlineOrExt, "EXTERNAL" ) ) {
-		return parse_node( dag, nodeType, nodeTypeKeyword, dagFile,
+		return parse_node( dag, nodeTypeKeyword, dagFile,
 					lineNum, directory, " EXTERNAL", "dagfile" );
 	}
 
@@ -444,7 +438,7 @@ parse_subdag( Dag *dag, Job::job_type_t nodeType,
 }
 
 static bool 
-parse_node( Dag *dag, Job::job_type_t nodeType,
+parse_node( Dag *dag, 
 			const char* nodeTypeKeyword,
 			const char* dagFile, int lineNum, const char *directory,
 			const char *inlineOrExt, const char *submitOrDagFile)
@@ -583,7 +577,7 @@ parse_node( Dag *dag, Job::job_type_t nodeType,
 
 	// looks ok, so add it
 	bool isFinal = strcasecmp( nodeTypeKeyword, "FINAL" ) == MATCH;
-	if( !AddNode( dag, nodeType, nodeName, directory,
+	if( !AddNode( dag, nodeName, directory,
 				submitFile, NULL, NULL, noop, done, isFinal, whynot ) )
 	{
 		debug_printf( DEBUG_QUIET, "ERROR: %s (line %d): %s\n",
@@ -1662,7 +1656,6 @@ parse_splice(
 							dag->RetrySubmitFirst(),
 							dag->RetryNodeFirst(),
 							dag->CondorRmExe(),
-							dag->StorkRmExe(),
 							dag->DAGManJobId(),
 							dag->ProhibitMultiJobs(),
 							dag->SubmitDepthFirst(),
diff --git a/src/condor_dagman/submit.h b/src/condor_dagman/submit.h
index df8b79a..d467701 100644
--- a/src/condor_dagman/submit.h
+++ b/src/condor_dagman/submit.h
@@ -58,9 +58,6 @@ bool condor_submit( const Dagman &dm, const char* cmdFile, CondorID& condorID,
 					const char* directory, const char *worflowLogFile,
 					bool prohibitMultiJobs, bool hold_claim );
 
-bool stork_submit( const Dagman &dm, const char* cmdFile, CondorID& condorID,
-				   const char* DAGNodeName, const char* directory );
-
 void set_fake_condorID( int subprocID );
 
 bool fake_condor_submit( CondorID& condorID, Job* job, const char* DAGNodeName,

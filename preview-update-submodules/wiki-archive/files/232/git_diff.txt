diff --git a/src/condor_examples/condor_config.generic b/src/condor_examples/condor_config.generic
index c928281..4cbd50a 100644
--- a/src/condor_examples/condor_config.generic
+++ b/src/condor_examples/condor_config.generic
@@ -2494,10 +2494,23 @@ KBDD_ADDRESS_FILE = $(LOG)/.kbdd_address
 HDFS_NAMENODE = hdfs://example.com:9000
 HDFS_NAMENODE_WEB = example.com:8000
 
+HDFS_BACKUPNODE = hdfs://example.com:50100
+HDFS_BACKUPNODE_WEB = example.com:50105
+
 ## You need to pick one machine as name node by setting this parameter
 ## to HDFS_NAMENODE. The remaining machines in a storage cluster will
 ## act as data nodes (HDFS_DATANODE).
-HDFS_SERVICES = HDFS_DATANODE
+HDFS_NODETYPE = HDFS_DATANODE
+
+## If machine is selected to be NameNode then by a role should defined.
+## If it selected to be DataNode then this paramer is ignored.
+## Available options:
+## ACTIVE: Active NameNode role (default value)
+## BACKUP: Always synchronized with the active NameNode state, thus 
+##         creating a backup of the namespace. Currently the NameNode
+##         supports one Backup node at a time.
+## CHECKPOINT: Periodically creates checkpoints of the namespace. 
+HDFS_NAMENODE_ROLE = ACTIVE
 
 ## The two set of directories that are required by HDFS are for name 
 ## node (HDFS_NAMENODE_DIR) and data node (HDFS_DATANODE_DIR). The 
@@ -2537,4 +2550,4 @@ HDFS_DATANODE_DIR = /scratch/tmp/hadoop_data
 #HDFS_DFSADMIN_CLASS=org.apache.hadoop.hdfs.tools.DFSAdmin
 
 ## In case an old name for hdfs configuration files is required.
-#HDFS_SITE_FILE = hadoop-site.xml
+#HDFS_SITE_FILE = hdfs-site.xml
diff --git a/src/condor_hdfs/hadoop.cpp b/src/condor_hdfs/hadoop.cpp
index 525abdb..9cb48ef 100644
--- a/src/condor_hdfs/hadoop.cpp
+++ b/src/condor_hdfs/hadoop.cpp
@@ -113,13 +113,6 @@ void Hadoop::initialize() {
                 free(dclass);
         }
 
-        m_secondaryNodeClass = "org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode";
-        char *snclass = param("HDFS_SECONDARYNODE_CLASS");
-        if (snclass != NULL) {
-                m_secondaryNodeClass = snclass;
-                free(snclass);
-        }
-
         m_dfsAdminClass = "org.apache.hadoop.hdfs.tools.DFSAdmin";
         char *dfsAdminclass = param("HDFS_DFSADMIN_CLASS");
         if (dfsAdminclass != NULL) {
@@ -162,7 +155,7 @@ void Hadoop::initialize() {
         m_hdfsAd.SetMyTypeName("hdfs");
         m_hdfsAd.SetTargetTypeName("");
         m_hdfsAd.Assign(ATTR_NAME, "hdfs");
-        m_hdfsAd.Assign("ServiceType", getServiceNameByType( m_serviceType) );
+        m_hdfsAd.Assign("ServiceType", getServiceNameByType( m_nodeType) );
         daemonCore->publish(&m_hdfsAd); 
 
         //Register a timer for periodically pushing classads.
@@ -269,6 +262,18 @@ void Hadoop::writeConfigFile() {
                 free(nnaddw);
         }
 
+        char *bnadd = param("HDFS_BACKUPNODE");
+	if (bnadd != NULL) {
+	        writeXMLParam("dfs.namenode.backup.address", bnadd, &xml);
+		free(bnadd);
+	}
+
+	char *bnaddw = param("HDFS_BACKUPNODE_WEB");
+	if (bnaddw != NULL) {
+	        writeXMLParam("dfs.namenode.backup.http-address", bnaddw, &xml);
+		free(bnaddw);
+	}
+
         char *rep = param("HDFS_REPLICATION");
         if (rep != NULL) {
                 writeXMLParam("dfs.replication", rep, &xml);
@@ -402,17 +407,28 @@ void Hadoop::killTimer() {
 }
 
 void Hadoop::startServices() {
-        char *services = param ("HDFS_SERVICES");
-
-        if (services == NULL) //by default run system as DataNode
-             m_serviceType = HADOOP_DATANODE;
-
-        else {
-            MyString s(services);
-            m_serviceType = getServiceTypeByName( s );
-        }
-
-        startService( m_serviceType );
+        char *nodeType = param ("HDFS_NODETYPE");
+
+        if ( nodeType == NULL ) //by default run system as DataNode
+             m_nodeType = HADOOP_DATANODE;
+
+        else {
+            MyString nt(nodeType);
+            m_nodeType = getServiceTypeByName( nt );
+
+	    if( m_nodeType == HADOOP_NAMENODE ){
+	        char *namenodeRole = param("HDFS_NAMENODE_ROLE");
+		MyString nnr( namenodeRole );
+		if( nnr =="BACKUP")
+		    m_namenodeRole = BACKUP;
+		else if (nnr == "CHECKPOINT")
+		    m_namenodeRole = CHECKPOINT;
+		else //default ACTIVE
+		    m_namenodeRole = ACTIVE;
+             }
+        }
+
+        startService( m_nodeType );
 }
 
 void Hadoop::startService( NodeType type ) {
@@ -442,13 +458,19 @@ void Hadoop::startService( NodeType type ) {
 
         if (type == HADOOP_NAMENODE) {
                 arglist.AppendArg(m_nameNodeClass);
+
+		switch(m_namenodeRole) {
+		  case CHECKPOINT:  arglist.AppendArg("-checkpoint");
+		       break;
+		  case BACKUP:  arglist.AppendArg("-backup");
+		       break;
+		  default: //case: ACTIVE
+		       arglist.AppendArg("-regular");
+		}
         }
         else if (type == HADOOP_DATANODE) {
                 arglist.AppendArg(m_dataNodeClass);
         }
-        else if (type == HADOOP_SECONDARY) {
-                arglist.AppendArg(m_secondaryNodeClass);
-        }
 
         int arrIO[3];
         arrIO[0] = -1;
@@ -665,7 +687,7 @@ void Hadoop::updateClassAd( MyString safemode, MyString stats) {
 
 void Hadoop::publishClassAd() {
 
-       if( m_serviceType == HADOOP_NAMENODE) { 
+       if( m_nodeType == HADOOP_NAMENODE) { 
            MyString mode  = runDFSAdmin("-safemode get");
            MyString stats = runDFSAdmin("-report");
 
@@ -798,37 +820,30 @@ end:
         return type;
 }
 
-NodeType Hadoop::getServiceTypeByName( MyString s )
-{
-	NodeType type;
-	if( s == "HDFS_NAMENODE" )
-     		type = HADOOP_NAMENODE;
-	
-	else if( s == "HDFS_SECONDARY" )
-		type = HADOOP_SECONDARY;
-	
-	else //by default run system as DataNode
-		type = HADOOP_DATANODE;
-	
-	return type;
-}
-
-MyString Hadoop::getServiceNameByType( NodeType type)
-{
-	MyString s;
-
-	switch (type){
-		case HADOOP_NAMENODE: 
-			s.sprintf("HADOOP_NAMENODE");
-			break;
-		
-		case HADOOP_SECONDARY:
-			s.sprintf("HADOOP_SECONDARY");
-			break;
-
-		default: //by default run system as DataNode
-			s.sprintf("HADOOP_DATANODE");
-	}
-
-	return s;
+NodeType Hadoop::getServiceTypeByName( MyString s )
+{
+	NodeType type;
+	if( s == "HDFS_NAMENODE" )
+     		type = HADOOP_NAMENODE;
+
+	else //by default run system as DataNode
+		type = HADOOP_DATANODE;
+
+	return type;
+}
+
+MyString Hadoop::getServiceNameByType( NodeType type )
+{
+	MyString s;
+
+	switch (type) {
+		case HADOOP_NAMENODE: 
+			s.sprintf("HADOOP_NAMENODE");
+			break;
+
+		default: //by default run system as DataNode
+			s.sprintf("HADOOP_DATANODE");
+	}
+
+	return s;
 }
diff --git a/src/condor_hdfs/hadoop.h b/src/condor_hdfs/hadoop.h
index 2e092cf..47d3c9c 100644
--- a/src/condor_hdfs/hadoop.h
+++ b/src/condor_hdfs/hadoop.h
@@ -24,8 +24,13 @@
 
 enum NodeType {
     HADOOP_NAMENODE,
-    HADOOP_DATANODE,
-    HADOOP_SECONDARY
+    HADOOP_DATANODE
+};
+
+enum NamenodeRole {
+    ACTIVE,
+    BACKUP,
+    CHECKPOINT
 };
 
 enum {
@@ -33,7 +38,6 @@ enum {
     STATE_RUNNING,
     STATE_REINIT,
     STATE_STOP_REQUESTED
-
 };
 
 enum {
@@ -84,7 +88,9 @@ class Hadoop : public Service {
 
         ClassAd m_hdfsAd;
 
-        NodeType m_serviceType;
+        NodeType m_nodeType;
+
+	NamenodeRole m_namenodeRole;
 
         //keeps tracks of std output and error of  hadoop process
         MyString m_line_stdout, m_line_stderr;
@@ -101,13 +107,8 @@ class Hadoop : public Service {
 
         MyString m_dataNodeClass;
 
-        MyString m_secondaryNodeClass;
-
         MyString m_dfsAdminClass;
 
-        //Name of hdfs's site configuration files differs among hadoop version
-        //Versions > 0.19 has hdfs-site.xml 
-        //versions < 0.19 has hadoop-site.xml
         MyString m_hdfsSiteFile;
 
         MyString m_coreSiteFile;
diff --git a/src/configure.ac b/src/configure.ac
index 17ebfab..f84f5ec 100644
--- a/src/configure.ac
+++ b/src/configure.ac
@@ -3214,7 +3214,7 @@ CHECK_EXTERNAL([curl],[7.19.6-p1],[soft])
 #Look for hadoop jars, required by HDFS daemon as well by hdfs file plugin
 # Only build hadoop external if hdfs is enabled
 AS_IF([test "x$enable_hdfs" = xyes], 
-  [CHECK_EXTERNAL([hadoop],[0.20.2],[soft])])
+  [CHECK_EXTERNAL([hadoop],[0.21.0],[soft])])
 
 if test "x$_cv_opsys" = "xLINUX" ; then
   CHECK_EXTERNAL([libxml2],[2.7.3],[soft],


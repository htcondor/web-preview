{"id": 4484, "title": "Ticket #4484: Significant blocking in Schedd::count_jobs", "description": "<blockquote>\nOn busy schedds (50k to 150k idle jobs), we noticed significant blocking in Schedd::count_jobs.\n\n<p>Looking through the code:\n</p><ol>\n<li>Do we have any profiling information available on where time is spent?\n</li><li>Would it be possible to walk jobs in parallel?\n</li><li>Can tasks in this function be split into separate callbacks?  This would allow us to decrease the entire time spent blocking.</li></ol>\n</blockquote>", "remarks": "<blockquote>\n<em>2014-Aug-09 12:30:45 by bbockelm:</em> <br/>\n\nI didn't really tackle the underlying issue (the time it takes to walk a queue of 100k jobs), but I did notice that there are three separate timeslices right now:\n<ul>\n<li>qmgmt's priorecarray (5%, not configurable).\n</li><li>Periodic expr (default 1%, configurable but undocumented).\n</li><li>count_jobs and friends (default 5%, configurable and documented).\n</li></ul>\n\n<p>The branch V8_3-gt4484 changes count_jobs and priorecarray to a 1% default and makes them both configurable with SCHEDD_INTERVAL_TIMESLICE.\n\n</p><p>The overall affect is that these periodic tasks will now take up to 3% of the schedd's time by default, down from 11%.  Since they can block for quite awhile (3s) each, I think this is more appropriate.\n\n</p><p></p><hr/>\n<em>2014-Aug-09 14:53:55 by sfiligoi:</em> <br/>\n\nInstead of throttling the housekeeping, could we do some things in parallel?\n\n<p>E.g. counting jobs and advertising to the Collector could easily be done from a forked process.\n\n</p><p>There is a reason we do these things, so I do not want to do them any less often than needed.\n\n</p><p></p><hr/>\n<em>2014-Aug-11 13:33:09 by tannenba:</em> <br/>\n\nReducing the amount of time we can do count_jobs and recompute the prio rec array can have other issues, as Igor says.  For instance, delaying the prio rec array timer could result in longer delays before changes in job priority take effect, i.e. jobs with worse job priority may be started ahead of jobs with better priority.\n\n<p>Some other brainstorm ideas:\n\n</p><p></p><ol>\n<li>Create an object in the schedd per owner that is kept throughput the lifetime of the existence of jobs in the queue from that owner.  This is something Miron has long wanted to store accounting information in the schedd per owner.  Assuming we do this, we could also keep a priority queue of job ids per owner.  This would remove the need to periodically count jobs and resort prio rec arrays - instead we would amortize this cost at job submit and removal time.   Of course, this is not a trivial change, and we may need to do something more clever than just a priority queue to enable insertion/removal of many thousands of jobs at once.\n\n<p></p></li><li>Use an openmp merge sort to do the prio rec array heavy lifting, instead of doing a qsort in a single thread.  This is less of a major change than the above, but also may not yield us very much...\n</li></ol>\n\n<p>More ideas to follow...\n\n</p><p></p><hr/>\n<em>2014-Aug-11 13:49:49 by bbockelm:</em> <br/>\n\nTo put it another way - what's the upper bound of the percentage of time the schedd should spend non-blocking?  My claim is that 11% is too high.\n\n<p>I have a hard time believing that, with a queue of 100k jobs, decreasing the frequency at which we update the .  The prio rec array is similar - how bad is it if changes in priority take a few minutes to propagate when we never change priorities and are running 10-20k jobs?  I'm not saying there's no effect of changing the default, just that it's out of balance.\n\n</p><p>With respect to the frequency - the timeslice provides an upper and lower bound for the frequency.  If decreasing the timeslice interval causes the frequency to go high for comfort, we should adjust the upper instead!\n\n</p><p>I think the question of \"how much time is spent doing this in the worst case\" should be separated from \"how quickly can we do this\".  I view the former question as more important.\n\n</p><p></p><hr/>\n<em>2014-Aug-11 14:38:37 by sfiligoi:</em> <br/>\n\nWhile I fully agree we should have good \"worst case\" emergency breaks, let's make sure we do not hit those limits on a regular basis on a busy system (like the one CMS is operating).\n\n<p></p><hr/>\n<em>2015-Feb-05 19:35:58 by bbockelm:</em> <br/>\n\nDeferring this for now.  With the fixes to the keepalive protocol in 8.3.2, the small pauses are far less critical.\n\n<p>We now can monitor this in the DC statistics; if it becomes a problem in the future, we'll re-open.</p></blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body></body></html>", "check_ins": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">2014-Aug-09 12:26</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"https://github.com/htcondor/htcondor/commit/e1db90572219e805bc73b33d283e1b1a630a63d1\">[40930]</a></span>: Change the default timeslice to 1% from 5%. <span class=\"ticket\"><a class=\"defer\" href=\"/wiki-archive/tickets/?ticket=4484\" onclick=\"get_ticket_and_populate_wrapper('4484'); return false;\" title=\"Significant blocking in Schedd::count_jobs\">#4484</a></span> Previously, the various timeslices in the schedd were at 1% (periodic) 5% (count_jobs and friends) and 5% (qmgmt). Now, everything defaults to 1% and is configurable.  (By Brian Bockelman )</td></tr>\n</tbody></table>", "type": "enhance", "last_change": "2015-Nov-12 14:58", "status": "defer", "created": "2014-Jul-23 20:30", "fixed_version": "2014-Jul-23 20:30", "broken_version": "", "priority": "3", "subsystem": "DaemonsSubmitNode", "assigned_to": "tannenba", "derived_from": "#4490", "creator": "jfrey", "rust": "", "customer_group": "cms", "visibility": "public", "notify": "sfiligoi@fnal.gov, bbockelm@cse.unl.edu", "due_date": ""}
From 266ab65969923034e140f1909f4209e0edc6669f Mon Sep 17 00:00:00 2001
From: Brian Bockelman <bbockelm@cse.unl.edu>
Date: Thu, 11 Aug 2011 22:18:02 -0500
Subject: [PATCH] Sketch out the multicast-router, which takes a single source job and creates a stream of destination jobs.

---
 src/condor_job_router/JobRouter.cpp        |  427 ++++++++++++++++++++++++++--
 src/condor_job_router/JobRouter.h          |   24 ++-
 src/condor_job_router/JobRouterHookMgr.cpp |    4 +-
 src/condor_job_router/PilotJob.h           |   86 ++++++
 src/condor_job_router/RoutedJob.h          |   12 +-
 5 files changed, 528 insertions(+), 25 deletions(-)
 create mode 100644 src/condor_job_router/PilotJob.h

diff --git a/src/condor_job_router/JobRouter.cpp b/src/condor_job_router/JobRouter.cpp
index 62fb7bf..e15e9e1 100644
--- a/src/condor_job_router/JobRouter.cpp
+++ b/src/condor_job_router/JobRouter.cpp
@@ -61,7 +61,7 @@ const char JR_ATTR_TARGET_UNIVERSE[] = "TargetUniverse";
 
 const int THROTTLE_UPDATE_INTERVAL = 600;
 
-JobRouter::JobRouter(Scheduler *scheduler): m_jobs(5000,hashFuncStdString,rejectDuplicateKeys) {
+JobRouter::JobRouter(Scheduler *scheduler): m_jobs(5000,hashFuncStdString,rejectDuplicateKeys), m_pilot_jobs(5000,hashFuncStdString,rejectDuplicateKeys) {
 	m_scheduler = scheduler;
 	m_release_on_hold = true;
 	m_job_router_polling_timer = -1;
@@ -422,7 +422,7 @@ JobRouter::EvalAllSrcJobPeriodicExprs()
 				continue;
 			}
 
-			job->SetSrcJobAd(job->src_key.c_str(), orig_ad, ad_collection);
+			job->SetSrcJobAd(job->src_key.c_str(), orig_ad);
 			if (false == push_dirty_attributes(job->src_ad,NULL,NULL))
 			{
 				dprintf(D_ALWAYS, "JobRouter failure (%s): "
@@ -725,7 +725,12 @@ JobRouter::AllocateRoutingTable() {
 
 void
 JobRouter::GracefullyRemoveJob(RoutedJob *job) {
-	job->state = RoutedJob::CLEANUP;
+	job->ChangeState(RoutedJob::CLEANUP);
+}
+
+void
+JobRouter::GracefullyRemoveJob(PilotJob *job) {
+	job->state = PilotJob::CLEANUP;
 }
 
 bool
@@ -733,6 +738,11 @@ JobRouter::AddJob(RoutedJob *job) {
 	return m_jobs.insert(job->src_key,job) == 0;
 }
 
+bool
+JobRouter::AddJob(PilotJob *job) {
+	return m_pilot_jobs.insert(job->src_key,job) == 0;
+}
+
 RoutedJob *
 JobRouter::LookupJobWithSrcKey(std::string const &src_key) {
 	RoutedJob *job = NULL;
@@ -742,6 +752,15 @@ JobRouter::LookupJobWithSrcKey(std::string const &src_key) {
 	return job;
 }
 
+PilotJob *
+JobRouter::LookupPilotJobWithSrcKey(std::string const &src_key) {
+	PilotJob *job = NULL;
+	if (m_pilot_jobs.lookup(src_key, job) == -1) {
+		return NULL;
+	}
+	return job;
+}
+
 RoutedJob *
 JobRouter::LookupJobWithKeys(std::string const &src_key,std::string const &dest_key) {
 	RoutedJob *job = LookupJobWithSrcKey(src_key);
@@ -762,6 +781,15 @@ JobRouter::RemoveJob(RoutedJob *job) {
 	return success != 0;
 }
 
+bool
+JobRouter::RemoveJob(PilotJob *job) {
+	int success;
+	ASSERT(job);
+	success = m_pilot_jobs.remove(job->src_key);
+	delete job;
+	return success != 0;
+}
+
 
 RoutedJob::RoutedJob() {
 	state = UNCLAIMED;
@@ -779,8 +807,29 @@ RoutedJob::RoutedJob() {
 	proxy_file_copy_chowned = false;
 	target_universe = CONDOR_UNIVERSE_GRID;
 	saw_dest_job = false;
+	pilot_job = NULL;
 }
 RoutedJob::~RoutedJob() {
+	if (pilot_job) {
+		pilot_job->RemoveRoutedJob(this);
+	}
+}
+
+PilotJob::PilotJob() : dest_jobs(500) {
+        state = UNCLAIMED;
+        src_proc_id.cluster = -1;
+        src_proc_id.proc = -1;
+        is_claimed = false;
+        is_done = false;
+        is_running = false;
+        is_success = false;
+        is_sandboxed = false;
+        submission_time = 0;
+        retirement_time = 0;
+        target_universe = CONDOR_UNIVERSE_GRID;
+        saw_dest_job = false;
+}
+PilotJob::~PilotJob() {
 }
 
 int
@@ -789,7 +838,7 @@ JobRouter::NumManagedJobs() {
 	RoutedJob *job = NULL;
 	m_jobs.startIterations();
 	while(m_jobs.iterate(job)) {
-		if(job->state != RoutedJob::RETIRED) count++;
+		if(job->GetState() != RoutedJob::RETIRED) count++;
 	}
 	return count;
 }
@@ -810,6 +859,16 @@ JobRouter::Poll() {
 	UpdateRouteStats();
 	GetCandidateJobs();
 
+	PilotJob  *pilot_job;
+	m_pilot_jobs.startIterations();
+	while(m_pilot_jobs.iterate(pilot_job)) {
+		TakeOverJob(pilot_job);
+		CheckPilotStatus(pilot_job);
+		FinalizePilot(pilot_job);
+		CleanupPilot(pilot_job);
+		CleanupRetiredPilot(pilot_job); // NOTE: this may delete pilot job pointer.
+	}
+
 	RoutedJob *job;
 	m_jobs.startIterations();
 	while(m_jobs.iterate(job)) {
@@ -855,6 +914,9 @@ CombineParentAndChildClassAd(classad::ClassAd *dest,classad::ClassAd *ad,classad
 
 void
 JobRouter::AdoptOrphans() {
+
+	// TODO: Adopt orphan pilot jobs.
+
     classad::LocalCollectionQuery query;
 	classad::ExprTree *constraint_tree;
 	classad::ClassAdParser parser;
@@ -923,13 +985,13 @@ JobRouter::AdoptOrphans() {
 
 		//If we get here, we have enough information to recover the routed job.
 		RoutedJob *job = new RoutedJob();
-		job->state = RoutedJob::SUBMITTED;
+		job->ChangeState(RoutedJob::SUBMITTED);
 		job->dest_key = dest_key;
 		job->dest_proc_id = getProcByString(dest_key.c_str());
 		job->route_name = route_name;
 		job->submission_time = time(NULL); //not true; but good enough
 		job->is_claimed = true;
-		if(!job->SetSrcJobAd(src_key.c_str(),src_ad,ad_collection)) {
+		if(!job->SetSrcJobAd(src_key.c_str(),src_ad)) {
 			dprintf(D_ALWAYS,"JobRouter failure (%s): error processing orphan src job ad\n",job->JobDesc().c_str());
 			delete job;
 			continue;
@@ -1017,6 +1079,45 @@ JobRouter::AdoptOrphans() {
 	} while (query.Next(src_key));
 }
 
+bool
+JobRouter::ConsiderAdForPilot(std::string& key, classad::ClassAd* src_ad, JobRoute* route) {
+
+	int requested_running = 0;
+	if( !src_ad->EvaluateAttrInt( "RequestedRunning", requested_running ) ) {
+		return false;
+	}
+	if (requested_running <= 0) {
+		return false;
+	}
+
+	int requested_min_queued;
+        if( !src_ad->EvaluateAttrInt( "RequestedMinQueued", requested_min_queued) ) {
+                requested_min_queued = 0;
+        }
+        if (requested_min_queued <= 0) {
+		// TODO: Emit error message!
+                return false;
+        }
+
+	PilotJob *job = new PilotJob();
+	job->SetDestJobAd(src_ad);
+	job->target = requested_running;
+	job->minq = requested_min_queued;
+	job->state = PilotJob::UNCLAIMED;
+	job->target_universe = route->TargetUniverse();
+	job->grid_resource = route->GridResource();
+	job->route_name = route->Name();
+	if(!job->SetSrcJobAd(key.c_str(),src_ad)) {
+		delete job;
+		return false;
+	}
+
+	dprintf(D_FULLDEBUG,"JobRouter (%s): found candidate pilot job\n",job->JobDesc().c_str());
+	AddJob(job);
+
+	return true;
+}
+
 void
 JobRouter::GetCandidateJobs() {
 	if(!m_enable_job_routing) return;
@@ -1149,7 +1250,7 @@ JobRouter::GetCandidateJobs() {
 			return; //router is full
 		}
 
-		if(LookupJobWithSrcKey(key)) {
+		if(LookupJobWithSrcKey(key) || LookupPilotJobWithSrcKey(key)) {
 			// We are already managing this job.
 			continue;
 		}
@@ -1168,13 +1269,18 @@ JobRouter::GetCandidateJobs() {
 			continue;
 		}
 
+		if (ConsiderAdForPilot(key, ad, route)) {
+			// We'll make this into a pilot job
+			continue;
+		}
+
 		RoutedJob *job = new RoutedJob();
-		job->state = RoutedJob::UNCLAIMED;
+		job->ChangeState(RoutedJob::UNCLAIMED);
 		job->target_universe = route->TargetUniverse();
 		job->grid_resource = route->GridResource();
 		job->route_name = route->Name();
 
-		if(!job->SetSrcJobAd(key.c_str(),ad,ad_collection)) {
+		if(!job->SetSrcJobAd(key.c_str(),ad)) {
 			delete job;
 			continue;
 		}
@@ -1264,7 +1370,7 @@ JobRouter::DaemonIdentityString() {
 
 void
 JobRouter::TakeOverJob(RoutedJob *job) {
-	if(job->state != RoutedJob::UNCLAIMED) return;
+	if(job->GetState() != RoutedJob::UNCLAIMED) return;
 
 	MyString error_details;
 	ClaimJobResult cjr = claim_job(job->src_ad,NULL,NULL,job->src_proc_id.cluster, job->src_proc_id.proc, &error_details, JobRouterName().c_str());
@@ -1282,7 +1388,7 @@ JobRouter::TakeOverJob(RoutedJob *job) {
 	}
 	case CJR_OK: {
 		dprintf(D_FULLDEBUG,"JobRouter (%s): claimed job\n",job->JobDesc().c_str());
-		job->state = RoutedJob::CLAIMED;
+		job->ChangeState(RoutedJob::CLAIMED);
 		job->is_claimed = true;
 		break;
 	}
@@ -1290,8 +1396,36 @@ JobRouter::TakeOverJob(RoutedJob *job) {
 }
 
 void
+JobRouter::TakeOverJob(PilotJob *job) {
+	if(job->state != PilotJob::UNCLAIMED) return;
+
+	MyString error_details;
+	ClaimJobResult cjr = claim_job(job->src_ad,NULL,NULL,job->src_proc_id.cluster, job->src_proc_id.proc, &error_details, JobRouterName().c_str());
+
+	switch(cjr) {
+		case CJR_ERROR: {
+			dprintf(D_ALWAYS,"JobRouter failure (%s): candidate job could not be claimed by JobRouter: %s\n",job->JobDesc().c_str(),error_details.Value());
+			GracefullyRemoveJob(job);
+			break;
+		}
+		case CJR_BUSY: {
+			dprintf(D_FULLDEBUG,"JobRouter failure (%s): candidate job could not be claimed by JobRouter because it is already claimed by somebody else.\n",job->JobDesc().c_str());
+			GracefullyRemoveJob(job);
+			break;
+		}
+		case CJR_OK: {
+			dprintf(D_FULLDEBUG,"JobRouter (%s): claimed job\n",job->JobDesc().c_str());
+			job->state = PilotJob::CLAIMED;
+			job->is_claimed = true;
+			break;
+		}
+	}
+}
+
+
+void
 JobRouter::SubmitJob(RoutedJob *job) {
-	if(job->state != RoutedJob::CLAIMED) return;
+	if(job->GetState() != RoutedJob::CLAIMED) return;
 
 #if HAVE_JOB_HOOKS
 	if (NULL != m_hook_mgr)
@@ -1394,7 +1528,7 @@ JobRouter::FinishSubmitJob(RoutedJob *job) {
 	// to update the src job now.  Just wait for the next update.
 	job->src_ad.InsertAttr(JR_ATTR_ROUTED_TO_JOB_ID,job->dest_key.c_str());
 
-	job->state = RoutedJob::SUBMITTED;
+	job->ChangeState(RoutedJob::SUBMITTED);
 	job->submission_time = time(NULL);
 }
 
@@ -1571,7 +1705,7 @@ JobRouter::UpdateRoutedJobStatus(RoutedJob *job, classad::ClassAd update) {
 
 void
 JobRouter::CheckSubmittedJobStatus(RoutedJob *job) {
-	if(job->state != RoutedJob::SUBMITTED) return;
+	if(job->GetState() != RoutedJob::SUBMITTED) return;
 
 #if HAVE_JOB_HOOKS
 	if (NULL != m_hook_mgr)
@@ -1711,14 +1845,86 @@ JobRouter::FinishCheckSubmittedJobStatus(RoutedJob *job) {
 	if(job_status == COMPLETED && job_finished != 0) {
 		dprintf(D_FULLDEBUG, "JobRouter (%s): found target job finished\n",job->JobDesc().c_str());
 
-		job->state = RoutedJob::FINISHED;
+		job->ChangeState(RoutedJob::FINISHED);
 	}
 }
 
+void
+JobRouter::CheckPilotStatus(PilotJob *job) {
+	classad::ClassAdCollection *ad_collection = m_scheduler->GetClassAds();
+	classad::ClassAd *src_ad = ad_collection->GetClassAd(job->src_key);
+	std::string keyword;
+	std::string copy_attr_param;
+	char* custom_attrs = NULL;
+
+	if(!src_ad) {
+		dprintf(D_ALWAYS,"JobRouter (%s): failed to find src ad in job collection mirror.\n",job->JobDesc().c_str());
+		GracefullyRemoveJob(job);
+		return;
+	}
+
+	int job_status = 0;
+	if( !src_ad->EvaluateAttrInt( ATTR_JOB_STATUS, job_status ) ) {
+		dprintf(D_ALWAYS, "JobRouter failure (%s): cannot evaluate JobStatus in src job\n",job->JobDesc().c_str());
+		GracefullyRemoveJob(job);
+		return;
+	}
+
+	if(job_status == REMOVED) {
+		dprintf(D_FULLDEBUG, "JobRouter (%s): found src job marked for removal\n",job->JobDesc().c_str());
+		WriteAbortEventToUserLog( *src_ad );
+		GracefullyRemoveJob(job);
+		return;
+	}
+
+	if(job_status == HELD && !hold_copied_from_target_job(*src_ad) ) {
+		dprintf(D_FULLDEBUG, "JobRouter (%s): found src job on hold\n",job->JobDesc().c_str());
+		GracefullyRemoveJob(job);
+		return;
+	}
+
+	// Check to see if we want jobs
+	// TODO: This really belongs to the PilotJob class
+	unsigned int running = 0, queued = 0;
+	for (std::list<RoutedJob *>::iterator it = job->dest_jobs.begin(); it != job->dest_jobs.end(); it++) {
+		RoutedJob* tmp_job = (*it);
+		if (tmp_job->is_running) {
+			running += 1;
+		} else if (tmp_job->GetState() == RoutedJob::SUBMITTED) {
+			// TODO: not running does NOT imply pending!  Could be held...
+			queued += 1;
+		}
+	}
+	if ((running >= job->target) || (queued >= job->minq)) {
+		return;
+	}
+
+	// We want to submit (job->minq-queued) RoutedJobs
+	while (queued < job->minq) {
+
+		RoutedJob *rjob = new RoutedJob();
+		rjob->ChangeState(RoutedJob::UNCLAIMED);
+		rjob->target_universe = job->target_universe;
+		rjob->grid_resource = job->grid_resource;
+		rjob->route_name = job->route_name;
+
+		if(!rjob->SetSrcJobAd(job->src_key.c_str(),&job->dest_ad)) {
+			delete rjob;
+			continue;
+		}
+		rjob->is_sandboxed = TestJobSandboxed(rjob);
+		rjob->pilot_job = job;
+
+		dprintf(D_FULLDEBUG,"JobRouter (%s): create new job from pilot\n",rjob->JobDesc().c_str());
+		AddJob(rjob);
+
+		queued++;
+	}
+}
 
 void
 JobRouter::FinalizeJob(RoutedJob *job) {
-	if(job->state != RoutedJob::FINISHED) return;
+	if(job->GetState() != RoutedJob::FINISHED) return;
 
 #if HAVE_JOB_HOOKS
 	if (NULL != m_hook_mgr)
@@ -1800,6 +2006,33 @@ JobRouter::FinishFinalizeJob(RoutedJob *job) {
 	GracefullyRemoveJob(job);
 }
 
+void
+JobRouter::FinalizePilot(PilotJob *job) {
+	/* TODO: What does a finalized pilot job mean?
+	if(!finalize_job(job->dest_ad,job->dest_proc_id.cluster,job->dest_proc_id.proc,NULL,NULL,job->is_sandboxed)) {
+		dprintf(D_ALWAYS,"JobRouter failure (%s): failed to finalize job\n",job->JobDesc().c_str());
+
+			// Put the src job back in idle state to prevent it from
+			// exiting the queue.
+		SetJobIdle(job);
+	}
+	else if(!WriteTerminateEventToUserLog(job->src_ad)) {
+	}
+	else {
+		EmailTerminateEvent(job->src_ad);
+
+		dprintf(D_ALWAYS,"JobRouter (%s): finalized pilot job\n",job->JobDesc().c_str());
+		job->is_done = true;
+
+		job->is_success = TestJobSuccess(job);
+		if(!job->is_success) {
+			dprintf(D_ALWAYS,"Job Router (%s): %s is true, so job will count as a failure\n",job->JobDesc().c_str(),JR_ATTR_JOB_FAILURE_TEST);
+		}
+	}
+	*/
+	GracefullyRemoveJob(job);
+}
+
 bool
 JobRouter::TestJobSuccess(RoutedJob *job)
 {
@@ -1941,7 +2174,7 @@ JobRoute::EvalSharedX509UserProxy(RoutedJob *job,std::string &proxy_file)
 
 void
 JobRouter::CleanupJob(RoutedJob *job) {
-	if(job->state != RoutedJob::CLEANUP) return;
+	if(job->GetState() != RoutedJob::CLEANUP) return;
 
 #if HAVE_JOB_HOOKS
 	if (NULL != m_hook_mgr)
@@ -2026,7 +2259,7 @@ JobRouter::FinishCleanupJob(RoutedJob *job) {
 		// Now, we need to leave this job in the list for a while to
 		// prevent lag in the job collection mirror from causing us to
 		// think this job is an orphan.
-		job->state = RoutedJob::RETIRED;
+		job->ChangeState(RoutedJob::RETIRED);
 		job->retirement_time = time(NULL);
 		job->route_name = "";
 	}
@@ -2034,7 +2267,7 @@ JobRouter::FinishCleanupJob(RoutedJob *job) {
 
 void
 JobRouter::CleanupRetiredJob(RoutedJob *job) {
-	if(job->state != RoutedJob::RETIRED) return;
+	if(job->GetState() != RoutedJob::RETIRED) return;
 
 	// Our job here is to check if the jobs that are hanging around in
 	// retirement state are safe to forget about.  We don't want to
@@ -2110,6 +2343,111 @@ JobRouter::CleanupRetiredJob(RoutedJob *job) {
 }
 
 void
+JobRouter::CleanupPilot(PilotJob *job) {
+	classad::ClassAdCollection *ad_collection = m_scheduler->GetClassAds();
+
+	for (std::list<RoutedJob*>::iterator it = job->dest_jobs.begin(); it != job->dest_jobs.end(); it++) {
+		(*it)->ChangeState(RoutedJob::CLEANUP);
+	}
+
+	if(job->is_claimed) {
+		MyString error_details;
+		bool keep_trying = true;
+		if(!yield_job(job->src_ad,NULL,NULL,job->is_done,job->src_proc_id.cluster,job->src_proc_id.proc,&error_details,JobRouterName().c_str(),m_release_on_hold,&keep_trying))
+		{
+			dprintf(D_ALWAYS,"JobRouter (%s): failed to yield job: %s\n",
+					job->JobDesc().c_str(),
+					error_details.Value());
+
+			classad::ClassAd *src_ad = ad_collection->GetClassAd(job->src_key);
+			if(!src_ad) {
+				// The src job has gone away, so do not keep trying.
+				keep_trying = false;
+			}
+			if(keep_trying) {
+				return;
+			}
+		}
+		else {
+			dprintf(D_FULLDEBUG,"JobRouter (%s): yielded job (done=%d)\n",job->JobDesc().c_str(),job->is_done);
+		}
+
+		job->is_claimed = false;
+	}
+
+	if(!job->is_claimed) {
+		dprintf(D_FULLDEBUG,"JobRouter (%s): Cleaned up and removed routed job.\n",job->JobDesc().c_str());
+
+		JobRoute *route = GetRouteByName(job->route_name.c_str());
+		if(route) {
+			if(job->is_success) {
+				// TODO: Should we have separate counters for pilots?  I think so...
+				route->IncrementSuccesses();
+			}
+			else {
+				route->IncrementFailures();
+			}
+		}
+
+		// Now, we need to leave this job in the list for a while to
+		// prevent lag in the job collection mirror from causing us to
+		// think this job is an orphan.
+		job->state = PilotJob::RETIRED;
+		job->retirement_time = time(NULL);
+		job->route_name = "";
+	}
+}
+
+void
+JobRouter::CleanupRetiredPilot(PilotJob *job) {
+	if(job->state != PilotJob::RETIRED) return;
+
+	// Our job here is to check if the jobs that are hanging around in
+	// retirement state are safe to forget about.  We don't want to
+	// forget about them until our mirrror of the originating schedd's
+	// job collection is in sync.  Otherwise, the jobs may be
+	// misidentified as orphans, belonging to this JobRouter, but not
+	// being actively managed by this JobRouter.  We don't want to
+	// hold them in retirement for an unnecessarily long time, in case
+	// the first attempt to route them failed and we want to try
+	// again.
+
+	bool src_job_synchronized = false;
+	bool dest_job_synchronized = false;
+
+	classad::ClassAdCollection *ad_collection = m_scheduler->GetClassAds();
+	classad::ClassAd *src_ad = ad_collection->GetClassAd(job->src_key);
+
+	// If src_ad cannot be found in the mirror, then the ad has probably
+	// been deleted, and we could just count that as being in sync.
+	// However, there is no penalty to keeping the job waiting around in
+	// retirement in this case, because without src_ad, we can't possibly
+	// try to route this job again or anything like that.  Therefore,
+	// play it safe and only count the mirror as synchronized if we
+	// can find src_ad and directly observe that it is not managed by us.
+
+	if(src_ad) {
+		std::string managed;
+		std::string manager;
+		src_ad->EvaluateAttrString(ATTR_JOB_MANAGED,managed);
+		src_ad->EvaluateAttrString(ATTR_JOB_MANAGED_MANAGER,manager);
+
+		if(managed != MANAGED_EXTERNAL || manager != m_job_router_name) {
+			// Our mirror of the schedd's job collection shows this
+			// job as not being managed by us.  Good.
+			src_job_synchronized = true;
+		}
+	}
+
+	// If there are remaining destination jobs, not all the launched jobs have been synchronized.
+	if(src_job_synchronized && dest_job_synchronized && (job->dest_jobs.size() == 0)) {
+		dprintf(D_FULLDEBUG,"JobRouter (%s): job mirror synchronized; removing job from internal 'retirement' status\n",job->JobDesc().c_str());
+		RemoveJob(job);
+		return;
+	}
+}
+
+void
 JobRouter::TimerHandler_UpdateCollector() {
 	daemonCore->sendUpdates(UPDATE_AD_GENERIC, &m_public_ad);
 }
@@ -2425,8 +2763,28 @@ RoutedJob::JobDesc() {
 	return desc;
 }
 
+std::string
+PilotJob::JobDesc() {
+	std::string desc;
+	if(!src_key.empty()) {
+		desc += "src=";
+		desc += src_key;
+	}
+/*	TODO: pretty print all the routed jobs
+	if(!dest_key.empty()) {
+		desc += ",dest=";
+		desc += dest_key;
+	}
+*/
+	if(!route_name.empty()) {
+		desc += ",route=";
+		desc += route_name;
+	}
+	return desc;
+}
+
 bool
-RoutedJob::SetSrcJobAd(char const *key,classad::ClassAd *ad,classad::ClassAdCollection * /*ad_collection*/ ) {
+RoutedJob::SetSrcJobAd(char const *key,classad::ClassAd *ad) {
 
 	this->src_key = key;
 	this->src_proc_id = getProcByString(key);
@@ -2452,3 +2810,32 @@ RoutedJob::SetDestJobAd(classad::ClassAd const *ad) {
 	ASSERT(dest_ad.CopyFromChain(*ad));
 	saw_dest_job = true;
 }
+
+bool
+PilotJob::SetSrcJobAd(char const *key,classad::ClassAd *ad) {
+
+	this->src_key = key;
+	this->src_proc_id = getProcByString(key);
+
+	//Set the src_ad to include all attributes from cluster plus proc ads.
+	if(!src_ad.CopyFromChain(*ad)) {
+		dprintf(D_FULLDEBUG,"JobRouter failure (%s): failed to combine cluster and proc ad.\n",JobDesc().c_str());
+		return false;
+	}
+	// From here on, keep track of any changes to src_ad, so we can push
+	// changes back to the schedd.
+	src_ad.ClearAllDirtyFlags();
+	src_ad.EnableDirtyTracking();
+	return true;
+}
+
+void
+PilotJob::SetDestJobAd(classad::ClassAd const *ad) {
+	// We do not want to just do dest_ad = *ad, among other reasons,
+	// because this copies the pointer to the chained parent, which
+	// could get deleted before we are done with this ad.
+
+	ASSERT(dest_ad.CopyFromChain(*ad));
+	saw_dest_job = true;
+}
+
diff --git a/src/condor_job_router/JobRouter.h b/src/condor_job_router/JobRouter.h
index ed54d1e..8087765 100644
--- a/src/condor_job_router/JobRouter.h
+++ b/src/condor_job_router/JobRouter.h
@@ -24,6 +24,7 @@
 #include "condor_daemon_core.h"
 #include "HashTable.h"
 #include "RoutedJob.h"
+#include "PilotJob.h"
 
 #include "classad/classad_distribution.h"
 
@@ -31,6 +32,7 @@
 #include "JobRouterHookMgr.h"
 #endif /* HAVE_JOB_HOOKS */
 
+class PilotJob;
 class RoutedJob;
 class Scheduler;
 class JobRouterHookMgr;
@@ -52,15 +54,20 @@ class JobRouter: public Service {
 	// Add a new job to be managed by JobRouter.
 	// Takes ownership of job, so caller should not delete it.
 	bool AddJob(RoutedJob *job);
+	bool AddJob(PilotJob *job);
 
 	// Mark a job entry for removal, but do not remove it immediately.
 	// The job will be removed after cleaning up any necessary things
 	// in the source schedd.
 	void GracefullyRemoveJob(RoutedJob *job);
+	void GracefullyRemoveJob(PilotJob  *job);
 
 	// Return a pointer to entry for job with given src key in routed job list
 	RoutedJob *LookupJobWithSrcKey(std::string const &key);
 
+	// Return a pointer to entry for job with given src key in routed pilot job list
+	PilotJob *LookupPilotJobWithSrcKey(std::string const &key);
+
 	// Return a pointer to entry for job with given keys in routed job list
 	RoutedJob *LookupJobWithKeys(std::string const &src_key,std::string const &dest_key);
 
@@ -109,7 +116,8 @@ class JobRouter: public Service {
 	Scheduler *GetScheduler() { return m_scheduler; }
 
  private:
-	HashTable<std::string,RoutedJob *> m_jobs;  //key="src job id"
+	HashTable<std::string,RoutedJob *> m_jobs;        //key="src job id"
+	HashTable<std::string,PilotJob  *> m_pilot_jobs;  //key="src job id"
 	RoutingTable *m_routes; //key="route name"
 
 	Scheduler *m_scheduler;        // provides us with a mirror of the real schedd's job collection
@@ -150,29 +158,42 @@ class JobRouter: public Service {
 	// because this function does not clean up the state of this job in
 	// the source schedd.
 	bool RemoveJob(RoutedJob *job);
+	bool RemoveJob(PilotJob *job);
 
 	// Find jobs to route.  Function calls AddJob() on each one.
 	void GetCandidateJobs();
 
+	// Consider whether this should be a pilot job.
+	// Return true if we turn it into a pilot.
+	bool ConsiderAdForPilot(std::string&, classad::ClassAd*, JobRoute*);
+
 	// Resume management of any jobs we were routing in a previous life.
 	void AdoptOrphans();
 
 	// Take ownership of jobs (e.g. by marking them as "managed" in src schedd)
 	void TakeOverJob(RoutedJob *job);
 
+	// Take ownership of pilot job
+	void TakeOverJob(PilotJob *job);
+
 	// Submit jobs to target resources/sites.
 	void SubmitJob(RoutedJob *job);
 
 	// Check if submitted jobs are finished.
 	void CheckSubmittedJobStatus(RoutedJob *job);
 
+	// Check if allocations need new jobs.
+	void CheckPilotStatus(PilotJob *job);
+
 	// Remove finished "destination" jobs and feed back results into
 	// the source jobs.
 	void FinalizeJob(RoutedJob *job);
+	void FinalizePilot(PilotJob *job);
 
 	// Handle any jobs that are on the way out of JobRouter.
 	// Clean up any state in the source schedd etc.
 	void CleanupJob(RoutedJob *job);
+	void CleanupPilot(PilotJob *job);
 
 	// Set the source job back to the idle state.
 	void SetJobIdle(RoutedJob *job);
@@ -180,6 +201,7 @@ class JobRouter: public Service {
 	// Sweep away memory of jobs that finished, once it is
 	// safe to do so.
 	void CleanupRetiredJob(RoutedJob *job);
+	void CleanupRetiredPilot(PilotJob *job);
 
 	bool EvalSrcJobPeriodicExpr(RoutedJob* job);
 
diff --git a/src/condor_job_router/JobRouterHookMgr.cpp b/src/condor_job_router/JobRouterHookMgr.cpp
index 2e3eadd..302dc58 100644
--- a/src/condor_job_router/JobRouterHookMgr.cpp
+++ b/src/condor_job_router/JobRouterHookMgr.cpp
@@ -817,7 +817,7 @@ ExitClient::hookExited(int exit_status) {
 						" Failed to update source job "
 						"status.  Job status NOT "
 						"updated.\n", m_routed_job->JobDesc().c_str());
-				m_routed_job->SetSrcJobAd(m_routed_job->src_key.c_str(), orig_ad, ad_collection);
+				m_routed_job->SetSrcJobAd(m_routed_job->src_key.c_str(), orig_ad);
 				job_router->RerouteJob(m_routed_job);
 				return;
 			}
@@ -827,7 +827,7 @@ ExitClient::hookExited(int exit_status) {
 						"Failed to update src job in "
 						"job queue.  Job status NOT "
 						"updated.\n", m_routed_job->JobDesc().c_str());
-				m_routed_job->SetSrcJobAd(m_routed_job->src_key.c_str(), orig_ad, ad_collection);
+				m_routed_job->SetSrcJobAd(m_routed_job->src_key.c_str(), orig_ad);
 				job_router->RerouteJob(m_routed_job);
 				return;
 			}
diff --git a/src/condor_job_router/PilotJob.h b/src/condor_job_router/PilotJob.h
new file mode 100644
index 0000000..8f306a2
--- /dev/null
+++ b/src/condor_job_router/PilotJob.h
@@ -0,0 +1,86 @@
+/***************************************************************
+ *
+ * Copyright (C) 1990-2011, Condor Team, Computer Sciences Department,
+ * University of Wisconsin-Madison, WI.
+ * 
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License.  You may
+ * obtain a copy of the License at
+ * 
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ ***************************************************************/
+
+#ifndef _PILOT_JOB_H
+#define _PILOT_JOB_H
+
+#include "classad/classad_distribution.h"
+#include "RoutedJob.h"
+#include <list>
+
+class PilotJob {
+ public:
+	PilotJob(); virtual ~PilotJob();
+
+	std::string src_key;  // job id of original job
+	PROC_ID src_proc_id;
+	std::list<RoutedJob *> dest_jobs;
+
+	// A pilot has a source and destination ad; the source is
+	// the ad of the original pilot.  The destination is originally
+	// a copy of the source, but we might decide to alter it somehow
+	// prior to making a copy for the RoutedJob.
+	classad::ClassAd src_ad;
+	classad::ClassAd dest_ad;
+
+	unsigned int minq;     // Minimum number of queued jobs
+	unsigned int target;   // Running jobs target
+
+	enum ManagementStateEnum {
+		UNCLAIMED,     // job is not yet officially ours to manage
+		CLAIMED,       // we are in control of this job now
+		RAMPUP,        // we would like more pilot jobs to be run
+		STEADY,        // no additional jobs are needed.
+		FINISHED,      // grid job is done, need to feed back results
+		CLEANUP,       // unclaim, etc.
+		RETIRED        // hangs around in this state to prevent incorrect
+		               // identification of orphans
+	};
+
+	ManagementStateEnum state;
+
+	bool is_claimed;  // true if we have ever gotten into the CLAIMED state
+	                  // and therefore need to unclaim the job eventually.
+
+	bool is_done;     // true if src job should be marked finished
+	bool is_running;  // true if job status is RAMPUP or STEADY
+	bool is_success;  // true if job finished successfully
+
+	bool is_sandboxed;// true if dest copy of job has a separate sandbox
+
+	bool saw_dest_job;// true if dest job ad showed up in job mirror
+
+	time_t submission_time;
+	time_t retirement_time;
+
+	int target_universe;
+	std::string grid_resource; // In ATTR_GRID_RESOURCE format
+	std::string route_name;
+
+	// return a description of the job keys, useful for debug trace messages
+	// Format: src=X,dest=X
+	std::string JobDesc();
+
+	bool SetSrcJobAd(char const *key,classad::ClassAd *ad);
+	void SetDestJobAd(classad::ClassAd const *ad);
+	void RemoveRoutedJob(RoutedJob * job) {dest_jobs.remove(job);}
+};
+
+#endif // _PILOT_JOB_H
+
diff --git a/src/condor_job_router/RoutedJob.h b/src/condor_job_router/RoutedJob.h
index 3ea007d..33b6ce8 100644
--- a/src/condor_job_router/RoutedJob.h
+++ b/src/condor_job_router/RoutedJob.h
@@ -24,6 +24,7 @@
 
 #include "classad/classad_distribution.h"
 
+class PilotJob;
 class JobRoute;
 
 class RoutedJob {
@@ -49,7 +50,7 @@ class RoutedJob {
 		               // identification of orphans
 	};
 
-	ManagementStateEnum state;
+	PilotJob* pilot_job;   // Associated pilot job, if any.
 
 	bool is_claimed;  // true if we have ever gotten into the CLAIMED state
 	                  // and therefore need to unclaim the job eventually.
@@ -73,7 +74,7 @@ class RoutedJob {
 	// Format: src=X,dest=X
 	std::string JobDesc();
 
-	bool SetSrcJobAd(char const *key,classad::ClassAd *ad,classad::ClassAdCollection *ad_collection);
+	bool SetSrcJobAd(char const *key,classad::ClassAd *ad);
 	void SetDestJobAd(classad::ClassAd const *ad);
 	bool IsRunning() {return is_running;}
 	bool SawDestJob() {return saw_dest_job;}
@@ -81,6 +82,13 @@ class RoutedJob {
 	bool PrepareSharedX509UserProxy(JobRoute *route);
 	bool CleanupSharedX509UserProxy(JobRoute *route);
 
+	// TODO: When state changes, inform connected pilot job.
+	inline void ChangeState(ManagementStateEnum new_state) {state = new_state;}
+	inline ManagementStateEnum GetState() {return state;}
+
+private:
+	ManagementStateEnum state;
+
 };
 
 
-- 
1.7.3.1


{"id": 7239, "title": "Ticket #7239: support for more usable selection+binding of GPU devices by user", "description": "<blockquote>\nUsers want to be able to require a specific GPU model. For Nvidia GPUs, Currently that information is published in the CUDADeviceName attribute for single-GPU machines, however for multi-GPU machines it's published in CUDA1DeviceName, CUDA2DeviceName, CUDA3DeviceName, etc. So users must cobble together something like:\n\n<p></p><div class=\"code\">\n<pre class=\"code\">Requirements = (CUDADeviceName == \"Tesla K10.G2.8GB\" || CUDA1DeviceName == \"Tesla K10.G2.8GB\" || CUDA2DeviceName == \"Tesla K10.G2.8GB\" || CUDA3DeviceName == \"Tesla K10.G2.8GB\")\n</pre></div>\n\n\n<p>Condor should support a better attribute scheme that enables users to select specific GPU models without all this...\n\n</p><p><span class=\"subsubsection\"></span></p><h4>workaround - multiple p-slots</h4>\nIf Multiple p-slots are configured, such that each p-slot contains only one type of GPU, then the job's requirements can be use to select the slot with the GPU that is needed without a lot of complexity in the job.  This could be something like\n\n<p>Assume that the machine has 4 GPUs like this\n\n</p><p></p><div class=\"snip\">\n<pre class=\"sniplabel\">partial output of condor_gpu_discovery</pre>\n<pre class=\"snip\">DetectedGPUs=\"CUDA0, CUDA1, CUDA2, CUDA3\"\nCUDA0DeviceName = \"Tesla K10.G2.8GB\"\nCUDA1DeviceName = \"Tesla K10.G2.8GB\"\nCUDA2DeviceName = \"TITAN RTX\"\nCUDA3DeviceName = \"TITAN RTX\"\n</pre></div>\n\n\n<p>The 4 gpus can be split into 2 p-slots, each with 2 gpus like this\n\n</p><p></p><div class=\"snip\">\n<pre class=\"sniplabel\">configuration fragment</pre>\n<pre class=\"snip\"># configure SLOT_TYPE_1 with 70% of resources and the first 2 gpus\nuse Feature:PartitionableSlot(1,70%, Gpus=2)\n\n# configure SLOT_TYPE_2 with 30% of resources and the second 2 gpus\nuse Feature:PartitionableSlot(2,30%, Gpus=2)\n\n# advertise CUDADeviceName in each p-slot using the name of the first gpu in that slot\nSTARTD_ATTRS = $(STARTD_ATTRS) CUDADeviceName\nSLOT_TYPE_1_CUDADeviceName = CUDA0DeviceName\nSLOT_TYPE_2_CUDADeviceName = CUDA2DeviceName\n</pre></div>\n\n\n<p>Since GPUs are assigned to slots in order, we know that the first 2 GPUs, which are both <code>DeviceName</code> \"Tesla K10.G2.8GB\" will be assign to the first p-slot.\n\n</p><p>If the output of condor_gpu_discovery indicates that the devices are interleaved (CUDA0 and CUDA2 have the same device name) then a wrapper script around condor_gpu_discovery is needed that re-arranges the order of devices in <code>DetectedGPUS</code>.  This wrapper script also also need to oversubscribe GPUs, so LIGO very likely already has a wrapper script.\n\n</p><p>This wrapper can be made simpler and more robust by making use of stable GPU ids, which are introduced in 8.9.8, see <span class=\"ticket\"><a class=\"resolved\" href=\"/wiki-archive/tickets/?ticket=7696\" onclick=\"get_ticket_and_populate_wrapper('7696'); return false;\" title=\"GPU discovery should report stable GPU ids\">#7696</a></span></p></blockquote>", "remarks": "<blockquote>\n<em>2019-Sep-05 05:23:40 by pfc:</em> <br/>\n\nThe sibling issue on the LIGO side is here: <a class=\"external\" href=\"https://github.com/lscsoft/osg-lvc/issues/92\">https://github.com/lscsoft/osg-lvc/issues/92</a>\n\n<p>Promoting to priority 2 b/c Uncle Greg thinks it's clear and doable.\n\n</p><p></p><hr/>\n<em>2019-Sep-17 14:08:03 by johnkn:</em> <br/>\n\nHe needs to explain how this is doable, because I think it isn't without a major redesign of the way negotiation works.\n\n<p></p><hr/>\n<em>2019-Dec-03 10:18:11 by pfc:</em> <br/>\n\nI just understood that the workaround above DOESN'T work, because when you land on a matching machine with heterogenous GPUs, you may not be assigned the GPU you required!\n\n<p></p><hr/>\n<em>2020-Mar-03 12:04:15 by johnkn:</em> <br/>\n\nThat is correct,  the d-slot will be assigned the next available GPU, which is not necessarily one that matches the constraint expression.\n\n<p></p><hr/>\n<em>2020-Mar-24 10:23:11 by pfc:</em> <br/>\n\nWe all agreed that Condor can/will solve this problem given that LIGO will agree to partition heterogenous GPUs on one machine across different pslots.\n\n<p></p><hr/>\n<em>2020-Jun-23 10:55:00 by anderson:</em> <br/>\n\nmultiple p-slots is only a short-term solution as it requires statically allocating memory and CPUs to those p-slots. Long-term a single p-slot solution is needed.\n\n<p></p><hr/>\n<em>2020-Dec-15 09:57:15 by gthain:</em> <br/>\n\nNote:  This is now HTCondor-127: <a class=\"external\" href=\"https://opensciencegrid.atlassian.net/browse/HTCONDOR-127\">https://opensciencegrid.atlassian.net/browse/HTCONDOR-127</a></blockquote>", "derived_tickets": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"resolved\" href=\"/wiki-archive/tickets/?ticket=7535\" onclick=\"get_ticket_and_populate_wrapper('7535'); return false;\" title=\"startd should allow slots to be assigned specific custom resources\">#7535</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nstartd should allow slots to be assigned specific custom resources</td></tr>\n</tbody></table>", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "enhance", "last_change": "2020-Dec-15 10:26", "status": "abandoned", "created": "2019-Sep-04 14:39", "fixed_version": "2019-Sep-04 14:39", "broken_version": "v080800", "priority": "2", "subsystem": "", "assigned_to": "johnkn", "derived_from": "", "creator": "gthain", "rust": "", "customer_group": "ligo", "visibility": "public", "notify": "", "due_date": ""}
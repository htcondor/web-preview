diff --git a/src/condor_hdfs/hadoop.cpp b/src/condor_hdfs/hadoop.cpp
index 9cb48ef..7f71e7a 100644
--- a/src/condor_hdfs/hadoop.cpp
+++ b/src/condor_hdfs/hadoop.cpp
@@ -96,7 +96,7 @@ void Hadoop::initialize() {
         buff.sprintf("%s/", m_hadoopHome.Value());
         m_classpath.insert(buff.Value());
 
-        buff.sprintf("%s/lib", m_hadoopHome.Value());
+        buff.sprintf("%s", m_hadoopHome.Value());
         recurrBuildClasspath(buff.Value());
 
         m_nameNodeClass  = "org.apache.hadoop.hdfs.server.namenode.NameNode";
@@ -335,8 +335,8 @@ void Hadoop::writeConfigFile() {
         }
 
         //TODO these shouldn't be hard-coded
-        writeXMLParam("dfs.namenode.plugins", "edu.wisc.cs.condor.NameNodeAds", &xml);
-        writeXMLParam("dfs.datanode.plugins", "edu.wisc.cs.condor.DataNodeAds", &xml);
+        //writeXMLParam("dfs.namenode.plugins", "edu.wisc.cs.condor.NameNodeAds", &xml);
+        //writeXMLParam("dfs.datanode.plugins", "edu.wisc.cs.condor.DataNodeAds", &xml);
 
         xml.append("</configuration>");
 
@@ -410,13 +410,13 @@ void Hadoop::startServices() {
         char *nodeType = param ("HDFS_NODETYPE");
 
         if ( nodeType == NULL ) //by default run system as DataNode
-             m_nodeType = HADOOP_DATANODE;
+             m_nodeType = HDFS_DATANODE;
 
         else {
             MyString nt(nodeType);
             m_nodeType = getServiceTypeByName( nt );
 
-	    if( m_nodeType == HADOOP_NAMENODE ){
+	    if( m_nodeType == HDFS_NAMENODE ){
 	        char *namenodeRole = param("HDFS_NAMENODE_ROLE");
 		MyString nnr( namenodeRole );
 		if( nnr =="BACKUP")
@@ -432,7 +432,7 @@ void Hadoop::startServices() {
 }
 
 void Hadoop::startService( NodeType type ) {
-        dprintf(D_ALWAYS, "Starting hadoop node service type = %s\n",
+        dprintf(D_ALWAYS, "Starting hdfs node service type = %s\n",
                 getServiceNameByType(type).Value() );
 
         ArgList arglist;
@@ -456,19 +456,19 @@ void Hadoop::startService( NodeType type ) {
                 free(ldir);
         }
 
-        if (type == HADOOP_NAMENODE) {
-                arglist.AppendArg(m_nameNodeClass);
+        if (type == HDFS_NAMENODE) {
+            arglist.AppendArg(m_nameNodeClass);
 
-		switch(m_namenodeRole) {
-		  case CHECKPOINT:  arglist.AppendArg("-checkpoint");
-		       break;
-		  case BACKUP:  arglist.AppendArg("-backup");
-		       break;
-		  default: //case: ACTIVE
-		       arglist.AppendArg("-regular");
-		}
+		    switch(m_namenodeRole) {
+		        case CHECKPOINT:  arglist.AppendArg("-checkpoint");
+		           break;
+		        case BACKUP:  arglist.AppendArg("-backup");
+		           break;
+		       default: //case: ACTIVE
+		           arglist.AppendArg("-regular");
+		    }
         }
-        else if (type == HADOOP_DATANODE) {
+        else if (type == HDFS_DATANODE) {
                 arglist.AppendArg(m_dataNodeClass);
         }
 
@@ -505,7 +505,7 @@ void Hadoop::startService( NodeType type ) {
 
         Directory dir(m_nameNodeDir.Value());
 
-        if (type == HADOOP_NAMENODE) {
+        if (type == HDFS_NAMENODE) {
                 arglist.RemoveArg(0);
                 arglist.InsertArg(m_java.Value(), 0);
 
@@ -641,8 +641,10 @@ void Hadoop::recurrBuildClasspath(const char *path) {
                 if (match && strlen(match) == 4) {
                         m_classpath.insert(dir.GetFullPath());
                 } else if (dir.IsDirectory()) {
-                        //current file is a subdirectory.
-                        recurrBuildClasspath(dir.GetFullPath());                    
+                	    const char *tmp = strstr(ctmp, "lib");
+                	    if (tmp && strlen(tmp) == 3) {
+                	    	recurrBuildClasspath(dir.GetFullPath());
+                	    }
                 }
         }
 }
@@ -687,7 +689,7 @@ void Hadoop::updateClassAd( MyString safemode, MyString stats) {
 
 void Hadoop::publishClassAd() {
 
-       if( m_nodeType == HADOOP_NAMENODE) { 
+       if( m_nodeType == HDFS_NAMENODE) {
            MyString mode  = runDFSAdmin("-safemode get");
            MyString stats = runDFSAdmin("-report");
 
@@ -824,10 +826,10 @@ NodeType Hadoop::getServiceTypeByName( MyString s )
 {
 	NodeType type;
 	if( s == "HDFS_NAMENODE" )
-     		type = HADOOP_NAMENODE;
+     		type = HDFS_NAMENODE;
 
 	else //by default run system as DataNode
-		type = HADOOP_DATANODE;
+		type = HDFS_DATANODE;
 
 	return type;
 }
@@ -837,12 +839,12 @@ MyString Hadoop::getServiceNameByType( NodeType type )
 	MyString s;
 
 	switch (type) {
-		case HADOOP_NAMENODE: 
-			s.sprintf("HADOOP_NAMENODE");
+		case HDFS_NAMENODE:
+			s.sprintf("HDFS_NAMENODE");
 			break;
 
 		default: //by default run system as DataNode
-			s.sprintf("HADOOP_DATANODE");
+			s.sprintf("HDFS_DATANODE");
 	}
 
 	return s;
diff --git a/src/condor_hdfs/hadoop.h b/src/condor_hdfs/hadoop.h
index 47d3c9c..a156431 100644
--- a/src/condor_hdfs/hadoop.h
+++ b/src/condor_hdfs/hadoop.h
@@ -23,8 +23,8 @@
 #include "condor_daemon_core.h"
 
 enum NodeType {
-    HADOOP_NAMENODE,
-    HADOOP_DATANODE
+    HDFS_NAMENODE,
+    HDFS_DATANODE
 };
 
 enum NamenodeRole {
@@ -90,7 +90,7 @@ class Hadoop : public Service {
 
         NodeType m_nodeType;
 
-	NamenodeRole m_namenodeRole;
+        NamenodeRole m_namenodeRole;
 
         //keeps tracks of std output and error of  hadoop process
         MyString m_line_stdout, m_line_stderr;

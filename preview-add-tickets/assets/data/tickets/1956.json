{"id": 1956, "title": "Ticket #1956: Move stduniv to new starter/shadow", "description": "<blockquote>\nWhat would it take to move stduniv to the new starter/shadow where\nWantRemoteIO would always be false?\n\n<p></p><div class=\"verbatim\">\n<pre>\nRemote system calls:\n    The lifeline socket from the job to the stduniv shadow should\n    instead be from the job to the new starter. All communication\n    by the stduniv job does should be funneled through to the new\n    starter. It needs a little more work to figure out how this socket\n    lives in relation to daemoncore.\n\n    The starter would only support those calls which the executable\n    needs to start up, shut down, and find out where the checkpoint\n    image is located or to where it should be written. All others\n    are shunted to the local file system via WantRemoteIO = False.\n\n    If we wanted to support the rest of Remote I/O, then the new\n    starter can either handle the remote i/o requests itself or shunt\n    them via a protocol over a daemoncore socket to the new shadow\n    who will provide answers. This could be done over chirp as well.\n\nCheckpointing:\n    A user job needs to locate the checkpoint when needing it, or a place to\n    put it when creating it.\n\n    Source checkpoints can live in these places:\n        submit machine\n        checkpoint server\n        third party file server\n\n    Destination checkpoints can live in these places:\n        submit machine\n        checkpoint server\n        third party file server\n\n    There is some leeway in how the job actually gets the\n    checkpoint. For example, the shadow could use file transfer to\n    transfer the checkpoint from the submit machine to the execute\n    machine before the job starts up.  When the starter starts the\n    job it'll tell it the on disk location to restart from.\n\n    If the checkpoint was always read/written from the local disk under\n    the starter's control. Then file transfer plugins could be invoked to\n    pull/push a checkpoint to wherever it needs to go.\n\nStarter:\n    A new StdunivProc has to be derived from OSProc which encapsulates things\n    like periodically checkpointing, handling the connection from the\n    stduniv job, starting the job with the right exec personality, etc.\n    I don't think this will be too hard.\n\n    A mechanism for periodic checkpoints would have to be made. This\n    could just be two attributed in the job ad, specified in the submit\n    file, which contain the periodic signal to be sent, plus the time\n    span between each sending.\n\n\nAPPENDIX A\n----------\n\nOrdered Remote I/O calls for a job which opens up stdout/stderr, but\nonly writes (34 bytes) to stdout and then exits.\n\n&lt;start&gt;\n\nCONDOR_register_fs_domain\n    FS_Domain = \"cs.wisc.edu\"\nCONDOR_register_uid_domain\n    UID_Domain = \"cs.wisc.edu\"\nCONDOR_register_ckpt_platform\nCONDOR_register_ckpt_server\nCONDOR_register_arch\nCONDOR_register_opsys\nCONDOR_startup_info_request\nCONDOR_get_a_out_name\n    answer = \"/scratch/condor/spool/cluster72.ickpt.subproc0\"\nCONDOR_file_info\n    logical_name = \"/scratch/condor/spool/cluster72.ickpt.subproc0\"\n    shadow process always deciding location of url\n    not remapped\n    full_path = \"/scratch/condor/spool/cluster72.ickpt.subproc0\"\n    actual_url: remote:/scratch/condor/spool/cluster72.ickpt.subproc0\nCONDOR_get_file_stream\n    Entering pseudo_get_file_stream\n    file = \"/scratch/condor/spool/cluster72.ickpt.subproc0\"\n    &lt;sends file data&gt;\nCONDOR_get_iwd\n    answer = \"/scratch/psilord/condor/test\"\nCONDOR_chdir\n    path = \"/scratch/psilord/condor/test\"\n    CurrentWorkingDir = \".\"\n    CurrentWorkingDir = \"/scratch/psilord/condor/test\"\nCONDOR_register_syscall_version\n    User job is compatible with this shadow version\nCONDOR_get_ckpt_name\n    answer = \"/scratch/condor/spool/cluster72.proc0.subproc0\"\n\nCONDOR_get_std_file_info\n    fd = 0\n    logical name: \"/dev/null\"\nCONDOR_get_file_info_new\n    logical_name = \"/dev/null\"\n    not remapped\n    full_path = \"/dev/null\"\nCONDOR_get_buffer_info\n    bytes=524288 block_size=32768\n\nCONDOR_get_std_file_info\n    fd = 1\n    logical name: \"foo.0.out\"\nCONDOR_get_file_info_new\n    logical_name = \"/scratch/psilord/condor/test/foo.0.out\"\n    not remapped\n    full_path = \"/scratch/psilord/condor/test/foo.0.out\"\nCONDOR_open\n    rval = 5\nCONDOR_lseek\n    fd = 5\n    offset = 0\n    whence = 2\nCONDOR_lseek\n    fd = 5\n    offset = 0\n    whence = 0\n\nCONDOR_get_std_file_info\n    fd = 2\n    logical name: \"foo.0.err\"\nCONDOR_get_file_info_new\n    logical_name = \"/scratch/psilord/condor/test/foo.0.err\"\n    not remapped\n    full_path = \"/scratch/psilord/condor/test/foo.0.err\"\nCONDOR_open\n    rval = 6\nCONDOR_lseek\n    fd = 6\n    offset = 0\n    whence = 2\nCONDOR_lseek\n    fd = 6\n    offset = 0\n    whence = 0\n\nCONDOR_fstat\n    fd = 5\nCONDOR_lseekwrite\n    fd = 5\n    offset = 0\n    whence = 0\n    len = 34\n\nCONDOR_close\n    fd = 5\n\nCONDOR_close\n    fd = 6\n\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_reallyexit\n\n&lt;end&gt;\n\nAPPENDIX B\n----------\n\nOrdered remote I/O for a process which does no I/O, but\ncalls ckpt_and_exit(), and checkpoints to pitcher.cs.wisc.edu\n(128.105.121.40). The output in the log is very confusing and implies the\ncheckpoint got written both to the shadow and to the checkpoint server. It\nis not easy to, in fact, figure out where it went!\n\nIt would appear, given the remote i/o calls below, that the checkpoint\nwas both written to the shadow and to the checkpoint server. I don't\nunderstand this since it is supposed to be one or the other. I confirmed that\nthe checkpoint was actually written to the checkpoint server. So wtf?\n\nCONDOR_register_fs_domain\n    FS_Domain = \"cs.wisc.edu\"\nCONDOR_register_uid_domain\n    UID_Domain = \"cs.wisc.edu\"\nCONDOR_register_ckpt_platform\nCONDOR_register_ckpt_server\nCONDOR_register_arch\nCONDOR_register_opsys\nCONDOR_startup_info_request\nCONDOR_get_a_out_name\n    answer = \"/scratch/condor/spool/cluster73.ickpt.subproc0\"\nCONDOR_file_info\n    logical_name = \"/scratch/condor/spool/cluster73.ickpt.subproc0\"\n    shadow process always deciding location of url\n    not remapped\n    full_path = \"/scratch/condor/spool/cluster73.ickpt.subproc0\"\n    actual_url: remote:/scratch/condor/spool/cluster73.ickpt.subproc0\nCONDOR_get_file_stream\n    file = \"/scratch/condor/spool/cluster73.ickpt.subproc0\"\n    &lt;file stream finishes being gotten&gt;\nCONDOR_get_iwd\n    answer = \"/scratch/psilord/condor/test\"\nCONDOR_chdir\n    path = \"/scratch/psilord/condor/test\"\nCONDOR_register_syscall_version\nCONDOR_get_ckpt_name\n    answer = \"/scratch/condor/spool/cluster73.proc0.subproc0\"\nCONDOR_get_std_file_info\n    fd = 0\n    logical name: \"/dev/null\"\nCONDOR_get_file_info_new\n    logical_name = \"/dev/null\"\nCONDOR_get_buffer_info\n\nCONDOR_get_std_file_info\n    fd = 1\n    logical name: \"foo.0.out\"\nCONDOR_get_file_info_new\n    logical_name = \"/scratch/psilord/condor/test/foo.0.out\"\n    not remapped\n    full_path = \"/scratch/psilord/condor/test/foo.0.out\"\nCONDOR_open\n    rval = 5\nCONDOR_lseek\n    fd = 5\n    offset = 0\n    whence = 2\nCONDOR_lseek\n    fd = 5\n    offset = 0\n    whence = 0\n\nCONDOR_get_std_file_info\n    fd = 2\n    logical name: \"foo.0.err\"\nCONDOR_get_file_info_new\n    logical_name = \"/scratch/psilord/condor/test/foo.0.err\"\n    not remapped\n    full_path = \"/scratch/psilord/condor/test/foo.0.err\"\n\nCONDOR_open\n    rval = 6\nCONDOR_lseek\n    fd = 6\nCONDOR_lseek\n    fd = 6\n    offset = 0\n    whence = 0\nCONDOR_fstat\n    fd = 5\n\nRead: About to send CHECKPOINT and EXIT signal to SELF\n\nCONDOR_send_rusage\nCONDOR_get_buffer_info\nRead: CondorFileTable::checkpoint\nCONDOR_close\n    fd = 5\nCONDOR_close\n    fd = 6\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\n\nCONDOR_image_size\n    kbytes = 689\n    Got Image Size report of 689 kilobytes\n    Adding executable size of 4500 kilobytes\nCONDOR_get_ckpt_mode\n\nRead: About to write checkpoint\nRead: Checkpoint name is \"/scratch/condor/spool/cluster73.proc0.subproc0\"\nRead: Tmp name is \"/scratch/condor/spool/cluster73.proc0.subproc0.tmp\"\n\nCONDOR_file_info\n    logical_name = \"/scratch/condor/spool/cluster73.proc0.subproc0.tmp\"\n    not remapped\n    full_path = \"/scratch/condor/spool/cluster73.proc0.subproc0.tmp\"\nCONDOR_put_file_stream\n    Entering pseudo_put_file_stream\n    file = \"/scratch/condor/spool/cluster73.proc0.subproc0.tmp\"\n\nStoreRequest returned addr: 128.105.121.40 58470\n\nRead: Opened \"/scratch/condor/spool/cluster73.proc0.subproc0.tmp\" via file stream\n\n03/28 11:16:54 (73.0) (27489):Read: About to rename \"/scratch/condor/spool/clust\ner73.proc0.subproc0.tmp\" to \"/scratch/condor/spool/cluster73.proc0.subproc0\"\n\nCONDOR_rename\nRead: Renamed OK\nRead: USER PROC: CHECKPOINT IMAGE SENT OK\nRead: Ckpt exit\n\nAt this point, due to how we coded the ckpt_and_exit() semantic, we immediately\nrestart the computation with the checkpoint we just saved.\n\nCONDOR_get_iwd\n    answer = \"/scratch/psilord/condor/test\"\nCONDOR_chdir\n    path = \"/scratch/psilord/condor/test\"\n\nRead: condor_restart:\n\nCONDOR_get_ckpt_name\n    answer = \"/scratch/condor/spool/cluster73.proc0.subproc0\"\nRead: Checkpoint file name is \"/scratch/condor/spool/cluster73.proc0.subproc0\"\n\nCONDOR_file_info\n    logical_name = \"/scratch/condor/spool/cluster73.proc0.subproc0\"\n    not remapped\n    full_path = \"/scratch/condor/spool/cluster73.proc0.subproc0\"\n\nCONDOR_get_file_stream\n    file = \"/scratch/condor/spool/cluster73.proc0.subproc0\"\n\nRestoreRequest returned address:\n  128.105.121.40\nat port 43156\n\nRead: Opened \"/scratch/condor/spool/cluster73.proc0.subproc0\" via file stream\n\nCONDOR_get_ckpt_mode\n\nRead: CondorFileTable::resume\n\nCONDOR_get_buffer_info\nCONDOR_chdir\n    path = \"/scratch/psilord/condor/test\"\nCONDOR_get_file_info_new\n    logical_name = \"/scratch/psilord/condor/test/foo.0.out\"\n    not remapped\n    full_path = \"/scratch/psilord/condor/test/foo.0.out\"\nCONDOR_open\n    rval = 5\nCONDOR_lseek\n    fd = 5\n    offset = 0\n    whence = 2\nCONDOR_lseek\n    fd = 5\n    offset = 0\n    whence = 0\nCONDOR_lseekwrite\n    fd = 5\n    offset = 0\n    whence = 0\nCONDOR_close\n    fd = 5\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_report_file_info_new\nCONDOR_reallyexit\n\n</pre></div>\n</blockquote>", "remarks": "<blockquote>\n<em>2011-Mar-30 11:16:09 by psilord:</em> <br/>\n\nGiven to tannenba for review so see if he desires more information.\n\n<p></p><hr/>\n<em>2012-Dec-17 12:21:37 by tannenba:</em> <br/>\n\nAs we are more likely to pursue the direction of adding ckpt support to vanilla universe via standalone checkpointing (or dmtcp), marking this ticket as defer.\n\n<p></p><hr/>\n<em>2020-Sep-11 12:14:49 by jfrey:</em> <br/>\n\nStandard universe is dead.</blockquote>", "derived_tickets": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"abandoned\" href=\"tktview?tn=2007\" title=\"Can stduniv run as a vanilla job with standalone checkpointing?\">#2007</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nCan stduniv run as a vanilla job with standalone checkpointing?</td></tr>\n</tbody></table>", "attachments": "", "check_ins": "", "type": "enhance", "last_change": "2020-Sep-11 12:14", "status": "abandoned", "created": "2011-Mar-10 10:28", "fixed_version": "2011-Mar-10 10:28", "broken_version": "", "priority": "5", "subsystem": "Std", "assigned_to": "adesmet", "derived_from": "", "creator": "psilord", "rust": "", "customer_group": "other", "visibility": "public", "notify": "tannenba@cs.wisc.edu", "due_date": ""}
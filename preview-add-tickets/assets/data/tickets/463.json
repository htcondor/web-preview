{"id": 463, "title": "Ticket #463: dagman doesn't do anything when condor_rm fails", "description": "<blockquote>\nin Dag::RemoveRunningJobs, the return value of running condor_rm is checked, but no real action is taken if it fails.  a message is logged (to the VERBOSE level--i would suggest NORMAL) but that is all.  so, if the remove fails for some reason (busy schedd, authentication fails, etc.) the jobs will be orphaned.</blockquote>", "remarks": "<blockquote>\n<em>2009-May-12 17:34:50 by wenger:</em> <br/>\n\nMaybe we want to re-try the condor_rm if it fails initially.  (We would need some configuration on how many times to re-try, etc.)  At minimum, we'd need configuration that would allow users to preserve the old behavior.\n\n<p></p><hr/>\n<em>2010-Jun-17 11:53:19 by psilord:</em> <br/>\n\nOddly, I just stumbled across this very thing while doing code inspection....\n\n<p>More information about it is that if condor_rm wants to remove so many jobs that it times out talking to the schedd, it'll <strong>never</strong> be able to remove those jobs.\n\n</p><p>The connection timeout on condor_rm is only configurable via the environment variable of _CONDOR_TOOL_TIMEOUT_MULTIPLIER.\n\n</p><p></p><hr/>\n<em>2010-Jun-17 12:09:22 by matt:</em> <br/>\n\nrm timeout recorded in <span class=\"ticket\"><a class=\"new\" href=\"/tickets?ticket=1067\" onclick=\"get_ticket_and_populate_wrapper('1067'); return false;\" title=\"condor_rm timeout when removing many (100Ks) jobs\">#1067</a></span></blockquote>", "derived_tickets": "", "attachments": "", "check_ins": "", "type": "defect", "last_change": "2017-May-16 11:11", "status": "new", "created": "2009-May-07 16:18", "fixed_version": "2009-May-07 16:18", "broken_version": "v070201", "priority": "3", "subsystem": "Dag", "assigned_to": "coatsworth", "derived_from": "#1067", "creator": "zmiller", "rust": "", "customer_group": "fermi", "visibility": "public", "notify": "zmiller@cs.wisc.edu, wenger@cs.wisc.edu, psilord@cs.wisc.edu", "due_date": ""}
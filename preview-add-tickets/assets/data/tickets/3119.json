{"id": 3119, "title": "Ticket #3119: add support for running \"unreliable\" jobs N times until they agree", "description": "<blockquote>\nThe specific use-case is that when running on consumer GPUs, jobs regularly produce bogus results.  We currently address this with a nasty hack to run each DAGMan node twice (by making it a \"queue 2\" job and having a POST script compare the two results and mark the node as failed if they disagree, so RETRY applies and runs them both again), but we would love for this to be a first-class Condor or DAGMan tool or feature.\n\n<p>You could generalize this to N copies, or to the related long-tail use-case where you want a node to submit N copies of a job and complete whenever the first one succeeds, or beyond -- but something to solve this basic run-twice-and-agree use-case in 7.9 would be preferable to something fancier in 2 years...</p></blockquote>", "remarks": "<blockquote>\n<em>2012-Nov-27 14:47:13 by wenger:</em> <br/>\n\nOkay, I think the first step here is to define in more detail how things should work from the user's perspective.\n\n<p>Even just in the \"two runs must agree\" case, I can see some different ways of handling it:\n</p><ol>\n<li>Two jobs are initially run; if they don't agree a third job is run; if that doesn't agree with either of the first two, a fourth job is run, etc.\n</li><li>You initially submit some number of jobs; as soon as any two finish and the output agrees, you're done and can condor_rm any jobs that haven't finished yet.\n</li><li>You initially submit two (or however many) jobs; if you don't get agreement you retry the whole node (more like LIGO's current hack and probably not the way to go).\n</li></ol>\n\n<p>I think we also need to figure out in more detail how the test for \"agreement\" should work.  My first thought is that that should be done by some kind of user-written script -- but then we need a way to invoke the script on all possible combinations of output files for the different jobs.  So maybe it shouldn't be a regular POST script, but some kind of newly-defined type of script.\n\n</p><p>Even if we're sure that the test we want is that the output of two jobs match, I don't think we dare just do a bytewise comparison of the two output files -- they might have timestamps in them or something.\n\n</p><p>We also have to think about how to not have the various output files overwrite each other.\n\n</p><p></p><hr/>\n<em>2012-Nov-29 15:29:30 by wenger:</em> <br/>\n\nHere's a more detailed idea:\n\n<p></p><ul>\n<li>The user is responsible for output files not overwriting each other (by putting $(cluster) in the output file names or something like that).\n</li><li>We have some way of telling DAGMan how many instances of the job to initially launch.\n</li><li>Each time one of the instances finishes, a user-defined test script gets called.  This script is passed the CondorIDs of all of the completed instances, from which it needs to be able to figure out all of the relevant output file names, or whatever it needs to do its checking.  This test script can return several values:\n<ul>\n<li>We're done successfully (delete any remaining job instances), node succeeds.\n</li><li>We're done unsuccessfully (delete any remaining job instances), node fails.\n</li><li>Launch another job instance (maybe have a way to launch more than one?).\n</li></ul>\n</li></ul>\n\n<p></p><hr/>\n<em>2013-Mar-05 10:06:11 by pfc:</em> <br/>\n\nAny updates on this?  I'm bumping down to priority==3 because it's not urgent... but that being said, we'd love to see progress and would get use out of it immediately if you implemented it.\n\n<p></p><hr/>\n<em>2013-Mar-05 15:07:44 by wenger:</em> <br/>\n\nThere was some further discussion of this in today's LIGO phone conference.  Peter said that another possible sub-case is this:  as you get towards the end of the DAG, start up more instances of the unreliable jobs, so that you get a good case sooner (to cut down the long tail of a few jobs than may make the overall workflow take much longer than it otherwise would).  (See the google map/reduce paper (<a class=\"external\" href=\"http://research.google.com/archive/mapreduce.html\">http://research.google.com/archive/mapreduce.html</a>) for some statistics on this.)\n\n<p></p><hr/>\n<em>2015-Sep-21 11:59:05 by tpdownes:</em> <br/>\n\nPeter:\n\n<p>What do you consider the status of this ticket?\n\n</p><p>I am doing ticket housecleaning and am starting to hit ones that might actually deserve to live.\n\n</p><p>Tom\n\n</p><p></p><hr/>\n<em>2015-Sep-22 13:56:50 by tpdownes:</em> <br/>\n\nThoughts after in-person visit to UW by TPD...\n\n<p>This is the view from 2015. My <strong>impression</strong> is that the fear of jobs producing numerically invalid output but exiting with code 0 is much lower than initially anticipated. That is to say, we are finding gamer GPUs to be more numerically accurate than we might have guessed. To me, the upshot is that the degree to which we demand <strong>new</strong> features of HTCondor to handle this fear should go down considerably and that we should consider how best to use existing features.\n\n</p><p>The \"queue 2 + RETRY N\" model in use seems very reasonable to me in that the node success/failed state accurately reflects the exit code of the POST script written by the scientist as the arbiter of \"did the 2 jobs agree?\"\n\n</p><p>The model has 1 potential failure in that it does not ensure that the 2 jobs ran on different \"unreliable\" resources. I think a simple extension of this model would be to add to these jobs a <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=ClassAd\" title=\"Class Ad\">ClassAd</a></span> such as\n\n</p><p>\"+RunSeparately = True\"\n\n</p><p>and then add to the Machine <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=ClassAds\" title=\"Class Ads\">ClassAds</a></span> a Requirement such as\n\n</p><p>TARGET.RunSeparately &amp;&amp; isOdd(ProcId)\n\n</p><p>for half the cluster and\n\n</p><p>TARGET.RunSeparately &amp;&amp; isEven(ProcId)\n\n</p><p>for the other half. This will have the impact of ensuring that unreliable jobs are assured of running on different resources without greatly affecting throughput on those resources (for uniform unreliable job population) or affecting the throughput of jobs that only use reliable resources.</p></blockquote>", "derived_tickets": "", "attachments": "", "check_ins": "", "type": "enhance", "last_change": "2015-Sep-23 10:05", "status": "defer", "created": "2012-Jul-10 15:16", "fixed_version": "2012-Jul-10 15:16", "broken_version": "", "priority": "5", "subsystem": "Dag", "assigned_to": "", "derived_from": "", "creator": "pfc", "rust": "", "customer_group": "ligo", "visibility": "public", "notify": "anderson@ligo.caltech.edu, wenger@cs.wisc.edu, pfcouvar@syr.edu, downes@uwm.edu,pcouvare@caltech.edu", "due_date": ""}
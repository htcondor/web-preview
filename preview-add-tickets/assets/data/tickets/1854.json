{"id": 1854, "title": "Ticket #1854: Stack corruption in condor_hdfs due to writing beyond an array's bound", "description": "<blockquote>\nIn condor_hdfs, an array of three integers (arrIO) is used to hold the pipe endpoints used for redirecting standard in/out/error.  The Create_Pipe function accepts as a parameter an array of two integers to store the index values for the two endpoints of the pipe it creates.  Whether the original coder was trying to be clever or something, arrIO is being reused while creating the pipes we want to use for output/error redirection.  One mistake in this cleverness is the passing of the address of the last element in the arrIO array to the Create_Pipe function to serve as the pointer to the array for storing the two endpoints it is supposed to give back.  This corrupts the stack when Create_Pipe writes outside the bounds of arrIO and may be the cause of the crash inside argList when condor_hdfs is compiled using O2 optimization on Windows.  O1 optimization does not detect it but when full debug is run inside Visual Studio, VS detects the stack corruption.\n\n<p>Solution: rewrite the way that the pipe endpoints are stored inside the initialization code for condor_hdfs.  The logic of that block of code is a bit convoluted and could use some simplification.</p></blockquote>", "remarks": "<blockquote>\n<em>2011-Jan-18 14:25:16 by ziliang:</em> <br/>\n\nCause confirmed.  The logic for creating and registering the pipes for stdout/err redirection was massively broken in condor_hdfs and I am not sure how, if it ever, worked, with the previous revision.  Not only were we overwriting the FD of the write end of the first pipe we created with the FD of the read end of the second pipe we created, we also were saving the FD of the write end of the second pipe outside the bounds of the array we had set up to hold the FDs.  We were also not saving the write ends and passing the read ends to the new process we were creating, so attempts to communicate back to the parent by writing out to stdout or stderr by the child probably was not working, or at the very least should not have worked, unless the pipes were full duplex.  We were also not closing the write ends of the two pipes created so were leaking resources every time we created a new instance of hadoop.  Something tells me we are not cleaning up the read ends either if we start up a new instance of hadoop and recreate the pipes for redirection.\n\n<p>With Dan's permission, this is going into the 7.5.5 branch.  Whether it makes it into the release will depend on if he needs to retag for the release, again.\n</p><hr/>\n<em>2011-Feb-01 16:20:02 by tannenba:</em> <br/>\n\nBulk change of target version from v070506 to NULL using ./ticket-target-mover.</blockquote>", "derived_tickets": "", "attachments": "", "check_ins": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">2011-Jan-19 12:25</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"chngview?cn=20018\">[20018]</a></span>: Fix bugs in condor_hdfs pipe handling to child process <span class=\"ticket\"><a class=\"resolved\" href=\"tktview?tn=1854\" title=\"Stack corruption in condor_hdfs due to writing beyond an array's bound\">#1854</a></span>  (By Greg Thain )</td></tr>\n</tbody></table>", "type": "defect", "last_change": "2011-Nov-21 15:20", "status": "resolved", "created": "2011-Jan-14 16:17", "fixed_version": "2011-Jan-14 16:17", "broken_version": "v070504", "priority": "4", "subsystem": "Daemons", "assigned_to": "gthain", "derived_from": "#1835", "creator": "ziliang", "rust": "", "customer_group": "other", "visibility": "public", "notify": "tstclair@redhat.com", "due_date": "20100119"}
{"id": 7254, "title": "Ticket #7254: Create plan for low-latency LIGO bayestar schedule", "description": "<blockquote>\nLIGO would like to start long-running services to process low-latency work.  These services will be started by condor, but will be triggered externally to actually do their work.  However, the work will be infrequent, so they'd like to backfill these machines with real HTCondor jobs.\n\n<p>In the short term, we will give them a recipe to start a 2nd start with a cgroup with a huge cpu shares, and suspect that will be good enough.</p></blockquote>", "remarks": "<blockquote>\n<em>2019-Oct-08 10:09:24 by gthain:</em> <br/>\n\n<div class=\"code\">\n<pre class=\"code\">Greg,\n\tLooks good. We are going to start deploying this at CIT tomorrow. The current/original STARTD will continue to run a single pslot and the new HIPRIO_STARTD will run a single static slot. Both will advertise all of the system CPUs, but we will make a hard partition of memory between the two to avoid oversubscription/swapping. How do we scope SLOT_TYPE_X_Y statements to bind them to one startd versus the other?\n\nThanks.\n\n\n&gt; On Sep 30, 2019, at 8:24 PM, Greg Thain &lt;gthain@cs.wisc.edu&gt; wrote:\n&gt;\n&gt; Here is the configuration for two startds on a single condor machine -- I'd need a bit more info from you about how you'd like them configured:  pslots or static slots, etc.  I have verified that jobs go into different cgroups, but haven't done much testing to see how well the cgroups isolate the cpu usage:\n&gt;\n&gt;\n&gt;\n&gt; basically, add the HIPRIO_START to your DAEMON_LIST\n&gt;\n&gt; DAEMON_LIST                     = MASTER, STARTD, HIPRIO_STARTD\n&gt;\n&gt; then add the following lines to the config file:\n&gt;\n&gt; HIPRIO_STARTD = $(STARTD)\n&gt; HIPRIO_STARTD_ARGS = -f -local-name HIPRIO_STARTD\n&gt; HIPRIO_STARTD_ENVIRONMENT = \"_CONDOR_STARTER_LOG=$(STARTER_LOG).hiprio _CONDOR_PROCD_ADDRESS=$(PROCD_ADDRESS).hiprio _CONDOR_BASE_CGROUP=hiprio\"\n&gt; HIPRIO_STARTD.STARTD_LOG = $(STARTD_LOG).hiprio\n&gt; HIPRIO_STARTD.STARTD_ADDRESS_FILE = $(STARTD_ADDRESS_FILES).hiprio\n&gt; HIPRIO_STARTD.EXECUTE = $(EXECUTE).hiprio\n&gt; HIPRIO_STARTD.STARTD_NAME = HIPRIO_STARTD\n&gt;\n&gt;\n&gt; Then in /sys/fs/cgroup/cpu/hiprio, set cpu.shares to some very large number, say 100,000.\n</pre></div>\n\n\n<p></p><hr/>\n<em>2019-Oct-08 10:36:34 by pfc:</em> <br/>\n\nShould we also increase the i/o priority of the high-priority slot?\n\n<p></p><hr/>\n<em>2019-Oct-20 19:37:22 by anderson:</em> <br/>\n\nThe following configuration looks like it is working to give priority multi-threaded jobs a 100:1 ratio of CPU time over background HTC jobs, and to add a hard limit to memory usage by the priority processes. This is currently running on 2 nodes at CIT for testing, and will be scaled out to qty 100 24-core nodes on Tues (Oct 22).\n\n<p>So far, it has been it has been confirmed that OpenMP jobs that take 3-sec when run interactively on identical hardware are also scheduled and complete execution via a Celery queue on top of Condor in &lt;4s, i.e., jobs are able to burst to 2-nodes and return their results with the goal of sub-second scheduling latency. The next test will be verify that scales to qty 100 parallel jobs, i.e., bursting to 2400-cores with sub-second latency and getting 99% of the kernel cpu-slices without disrupting HTC jobs already running on the same hardware.\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">[root@node2060 ~]# cat /etc/systemd/system/condor.service.d/emfollow.conf\n[Service]\nMemoryAccounting = true\nExecStartPost    = /bin/bash -c \"cgcreate -g *:emfollow; cgset -r memory.limit_in_bytes=8G -r memory.memsw.limit_in_bytes=8G -r cpu.shares=102400 /emfollow\"\n</pre></div>\n\n\n<p></p><div class=\"code\">\n<pre class=\"code\">[root@node2060 ~]# cat /etc/condor/config.d/04-emfollow\n#\n# Configuration to add a second STARTD in a different cgroup for occasional\n# high priority EMFollow jobs to run--initial use case Bayestar sky map jobs.\n# Note, bayestar jobs do not need much memory so no need to reduce the mem\n# setting for the normal STARTD, but only advertise a small amount of memory\n# for the EMFOLLOW STARTD.\n#\n# For now a separte Condor systemd config file is used to create the initial\n# EMFOLLOW cgroup, set a small 2GB memory limit and give that collection of\n# processes a 100:1 ratio over \"regular\" condor jobs. Note, if that needs to\n# be dynamically changed just echo a new value to /sys/fs/cgroup/cpu/emfollow/cpu.shares\n# that has an appropriate ratio to /sys/fs/cgroup/cpu/htcondor/cpu.shares\n#\n# /etc/systemd/system/condor.service.d/emfollow.conf\n# [Service]\n# MemoryAccounting = true\n# ExecStartPost    = /bin/bash -c \"cgcreate -g *:emfollow; cgset -r memory.limit_in_bytes=2G -r memory.memsw.limit_in_bytes=2G -r cpu.shares=102400 /emfollow\"\n#\n# To verify working properly run systemd-cgtop to see relative CPU use between\n# processes running under the 2 STARTD instances and grep cpuacct /proc/PID/cgroup\n# to see what cgroup a given process is registered with, i.e.,\n# /emfollow/condor_local_condor_execute.emfollow_EMFOLLOW_STARTD@node*\n# or\n# /htcondor/condor_local_condor_execute_slot1_1@node*\n#\n# Note, there are other bugs/limitations the SLOT definitions that the Condor team\n# is aware of.  See email thread with Greg Thain for more details\n#\n# Stuart Anderson (Oct 17, 2019)\n#\n\n# Daemon list for condor worker nodes\nDAEMON_LIST\t\t= $(DAEMON_LIST) EMFOLLOW_STARTD\n\n# Bug in Condor 8.8 with shared ports and multiple startd (override setting in 00-ldas)\nUSE_SHARED_PORT = False\n\n# Slot definition\nEMFOLLOW_STARTD.SLOT_TYPE_1_PARTITIONABLE = False\nEMFOLLOW_STARTD.SLOT_TYPE_1             = cpu=100%, mem=4%\nEMFOLLOW_STARTD.NUM_SLOTS_TYPE_1        = 1\n\n# Optimization_BayesWave-related class ads\nOnline_EMFollow\t\t\t= True\nEMFOLLOW_STARTD.START\t\t= (TARGET.Online_EMFollow =?= True)\nEMFOLLOW_STARTD.START\t\t= $(EMFOLLOW_STARTD.START) || (TARGET.Haswell =?= True)\nEMFOLLOW_STARTD.STARTD_ATTRS\t= $(EMFOLLOW_STARTD.STARTD_ATTRS), Online_EMFollow\n\nEMFOLLOW_STARTD = $(STARTD)\nEMFOLLOW_STARTD_ARGS = -f -local-name EMFOLLOW_STARTD\nEMFOLLOW_STARTD_ENVIRONMENT = \"_CONDOR_STARTER_LOG=$(STARTER_LOG).emfollow _CONDOR_PROCD_ADDRESS=$(PROCD_ADDRESS).emfollow _CONDOR_BASE_CGROUP=emfollow\"\nEMFOLLOW_STARTD.STARTD_LOG = $(STARTD_LOG).emfollow\nEMFOLLOW_STARTD.STARTD_ADDRESS_FILE = $(STARTD_ADDRESS_FILES).emfollow\nEMFOLLOW_STARTD.EXECUTE = $(EXECUTE).emfollow\nEMFOLLOW_STARTD.STARTD_NAME = EMFOLLOW_STARTD\n\nEMFOLLOW_STARTD.BASE_CGROUP = htcondor/emfollow\n</pre></div>\n\n\n<p>Here is a list of problems encountered when setting up 2 startd on a single execute machine:\n\n</p><p>1) Is it a bug or a feature that I have to manually mkdir $(EMFOLLOW_STARTD.EXECUTE) once on each execute node?\n\n</p><p>2) How do I change \"mem=4%\" above to \"8 GByte\" to make it more portable (initially running on systems with 196GB of memory)?\n\n</p><p>3) USE_SHARED_PORT = True fails with multiple startd in 8.8.5. This is a known issue, but can the 8.9 fix be backported to 8.8?\n</p><div class=\"code\">\n<pre class=\"code\">10/17/19 13:18:46 (pid:10412) IO: Message Digest/MAC verification failed!\n</pre></div>\n\n\n<p>4)  The following creates 1 partitionable slot for the main STARTD, and 1 static slot for EMFOLLOW_STARTD,\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">DAEMON_LIST             = MASTER, STARTD, EMFOLLOW_STARTD, CKPT_SERVER\n\nSLOT_TYPE_1_PARTITIONABLE = True\nSTARTD.SLOT_TYPE_1             = cpu=50%, mem=80%\nSTARTD.NUM_SLOTS_TYPE_1 = 1\n\nEMFOLLOW_STARTD.SLOT_TYPE_1_PARTITIONABLE = False\nEMFOLLOW_STARTD.SLOT_TYPE_1             = cpu=10%, mem=10%\nEMFOLLOW_STARTD.NUM_SLOTS_TYPE_1        = 1\n</pre></div>\n\n\n<p>However, if I scope the first slot_type to,\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">STARTD.SLOT_TYPE_1_PARTITIONABLE = True\n</pre></div>\n\n\n<p>then I end up with two Static slots, as if the default STARTD named startd is not honoring an explicit STARTD. scope.\n\n</p><p>5) If I change EMFOLLOW_STARTD entries in the above from TYPE_1 to TYPE_2 then I end up with 3 slots.\n\n</p><p>6) condor_restart -daemon does not work, however, condor_on and condor_off do, e.g.,\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">[root@node2066 ~]# condor_restart -daemon emfollow_startd\nCan't find address for local emfollow_startd\nPerhaps you need to query another pool.\n\n[root@node2066 ~]# condor_off -daemon emfollow_startd\nSent \"Kill-Daemon\" command for \"emfollow_startd\" to local master\n\n[root@node2066 ~]# condor_on -daemon emfollow_startd\nSent \"Spawn-Daemon\" command for \"emfollow_startd\" to local master\n</pre></div>\n\n\n<p>Note, most of these will likely remain low-priority issues for LIGO as the plan is to move this functionality into a single STARTD, i.e., obtain the same (or better) cgroup control over separate slots in a single startd via Condor configuration files that get passed to the kernel.\n\n</p><p></p><hr/>\n<em>2019-Oct-29 10:54:30 by pfc:</em> <br/>\n\nNow that we have shown that cgroups seem to work for this, Greg says the next step is to do it within one startd.\n\n<p></p><hr/>\n<em>2020-Jan-17 18:43:41 by anderson:</em> <br/>\n\nIn an attempt to address the following problem,\n\n<p></p><div class=\"code\">\n<pre class=\"code\">[root@node2061 ~]# condor_restart -daemon emfollow_startd\nCan't find address for local emfollow_startd\nPerhaps you need to query another pool.\n</pre></div>\n\n\n<p>I switched the DAEMON_LIST entry from EMFOLLOW_STARTD to STARTD_EMFOLLOW on a different execute node as suggested by Greg,\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">I've been working on the condor_off problem, and I think I gave you a sub-optimal recipe for starting multiple startds on a machine.\n\n\nIf you have 2 startds in the DAEMON_LIST like\n\n\nDAEMON_LIST = STARTD, STARTD2\n\nand you define STARTD2 as\n\nSTARTD2 = $(SBIN)/condor_startd\n\nThen, you don't need the line\n\nSTARTD2_ARGS = -f -local-name STARTD2\n\nThen, you should be able to condor_off (or reconfig or restart) just the STARTD2 with\n\n\ncondor_off -daemon startd2\n</pre></div>\n\n\n<p>However, I still observe,\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">[root@node2190 ~]# condor_config_val daemon_list\nMASTER, STARTD, CKPT_SERVER STARTD_EMFOLLOW\n\n[root@node2190 ~]# condor_restart -daemon startd_emfollow\nCan't find address for local startd_emfollow\nPerhaps you need to query another pool.\n</pre></div>\n\n\n<p>Perhaps a clue is that <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=MasterLog\" title=\"Master Log\">MasterLog</a></span> logs both condor_startd instances as \"STARTD\"\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">01/17/20 16:34:26 Started DaemonCore process \"/usr/sbin/condor_startd\", pid and pgroup = 13852\n01/17/20 16:34:26 Started process \"/usr/sbin/condor_ckpt_server\", pid and pgroup = 13853\n01/17/20 16:34:26 Started DaemonCore process \"/usr/sbin/condor_startd\", pid and pgroup = 13856\n01/17/20 16:34:36 Setting ready state 'Ready' for STARTD\n01/17/20 16:34:36 Setting ready state 'Ready' for STARTD\n</pre></div>\n\n\n<p></p><hr/>\n<em>2020-Mar-10 10:13:33 by pfc:</em> <br/>\n\nClosing this and Greg will submit new tickets specific to remaining work.</blockquote>", "derived_tickets": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"resolved\" href=\"tktview?tn=7335\" title=\"Turning off shared port crashes condor_master\">#7335</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nTurning off shared port crashes condor_master</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"abandoned\" href=\"tktview?tn=7348\" title=\"Can't have more than 1 startd on same machine behind shared port\">#7348</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nCan't have more than 1 startd on same machine behind shared port</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"resolved\" href=\"tktview?tn=7390\" title=\"Allow 1 startd to have starters in different BASE_CGROUPs\">#7390</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nAllow 1 startd to have starters in different BASE_CGROUPs</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"new\" href=\"tktview?tn=7564\" title=\"condor_off and condor_restart usage with multiple startds\">#7564</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\ncondor_off and condor_restart usage with multiple startds</td></tr>\n</tbody></table>", "attachments": "", "check_ins": "", "type": "todo", "last_change": "2020-Mar-10 10:13", "status": "resolved", "created": "2019-Sep-11 13:21", "fixed_version": "2019-Sep-11 13:21", "broken_version": "v080000", "priority": "2", "subsystem": "", "assigned_to": "gthain", "derived_from": "", "creator": "gthain", "rust": "", "customer_group": "ligo", "visibility": "public", "notify": "peter.couvares@ligo.org, anderson@ligo.caltech.edu", "due_date": ""}
{"id": 3013, "title": "Ticket #3013: Pslots prevent Condor from negotiating jobs on idle pool", "description": "<blockquote>\nIt looks like the negotiator cannot hand pslots off to a schedd unless there's a sufficient number of jobs in the queue.\n\n<p>Setup a pslot with 100 CPUs, and submit a single job.  The negotiator will assign the submitter a pieslice of 1; because the smallest pslot has a slot weight of 100, you get something along the lines of:\n\n</p><p></p><div class=\"verbatim\">\n<pre>05/28/12 14:52:13 matchmakingAlgorithm: limit 4.000000 used 0.000000 pieLeft 4.000000\n05/28/12 14:52:13       Rejected 1.14 bbockelm@unl.edu &lt;129.93.239.129:60246&gt;: submitter limit exceeded\n</pre></div>\n\n\n<p>In other words, a private Condor setup cannot run jobs unless there are a sufficient number in queue to fragment the pslot.\n\n</p><p>I'm running from master, about 2 weeks old.</p></blockquote>", "remarks": "<blockquote>\n<em>2012-Jun-20 11:24:02 by eje:</em> <br/>\n\nWhat we're seeing here is another symptom of partitionable slots causing a breakdown of the semantics of 'slot weight'.\n\n<p>The original semantic of slot weights was 'cost (in fairshare units) that a submitter must pay for matching a slot'\n\n</p><p>With p-slots, this semantic breaks down: the default weight (number of cpus) doesn't represent a true cost of matching, since a job in reality peels off only some fraction of the p-slot's resources, and so in general it should not be charged the entire weight.\n\n</p><p>Another closely related model breakdown is that with the original static slots, only one job could match against a slot, where for p-slots, multiple jobs can match.\n\n</p><p>I'm going to look into how cleanly I can augment the negotiation loop with the ability to manage the accounting of matching multiple jobs against against p-slots in a single cycle.\n\n</p><p>More generally, we (or at least I) need to clarify the <strong>desired</strong> relationship between p-slots, p-slot multi-resource accounting, slot weights, fairshare and quotas.\n\n</p><p></p><hr/>\n<em>2012-Jun-20 11:51:29 by tannenba:</em> <br/>\n\nAbove eje says:\n<div class=\"blockquote\">\n<table width=\"95%\"><tbody><tr><td>\u00a0</td><td>\n  \nI am going to look into how cleanly I can augment the negotiation loop with the ability to manage the accounting of matching multiple jobs against against p-slots in a single cycle.\n</td></tr></tbody></table></div>\n\n\n<p>No!!! This is the wrong approach!!!  There is no need for the matchmaker to do anything \"smart\" re pslots to solve this problem.  There is no need for the matchmaker to even know about pslots period.  The last thing we need is yet more attempts for the negotiator to be \"smart\".  The whole idea is we don't want the matchmaker to match individual jobs against a pslot, we want the schedd to handle this level of detail.\n\n</p><p></p><hr/>\n<em>2012-Jun-20 11:59:16 by tannenba:</em> <br/>\n\neje above wrote:\n<div class=\"blockquote\">\n<table width=\"95%\"><tbody><tr><td>\u00a0</td><td>\n  \nWith p-slots, this semantic breaks down: the default weight (number of cpus) doesn't represent a true cost of matching, since a job in reality peels off only some fraction of the p-slot's resources, and so in general it should not be charged the entire weight.\n</td></tr></tbody></table></div>\n\n\n<p>I also disagree with the above statement.\n\n</p><p>If the matchmaker matches a pslot with 100cpus to a schedd, then 100 IS the true cost of the match - that entire pslot was delegated to that schedd, so it should pay.  When the next negotiation cycle happens, the accountant will see that (perhaps) the schedd only is really using 1 cpu out of the initial 100, and the charging will automatically adjust - just as if the schedd had 1 long running job and 99 1 second jobs.\n\n</p><p>Think of it this way: the matchmaker is not in the business of matching jobs to slots.  It is in the business of matching resource requests coming from schedds to slots.  The schedd says \"gimme a resource like this\", and the matchmaker gives it out.  What job, and even how many concurrent jobs, the schedd decides to run on this resource is up to the schedd.  The matchmaker should not know or care at this level.\n\n</p><p>Perhaps we should talk before you do anything on this ticket....\n\n</p><p></p><hr/>\n<em>2012-Jun-20 12:00:42 by bbockelm:</em> <br/>\n\nIs it possible for the negotiator to \"gift\" the submitter more resources than they ask?  In this case, we could increase the pie slice.\n\n<p>The tougher case is when there is contention for resources.  If we have 10 submitters with 10 jobs each and one resource with slot weight 100, who gets the resource?  This gets even stickier with respect to concurrency limits.\n\n</p><p></p><hr/>\n<em>2012-Jun-20 12:11:56 by tannenba:</em> <br/>\n\n<div class=\"blockquote\">\n<table width=\"95%\"><tbody><tr><td>\u00a0</td><td>\n  \nIf we have 10 submitters with 10 jobs each and one resource with slot weight 100...\n</td></tr></tbody></table></div>\n\n\n<p>Answer:  The submitter with the best user priority. Same as if there was one resource with slot weight 1.\n\n</p><p></p><hr/>\n<em>2012-Jun-20 17:04:55 by eje:</em> <br/>\n\nBrian, in your repro scenario, are there any accounting groups involved, or no acct groups?  (if you can attach repro config file(s) that would help me)\n\n<p></p><hr/>\n<em>2012-Jun-20 17:36:23 by bbockelm:</em> <br/>\n\nHere's the relevant config file for our negotiator:\n\n<p><a class=\"external\" href=\"https://github.com/bbockelm/hcc-config/blob/master/modules/condor/files/config.d/03-red-collector\">https://github.com/bbockelm/hcc-config/blob/master/modules/condor/files/config.d/03-red-collector</a>\n\n</p><p>So, accounting groups are involved.  Not sure if it's relevant to the issue.\n\n</p><p></p><hr/>\n<em>2012-Jun-21 17:45:12 by eje:</em> <br/>\n\nA simplified repro/test configuration:\n\n<p></p><div class=\"code\">\n<pre class=\"code\">NEGOTIATOR_DEBUG = D_FULLDEBUG\n\n# schedd-side optimizations can obscure negotiator behaviors\nCLAIM_WORKLIFE = 0\n\n# shouldn't matter here, but customer site uses it:\nNEGOTIATOR_CONSIDER_PREEMPTION = FALSE\n\n# Test whether turning this off fixes the described problem\nNEGOTIATOR_USE_SLOT_WEIGHTS = TRUE\n\n# a single p-slot which will have weight == 10\nNUM_CPUS = 10\nSLOT_TYPE_1 = cpus=10\nSLOT_TYPE_1_PARTITIONABLE = TRUE\nNUM_SLOTS_TYPE_1 = 1\n\n# simple group quota structure to bring 'allocated' logic into play\nGROUP_NAMES = a\nGROUP_QUOTA_a = 10\n\n# This will cause matchmaker to use 'allocated' instead of whole quota, which can cause\n# starvation problems if all available slots cost more than 'allocated'\nGROUP_ACCEPT_SURPLUS = TRUE\n</pre></div>\n\n\n<p>Running the above configuration, submit a job (to group \"a\"):\n</p><div class=\"code\">\n<pre class=\"code\">universe = vanilla\ncmd = /bin/sleep\nargs = 600\nshould_transfer_files = if_needed\nwhen_to_transfer_output = on_exit\n+AccountingGroup=\"a.user\"\nqueue 1\n</pre></div>\n\n\n<p>You should see the job fail to match, because 'accept surplus' induces the 'allocated' quota to be (1) which is sent to the negotiation loop.  The only available slot is partitionable, but has weight 10 and so the submitter \"a.user\" can't afford it:\n</p><div class=\"code\">\n<pre class=\"code\">06/21/12 15:13:38 matchmakingAlgorithm: limit 1.000000 used 0.000000 pieLeft 1.000000\n06/21/12 15:13:38     Sending SEND_JOB_INFO/eom\n06/21/12 15:13:38     Getting reply from schedd ...\n06/21/12 15:13:38     Got NO_MORE_JOBS;  done negotiating\n06/21/12 15:13:38   This submitter hit its submitterLimit.\n06/21/12 15:13:38  resources used scheddUsed= 0.000000\n</pre></div>\n\n\n<p>If you turn off slot weights:\n</p><div class=\"code\">\n<pre class=\"code\">NEGOTIATOR_USE_SLOT_WEIGHTS = FALSE</pre></div>\n\n\n<p>and restart the negotiator, the job should match:\n</p><div class=\"code\">\n<pre class=\"code\">06/21/12 15:14:05 matchmakingAlgorithm: limit 1.000000 used 0.000000 pieLeft 1.000000\n06/21/12 15:14:05       Connecting to startd slot1@rorschach at &lt;192.168.1.2:57864&gt;\n06/21/12 15:14:05 File descriptor limits: max 1024, safe 820\n06/21/12 15:14:05       Sending PERMISSION, claim id, startdAd to schedd\n06/21/12 15:14:05       Notifying the accountant\n06/21/12 15:14:05       Successfully matched with slot1@rorschach\n06/21/12 15:14:05     Reached submitter resource limit: 1.000000 ... stopping\n</pre></div>\n\n\n<p></p><hr/>\n<em>2012-Jun-28 13:29:10 by eje:</em> <br/>\n\nRegarding the 'fencepost' concept of allowing a slot to negotiate regardless of weight/cost: we need to keep in mind that it enables scenarios such as:\n\n<p></p><div class=\"verbatim\">\n<pre>1) all slots are p-slots of 100 cpus\n2) an acct group \"a\" with quota of 10\n3) submit 100 jobs against \"a\", and against a cc-limit of 10\n4) the negotiator matches a 100-cpu p-slot\n5) the schedd gets ahold of it, and uses its new logic to parsel it out to all 100 jobs.\n6) \"a\"'s quota is violated, and so is cc-limit\n</pre></div>\n\n\n<p>In a pool with a large number of smaller-quota acct groups, this scenario becomes more likely.  In a pool with p-slots of large weight, this scenario becomes more likely.\n\n</p><p>I consider this an argument for the notion that whatever object is doing the matching of jobs to resources needs to have access to centralized accounting of cc-limits and group quotas, and that same object needs to be responsible for parseling out p-slots.\n\n</p><p>Currently that object is the negotiator.  If that object ever becomes the scheduler, the current practice of supporting multiple schedulers on a pool raises issues of centralized accounting of quotas, cc-limits, resource usages, prios, etc.  It also seems a lot like making the scheduler into something that is nearly isomorphic to what we now call the matchmaker.\n</p><hr/>\n<em>2012-Aug-23 13:52:15 by zmiller:</em> <br/>\n\nBulk change of target version from v070802 to v070803 using ./ticket-target-mover.\n<hr/>\n<em>2012-Sep-11 11:06:53 by johnkn:</em> <br/>\n\nBulk change of target version from v070803 to v070804 using ./ticket-target-mover.\n<hr/>\n<em>2012-Sep-19 16:15:53 by zmiller:</em> <br/>\n\nBulk change of target version from v070804 to v070805 using ./ticket-target-mover.\n<hr/>\n<em>2012-Oct-25 17:52:16 by johnkn:</em> <br/>\n\nBulk change of target version from v070806 to v070807 using ./ticket-target-mover.\n<hr/>\n<em>2012-Dec-19 09:30:47 by johnkn:</em> <br/>\n\nBulk change of target version from v070807 to v070808 using ./ticket-target-mover.\n<hr/>\n<em>2013-Mar-28 17:16:00 by johnkn:</em> <br/>\n\nBulk change of target version from v070808 to v070809 using ./ticket-target-mover.</blockquote>", "derived_tickets": "", "attachments": "", "check_ins": "", "type": "defect", "last_change": "2013-Mar-28 17:16", "status": "active", "created": "2012-May-28 15:12", "fixed_version": "2012-May-28 15:12", "broken_version": "v070800", "priority": "2", "subsystem": "Daemons", "assigned_to": "eje", "derived_from": "", "creator": "bbockelm", "rust": "", "customer_group": "other", "visibility": "public", "notify": "bbockelm@cse.unl.edu, eje@cs.wisc.edu, tstclair@redhat.com tannenba@cs.wisc.edu, gthain@cs.wisc.edu", "due_date": ""}
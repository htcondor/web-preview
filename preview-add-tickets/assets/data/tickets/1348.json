{"id": 1348, "title": "Ticket #1348: Updating the granularity of accepts per cycle through event loop", "description": "<blockquote>\nWhen updating the maximum accepts per cycle through the event loop we notice a significant increase in the performance of the schedd.\n\n<p>Test:\n    3000 slots\n    100K 60 second sleep jobs\n\n</p><p>When MAX_ACCEPTS_PER_CYCLE=1\n~27 jobs/sec\n\n</p><p>When MAX_ACCEPTS_PER_CYCLE=0 &lt;-accept all inbound connections\n~277 jobs/sec - a ten fold increase.  We are still performing more test, but this is a dramatic increase in performance.</p></blockquote>", "remarks": "<blockquote>\n<em>2010-Apr-14 14:20:12 by tannenba:</em> <br/>\n\nCurious of the above numbers are timings for entire lifecycle of the job, or just job launches.\n\n<p>My worry is if we are we robbing peter to pay paul --- seems like the performance  gain is happening at a fundamental level by raising the priority of command handler callbacks -vs- everything else.\nIt certainly was the intent of daemonCore to round-robin across all events, and this patch brings us closer to the original intent.\nBut I worry a bit at what the cost of this will be - e.g. if the schedd is spending more time on command callbacks, are reaper callbacks going to be more starved and thus we'll see a pile up of unreaped shadow pids? On the other hand, perhaps what is being deprioritized are timer callbacks which are firing too often anyway (because nobody ever thought hard about how they should be tuned/set), all is good in the hood.\n\n</p><p>Perhaps ultimately priority of different callbacks handlers should be tuned for different workloads.\n\n</p><p></p><hr/>\n<em>2010-Apr-14 14:24:48 by tannenba:</em> <br/>\n\nAll in all I really like what Tim unearthed here... A rather subtle change that has a much more profound impact than anyone would have ever guessed.  Nice sleuth work Tim!!!\n\n<p></p><hr/>\n<em>2010-Apr-14 14:38:02 by tstclair:</em> <br/>\n\nThis is the average when it bursts, before it fills your pool.\n\n<p>The overall throughput is significantly higher, we still need to have a direct comparison, but the total time to complete 100K jobs decreased significantly.\n\n</p><p></p><hr/>\n<em>2010-Apr-14 14:55:14 by danb:</em> <br/>\n\nYup, this is good stuff.  I'm very curious to find out whether this is helping by changing the relative priority of the command socket or whether it is helping by simply getting more work done per iteration of the event loop: i.e. making accept() higher priority or reducing overhead per accept().  If the latter, then it may be time to think about alternatives to select().\n\n<p></p><hr/>\n<em>2010-Apr-16 15:08:34 by tstclair:</em> <br/>\n\nUnless there is any objection I plan to merge, by default the behaviour should be the same, and by changing the param one can adjust.\n\n<p></p><hr/>\n<em>2010-Apr-17 17:41:00 by danb:</em> <br/>\n\nFor what it's worth: in my test environment, MAX_ACCEPTS_PER_CYCLE=0 produces a job completion rate that is 1.1x to 1.2x the rate with MAX_ACCEPTS_PER_CYCLE=1.  I was hoping to see a bit more!  Now I am curious to know what the actual throughput was in the environment at <code>RedHat</code>.  My test machine is not particularly powerful (32-bit dual core 2.4GHz).\n\n<p>As expected, MAX_ACCEPTS_PER_CYCLE=0 noticeably reduces shadow --&gt; schedd communication failures, indicating that these communication failures are caused by starvation of the command socket.\n\n</p><p>My tests were based on the head of the master branch, so pre-7.5.3.\n\n</p><p></p><hr/>\n<em>2010-Apr-18 10:17:59 by matt:</em> <br/>\n\nWill you both add info about your methodology?\n\n<p></p><hr/>\n<em>2010-Apr-18 19:41:56 by danb:</em> <br/>\n\nI used 1s sleep jobs instead of 60s sleep jobs and I only submitted 20k.  Here's my submit file:\n\n<p></p><div class=\"verbatim\">\n<pre>executable = /bin/sleep\narguments = 1\nnotification = never\nrequirements = (Arch == \"INTEL\" || Arch == \"X86_64\")\nshould_transfer_files = yes\nwhen_to_transfer_output = on_exit\ntransfer_executable = false\nqueue 20000\n</pre></div>\n\n\n<p>I then measured the time it took from the time the first job started to the time the last one finished.  The jobs ran on the UW ghost pool.  Because of the limited amount of memory on my test submit machine, I set MAX_JOBS_RUNNING=2000 instead of 3000.\n\n</p><p>I tried two different configurations: one with SPOOL on a local disk and one SPOOL in /dev/shm.  The speedup in job throughput from setting MAX_ACCEPTS_PER_CYCLE=0 was about 1.1x in the first case and 1.2x in the second case.\n\n</p><p></p><hr/>\n<em>2010-Apr-19 09:04:11 by tstclair:</em> <br/>\n\nSo matt's comment is a good one.  We should probably start to note the following:\n\n<p>==Preconditions: (Going to start using wallaby to store exact info)==\n\n</p><p>1.) 6 8-way over-provisioned nodes using layering(glide ins) to make it appear that we have ~ 3k nodes.\n\n</p><p>2.) SPOOL -&gt; local drive /var/lib/condor/spool\n\n</p><p>3.) Our submit file is pretty much the same expect we queue 100000\n\n</p><p>==What &amp; How we are measuring:==\n\n</p><p>The job spawning rate by the schedd, using a series of scripts to parse the history log and the user job logs.\n\n</p><p>==Results:==\n\n</p><p>We see a huge spike in how fast the schedd to spawn jobs, but despite this fact, we see a saw tooth graph on pool utilization which points to another source of down time, which we suspect it likely due to negotiation (or some sub optimal configuration)</p></blockquote>", "derived_tickets": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=1487\" onclick=\"get_ticket_and_populate_wrapper('1487'); return false;\" title=\"document config knob MAX_ACCEPTS_PER_CYCLE\">#1487</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\ndocument config knob MAX_ACCEPTS_PER_CYCLE</td></tr>\n</tbody></table>", "attachments": "<blockquote>\n<ul>\n<li><a href=\"attach_get/154/maxaccept.patch\">maxaccept.patch</a>\n5726 bytes added by tstclair on 2010-Apr-14 13:54:00 UTC.\n<br/>\nUpdates to daemon-core<br/>\n</li></ul>\n</blockquote>", "check_ins": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">2010-Jul-15 23:16</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"chngview?cn=18571\">[18571]</a></span>: Update for Tickets <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=1348\" onclick=\"get_ticket_and_populate_wrapper('1348'); return false;\" title=\"Updating the granularity of accepts per cycle through event loop\">#1348</a></span> &amp; <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=1487\" onclick=\"get_ticket_and_populate_wrapper('1487'); return false;\" title=\"document config knob MAX_ACCEPTS_PER_CYCLE\">#1487</a></span> - Update default MAX_ACCEPTS_PER_CYCLE=4  (By Timothy St. Clair )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2010-Apr-23 14:37</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"chngview?cn=17906\">[17906]</a></span>: Fixed a bad free(). <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=1348\" onclick=\"get_ticket_and_populate_wrapper('1348'); return false;\" title=\"Updating the granularity of accepts per cycle through event loop\">#1348</a></span>  (By Dan Bradley )</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">2010-Apr-19 10:55</td>\n<td align=\"center\" valign=\"top\" width=\"30\">\n<span class=\"icon dot\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\"> \nCheck-in <span class=\"chng\"><a href=\"chngview?cn=17877\">[17877]</a></span>: Ticket <span class=\"ticket\"><a class=\"resolved\" href=\"/tickets?ticket=1348\" onclick=\"get_ticket_and_populate_wrapper('1348'); return false;\" title=\"Updating the granularity of accepts per cycle through event loop\">#1348</a></span> update the max accepts per event loop cycle. It is recommended that you update your entire pool if you update MAX_ACCEPTS_PER_CYCLE.  (By Timothy St. Clair )</td></tr>\n</tbody></table>", "type": "enhance", "last_change": "2010-Jul-15 23:18", "status": "resolved", "created": "2010-Apr-14 08:53", "fixed_version": "2010-Apr-14 08:53", "broken_version": "v070400", "priority": "3", "subsystem": "Daemons", "assigned_to": "tstclair", "derived_from": "", "creator": "tstclair", "rust": "", "customer_group": "other", "visibility": "public", "notify": "matt@cs.wisc.edu, dan@hep.wisc.edu, tstclair@redhat.com", "due_date": "20100421"}
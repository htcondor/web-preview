{"id": 4001, "title": "Ticket #4001: Blocking in condor_write", "description": "<blockquote>\nJames pointed me to a dying CMS schedd and I found something interesting.  There are multiple \"gaps\" in the log where there is no activity (this is on a schedd which averages many hundred log messages per minute).\n\n<p>After the gaps, there is always this message:\n\n</p><p></p><div class=\"verbatim\">\n<pre>10/21/13 22:00:10 (pid:5304) condor_write(): Socket closed when trying to write 29 bytes to &lt;$IP_V4_ADDRESS:37107&gt;, fd is 20\n</pre></div>\n\n\n<p>The message immediately before varies - always in a different part of the code.  To me, this is strong evidence that we are seeing blocking in condor_write.\n\n</p><p>The call to the selector in that function is approximately this sequence (stripping out error handling and other conditionals):\n\n</p><p></p><div class=\"verbatim\">\n<pre>selector.execute();\nselector.has_ready();\nselector.fd_ready( fd, Selector::IO_READ ) ;\nnro = recv(fd, tmpbuf, 1, MSG_PEEK); // this blocks\n</pre></div>\n\n\n<p>If the recv blocks but eventually gets data, you can have this:\n\n</p><p></p><div class=\"verbatim\">\n<pre>10/21/13 22:31:19 (pid:16982) condor_write() failed: send() 4096 bytes to &lt;$IPV4_ADDRESS:43363&gt; returned -1, timeout=20, errno=32 Broken pipe.\n</pre></div>\n\n\n<p>In our case, the former error message occurred after the schedd blocked for 15 minutes; the latter, 25 minutes.  Blocking here was deadly - a huge number of jobs failed out immediately afterwards and the schedd strangled itself to death (well, into swap...).\n\n</p><p>Note that the Linux man page explicitly says that if select() returns a FD as ready to read, it DOES NOT mean that subsequent read/recv will be non-blocking, even for a single byte!  If you want to have non-blocking IO, then you must mark the socket as non-blocking.\n\n</p><p>The simplest solution is to add a non-blocking flag for the duration of this function.\n\n</p><p>I'm not totally convinced we need to ever select on read.  Why we do this is lost to history.</p></blockquote>", "remarks": "<blockquote>\n<em>2013-Oct-22 13:58:40 by tannenba:</em> <br/>\n\nIn HTCondor CEDAR, when the timeout(x) method is invoked on a tcp socket and x is greater than 0, the socket is put into non-blocking mode.  In your above log snippet, note that condor_write states the timeout is 20 - and thus this socket should already have been placed into non-blocking mode.   Thus either your guess about what is causing the blocking is incorrect, or somehow the socket ended up with a non-zero cedar timeout and escaped having it set to non-blocking (maybe some relatively 'new' code path involving the shared port service or?).\n\n<p>Do you have full logs someplace?  Hard to understand how the timeout could be 20 and yet the socket is not already in non-blocking state...\n\n</p><p></p><hr/>\n<em>2013-Oct-24 10:56:55 by jfrey:</em> <br/>\n\nIn some of the instances where a large gap in time is followed by failure in condor_write(), the failure is in a forked child process. The main schedd's pid is 5304. In the logs that Brian sent to Todd out-of-band, I see two cases where a large gap in time is followed by a condor_write() failure in the main schedd process. In both cases, the socket is closed, detected by the read field from select().\n\n<p>I'm not convinced that the large gaps are caused by condor_write() blocking on a recv(). As Todd said, those fds should be be marked non-blocking. I suspect those writes were just the next thing the schedd tried to do after becoming unblocked.\n\n</p><p></p><hr/>\n<em>2013-Oct-24 15:18:07 by bbockelm:</em> <br/>\n\nAh - any alternate theory then?  Certainly something is causing the main schedd to be blocked for awhile.\n\n<p>Brian\n\n</p><p></p><hr/>\n<em>2013-Oct-24 15:45:10 by sfiligoi:</em> <br/>\n\nMy (possibly naive) interpretation of the incident is that it was due to Schedd's memory fragmentation, paired with heavy spwapping.\n\n<p>The schedd was 3G in size, and my guess is that (almost) every single page of those 3G was needed by the schedd to do any work. So when 20 childs forked over the span of a few minutes, the updates made a large fraction of the pages dirty, triggering heavy swapping activity.\n\n</p><p>At that point, it started to take forever for the schedd to do any work, resulting in long gaps in the logs until the swapping died out.\n(and here my theory is that it was due to the shadows dying after hitting the timeout in communication with the schedd)\n\n</p><p>PS: Don't have detailed facts to back it up... just my interpretation of the situation from being logged in at that time.\n\n</p><p></p><hr/>\n<em>2014-Feb-05 14:55:39 by jfrey:</em> <br/>\n\nIgor reports that they limited MAX_JOBS_RUNNING to reduce memory usage and haven't seen this problem since then. I am resolving this ticket. It can be reopened if the problem recurs.</blockquote>", "derived_tickets": "", "attachments": "", "check_ins": "", "type": "incident", "last_change": "2014-Feb-05 14:55", "status": "resolved", "created": "2013-Oct-21 21:11", "fixed_version": "2013-Oct-21 21:11", "broken_version": "v080000", "priority": "2", "subsystem": "DaemonsSubmitNode", "assigned_to": "jfrey", "derived_from": "", "creator": "bbockelm", "rust": "", "customer_group": "cms", "visibility": "public", "notify": "bbockelm@cse.unl.edu, sfiligoi@fnal.gov, tannenba@cs.wisc.edu", "due_date": ""}
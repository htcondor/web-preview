{"id": 3233, "title": "Ticket #3233: condor_startd segfaults on work fetch", "description": "<blockquote>\nstartds, perhaps just with partitionable slots are crashing in chtc.  Stack trace is:\n\n<p></p><div class=\"code\">\n<pre class=\"code\">Stack dump for process 29329 at timestamp 1348001600 (17 frames)\n/usr/lib64/condor/libcondor_utils_7_9_0.so(dprintf_dump_stack+0x58)[0x2b286a0492e8]\n/usr/lib64/condor/libcondor_utils_7_9_0.so[0x2b286a03e322]\n/lib64/libpthread.so.0[0x322f60ebe0]\n/lib64/libpthread.so.0(read+0x10)[0x322f60d9b0]\n/usr/lib64/condor/libcondor_utils_7_9_0.so(_condor_full_read+0x2e)[0x2b286a01cd6e]\n/usr/lib64/condor/libcondor_utils_7_9_0.so(DaemonCore::Create_Process(char const*, ArgList const&amp;, priv_state, int, int, Env const*, char const*, FamilyInfo*, Stream**, int*, int*, int, __sigset_t*, int, unsigned long*, int*, char const*, MyString*, FilesystemRemap*, long)+0xdfa)[0x2b286a133f9a]\n/usr/lib64/condor/libcondor_utils_7_9_0.so(HookClientMgr::spawn(HookClient*, ArgList*, MyString*, priv_state, Env*)+0x1e1)[0x2b286a145331]\ncondor_startd(FetchClient::startFetch()+0x7e)[0x45fd5e]\ncondor_startd(StartdHookMgr::invokeHookFetchWork(Resource*)+0x18)[0x460848]\ncondor_startd(ResState::eval()+0x303)[0x45ab93]\ncondor_startd(ResMgr::walk(void (Resource::*)())+0xc4)[0x440144]\ncondor_startd(ResMgr::eval_all()+0x4e)[0x4401ee]\n/usr/lib64/condor/libcondor_utils_7_9_0.so(TimerManager::Timeout(int*, double*)+0x182)[0x2b286a14ce92]\n/usr/lib64/condor/libcondor_utils_7_9_0.so(DaemonCore::Driver()+0x56a)[0x2b286a13a81a]\n/usr/lib64/condor/libcondor_utils_7_9_0.so(dc_main(int, char**)+0xfe1)[0x2b286a150c61]\n/lib64/libc.so.6(__libc_start_main+0xf4)[0x322ee1d994]\ncondor_startd(CronJob::Reaper(int, int)+0x179)[0x422c89]\n\n</pre></div>\n\n\n<p>Machines with this problem include e061 and e063, which have core dumps in the log directory on chtc.\n\n</p><p>I didn't think that boinc work fetch was enabled in chtc, looks like workfetch boinc jobs are having problems</p></blockquote>", "remarks": "<blockquote>\n<em>2012-Sep-20 13:20:45 by adesmet:</em> <br/>\n\nPossibly related incidents:\n\n<p></p><div class=\"code\">\n<pre class=\"code\">e004/MasterLog:09/14/12 19:55:34 The STARTD (pid 978) was killed because it was no longer responding\nc034/MasterLog:09/15/12 02:09:25 The STARTD (pid 18697) was killed because it was no longer responding\nc022/MasterLog:09/17/12 06:42:06 The STARTD (pid 25640) was killed because it was no longer responding\nc039/MasterLog:09/17/12 08:41:18 The STARTD (pid 17855) was killed because it was no longer responding\nc042/MasterLog:09/17/12 11:41:59 The STARTD (pid 22641) was killed because it was no longer responding\ne061/MasterLog:09/18/12 15:56:30 The STARTD (pid 29329) was killed because it was no longer responding\ne056/MasterLog:09/18/12 16:19:42 The STARTD (pid 30233) was killed because it was no longer responding\ne063/MasterLog:09/18/12 17:42:39 The STARTD (pid 20170) was killed because it was no longer responding\n</pre></div>\n\n\n<p></p><hr/>\n<em>2012-Sep-20 13:26:37 by adesmet:</em> <br/>\n\ne061 and e063 have very similar backtraces.\n\n<p>e056 does not:\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">#0  0x0000003916c0eaad in raise () from /lib64/libpthread.so.0\n#1  0x00002b67a52d435f in ?? () from /usr/lib64/condor/libcondor_utils_7_9_0.so\n#2  &lt;signal handler called&gt;\n#3  0x0000000000446ed2 in Resource::compute(int) ()\n#4  0x000000000043cde2 in ResMgr::walk(void (Resource::*)(int), int) ()\n#5  0x000000000043fe08 in ResMgr::compute(int) ()\n#6  0x00000000004401ce in ResMgr::eval_all() ()\n#7  0x00002b67a53e2e92 in TimerManager::Timeout(int*, double*) ()\n   from /usr/lib64/condor/libcondor_utils_7_9_0.so\n#8  0x00002b67a53d081a in DaemonCore::Driver() () from /usr/lib64/condor/libcondor_utils_7_9_0.so\n#9  0x00002b67a53e6c61 in dc_main(int, char**) () from /usr/lib64/condor/libcondor_utils_7_9_0.so\n#10 0x000000391641d994 in __libc_start_main () from /lib64/libc.so.6\n#11 0x0000000000422c89 in _start ()\n</pre></div>\n\n\n<p></p><ul>\n<li>09/18/12 14:30:02 Last log message from STARTD\n</li><li>09/18/12 16:11:00 Master Sent SIGTERMs STARTD to cope with clock skew\n</li><li>09/18/12 16:19:32 Master detects STARTD hung, kills it\n</li><li>09/18/12 16:19:42 core.30233 created\n</li><li>09/18/12 16:19:42 Master reports STARTD dead\n</li></ul>\n\n<p></p><hr/>\n<em>2012-Sep-20 14:42:23 by adesmet:</em> <br/>\n\nNosing around in other dumps, there are a number of hard to explain crashes.  Sadly, this is a stripped binary, so I can't look closely.  Current guess: libpthread is detecting internal corruption and abort()ing.\n\n<p></p><hr/>\n<em>2012-Sep-20 17:28:20 by adesmet:</em> <br/>\n\nstartds have crashed about 150 times in CHTC's e0* and c0* machines since the August 1st.  47 of them look similar to this stack trace.  These are worrying numbers, but doesn't immediately fincer the startd as a particular problem.\n\n<p></p><hr/>\n<em>2012-Sep-20 21:11:01 by adesmet:</em> <br/>\n\nResearch so far is collected at: /p/condor/workspaces/adesmet/startd-crash.tar.gz . In particular it contains backtraces for all startd core dumps since the beginning of August and log excerpts.\n\n<p>I don't see a clear correlation to partitionable slots and these aborts. Most of the machines that crashed do have partitionable slots, but 4 (c022, c032, c034, and c042) don't.\n\n</p><p></p><hr/>\n<em>2012-Sep-24 14:09:18 by adesmet:</em> <br/>\n\nAttached stack-summaries, a summary of stack dumps on crashed startds on CHTC e0* and c0* machines, September 1, 2012 through September 20, 2012.\n\n<p>Fetchwork does leak the pack, responsible for a comfortable majority.  But there are other sources.  And while a failing in read() while spawning fetchwork does have the plurality, there are other crashes in different parts of fetchwork.\n\n</p><p>While there are exceptions, these are overwhelmingly exceptions from SIGABORT, and from libpthread (read() and write()) or libc (select()).  This suggests memory corruption that libc detects and panics over.\n\n</p><p>Will see if infrastructure will let me replace a startd with one running under valgrind, let it run a day or so, and see if anything interesting pops up.  Should also run on my own system with a trivial workload as a basic smoke test.\n\n</p><p></p><hr/>\n<em>2012-Sep-25 15:37:20 by adesmet:</em> <br/>\n\nNo immediate results or obvious evidence of corruption on local tests.  Possible regression of clone() was considered; code appears to disable clone in the startd as expected.\n\n<p>Immediate plans: ask infrastructure to turn off workfetch. Revisit in a week or two and see if the crash rate has significantly changed.</p></blockquote>", "derived_tickets": "", "attachments": "<blockquote>\n<ul>\n<li><a href=\"attach_get/637/stack-summaries\">stack-summaries</a>\n18156 bytes added by adesmet on 2012-Sep-24 19:04:38 UTC.\n<br/>\nSummaries of stacks from crashes startds in CHTC's e0* and c0* machines, Sep 1, 2012-Sep 20, 2012<br/>\n</li></ul>\n</blockquote>", "check_ins": "", "type": "todo", "last_change": "2013-May-29 16:00", "status": "stalled", "created": "2012-Sep-19 14:11", "fixed_version": "2012-Sep-19 14:11", "broken_version": "v070900", "priority": "3", "subsystem": "Daemons", "assigned_to": "adesmet", "derived_from": "", "creator": "gthain", "rust": "", "customer_group": "chtc", "visibility": "public", "notify": "", "due_date": ""}
{"id": 474, "title": "Ticket #474: BNL pilot job disaster when Condor-G started submitted to many sites", "description": "<blockquote>\nWe are having a problem on BNL condor-g submit host.\n\n<p>We have seen slowness of pilot job submission to BNL T1 site after reconfiguring the PANDA autopilot to submit from just BNL to BNL, US T2, and a few overseas sites. Condor-G could only keep ~500 slots out of ~1500 slots @ BNL busy. When killing and restarting gahp server, it ramped up then slowed down again.  When reconfiguring the PANDA autopilot to only submit to BNL, Condor-G was again able to keep all 1500 slots busy.\n\n</p><p>Todo\n\n</p><p></p><ol>\n<li><em>Investigate</em> : Look at how long individual GAHP\ncommands are taking round trip?  Perhaps the GRAM non-blocking interface is not really non-blocking all the way down to the GSI level, and for instance, cross-atlantic GSI round-trips are killing the throughput of the GAHP.  [Jaime]\n</li><li><em>Code</em> : Create option for a separate gridmanager per site <span class=\"ticket\"><a class=\"resolved\" href=\"tktview?tn=475\" title=\"Option to run w/ one gridmanager per remote site.\">#475</a></span> [Todd]\n</li><li><em>Investigate</em> : What is going on re the reported 30 min of no grid_monitor updates on the submit host after Xin restarted the gahp? [Jaime]\n</li><li><em>Investigate</em> : What is going on re the 95% of the 1000 non-BNL jobs\nreporting status \"DONE\" w/ <code>condor_q -globus</code> ? [Jaime]\n</li><li><em>Todo</em> : Improve out ability to monitor USATLAS pilot health @ UW <span class=\"ticket\"><a class=\"resolved\" href=\"tktview?tn=477\" title=\"Improve monitoring BNL pilot submission at UW\">#477</a></span> [Jaime]\n</li></ol>\n\n<p><em>Note</em>: Xin ruled out the possibility of the globus select() bug, since he observed via strace the GAHP correctly adding the listen sockets into calls to select() during the slowdown.</p></blockquote>", "remarks": "<blockquote>\n<em>2009-May-11 12:10:39 by jfrey:</em> <br/>\n\nOn item 4, all of the jobs have <span class=\"quote\">LeaveJobInQueue</span> set to <span class=\"quote\">JobStatus</span> == 4. That will cause completed jobs to stay in the queue until the client removes them.\n\n<p></p><hr/>\n<em>2009-May-11 13:26:28 by jfrey:</em> <br/>\n\nOn item 1, a quick comparison of time taken to execute GAHP commands on Saturday and today show very similar behavior. I'm assuming today's behavior is more-or-less normal.\n\n<p>Todd had a theory that the GSI code in the GAHP may be insufficiently non-blocking and authentication on over-seas connections could be delaying connections to the BNL cluster. That doesn't appear to be the case, though.\n\n</p><p></p><hr/>\n<em>2009-May-14 17:12:35 by jfrey:</em> <br/>\n\nItem 3: gridgk04.racf.bnl.gov is using jobmanager-managedfork (i.e. GRAM fork jobs become Condor local universe jobs). During the meltdown, grid monitor jobs would sit idle for up to 13 minutes before Condor started them. Back on the submit machine, if the jobs didn't start reporting after 5 minutes, the gridmanager would give up on them and try a new job submission. The old job stayed in the queue, but failed immediately on startup.\n\n<p>The 30 to 50 minute gaps where the grid monitor didn't report anything consisted of a string of grid monitor jobs that each took more than 5 minutes to begin execution. I'll make sure to fix this as part of our grid-monitor improvements (<span class=\"ticket\"><a class=\"defer\" href=\"tktview?tn=429\" title=\"Grid monitor improvements for BNL ATLAS\">#429</a></span>).\n\n</p><p></p><hr/>\n<em>2009-May-14 18:21:11 by jfrey:</em> <br/>\n\nItem 3: Even now, gridgk04 is being very slow to start grid monitor jobs. 8 are sitting idle in the queue, the oldest submitted more than 15 minutes ago. Here are some relevant attributes from the schedd ad:\n\n<p></p><pre>  TotalIdleJobs = 37\n  TotalRunningJobs = 1811\n  TotalJobAds = 1855\n  StartLocalUniverse = TotalLocalJobsRunning &lt; 50 || GridMonitorJob =?= TRUE\n  MaxJobsRunning = 5000\n</pre>\n\n<p>All of the grid monitor jobs have <span class=\"quote\">GridMonitorJob</span> set to True. And there are indeed roughly 1500 condor_shadow processes.\n\n</p><p>Ah, I think I see what's going on. JOB_START_DELAY is set to 2 and JOB_START_COUNT is unset. Thus, the schedd is starting one job every two seconds. So there's a 15 minute backlog of jobs matched to machines waiting for a shadow so they can execute. condor_status shows 212 slots in Claimed/Idle right now.\n\n</p><p>I think we need to get shadows started more quickly. How about making the following changes in the Condor config file on gridgk04:\n\n</p><p></p><pre>  JOB_START_DELAY = 1\n  JOB_START_COUNT = 5\n</pre>\n\n<p></p><hr/>\n<em>2009-May-22 14:00:36 by jfrey:</em> <br/>\n\nI ran some simple tests to see if jobs submitted to a slow site can negatively impact the throughput of jobs to a fast site in Condor-G. In brief, adding jobs to a second site slowed down the throughput of jobs to the first site by about 10%. Loading the second site to make it slow (emulating trans-oceanic links) had no additional effect.\n\n<p>I set up three machines in our department, client, server-A and server-B. In each test run, I submitted 1000 thousand 'sleep 300' jobs via Condor-G (on client) to the fork jobmanager of one or both servers.\nTo slow down server-B, I ran several processes in tight loops, and ran the GRAM software at a nice level of 19. Submitting one /bin/date job via globusrun took roughly twice as long on server-B when loaded.\nThe Condor-G throttles were set as follows:\n</p><div class=\"code\">\n<pre class=\"code\">GRIDMANAGER_MAX_SUBMITTED_JOBS_PER_RESOURCE = 2500\nGRIDMANAGER_MAX_PENDING_SUBMITS_PER_RESOURCE = 1000000\nGRIDMANAGER_MAX_JOBMANAGERS_PER_RESOURCE = 25\nGRIDMANAGER_MAX_PENDING_REQUESTS = 500\n</pre></div>\n\n\n<p>Here are the numbers:\n\n</p><p></p><div class=\"code\">\n<pre class=\"code\">Run 1\nserver-A only\n38 minutes for all server-A jobs to complete\n\nRun 2\nserver-A and server-B, both unloaded\n42 minutes for all server-A jobs to complete\n53 minutes for all server-B jobs to complete\n\nRun 3\nserver-A and server-B, server-B loaded\n42 minutes for all server-A jobs to complete\n83 minutes for all server-B jobs to complete\n</pre></div>\n\n\n<p>There are many differences between this test and the situation observed at BNL, including the number of additional sites and the pattern of job submissions.</p></blockquote>", "derived_tickets": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"resolved\" href=\"tktview?tn=475\" title=\"Option to run w/ one gridmanager per remote site.\">#475</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nOption to run w/ one gridmanager per remote site.</td></tr>\n<tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"resolved\" href=\"tktview?tn=477\" title=\"Improve monitoring BNL pilot submission at UW\">#477</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nImprove monitoring BNL pilot submission at UW</td></tr>\n</tbody></table>", "attachments": "", "check_ins": "", "type": "incident", "last_change": "2010-Jan-31 08:23", "status": "resolved", "created": "2009-May-11 11:53", "fixed_version": "2009-May-11 11:53", "broken_version": "v070301", "priority": "1", "subsystem": "Grid", "assigned_to": "jfrey", "derived_from": "", "creator": "tannenba", "rust": "s7802", "customer_group": "atlas", "visibility": "public", "notify": "tannenba@cs.wisc.edu", "due_date": ""}
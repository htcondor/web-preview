{"id": 3920, "title": "Ticket #3920: Running the same Dag Test in parallel say N instances at once", "description": "<blockquote>\nMany of the current dag tests can only have one instance running at once  because of either node log files or dag lock files clashing with siblings.</blockquote>", "remarks": "<blockquote>\n<em>2013-Sep-16 12:15:04 by wenger:</em> <br/>\n\nAs Bill notes above, I think there are two (at least) issues with running more than one instance of a given DAG test at the same time:\n<ul>\n<li>Node job log file(s).\n</li><li>DAG lock files.\n</li></ul>\n\n<p>For the majority of the tests that use the default node job log file, the log file issue could be overcome by specifying a unique default log file name (maybe based on the test job's PID).  For the tests that still use the old-style (individually-specified) node job log files, we'd have to take a different approach.  The only thing I can think of offhand is to have the test script create a unique subdirectory for each instance, and actually run the test in that subdirectory (and have all of the node job log files in the per-test subdirectory).  (Note that you can't use macros in the log file names in this case.)\n\n</p><p>The lock file is currently specified as an argument in the .condor.sub file, but you can't change it via condor_submit_dag.  It wouldn't be hard to add a -lockfile argument to condor_submit_dag to allow this.  (We also could set DAGMAN_ABORT_DUPLICATES to false, but I think that would goof up a second or third instance of almost all of the tests.)</p></blockquote>", "derived_tickets": "", "attachments": "", "check_ins": "", "type": "enhance", "last_change": "2013-Sep-16 12:15", "status": "new", "created": "2013-Sep-16 12:01", "fixed_version": "2013-Sep-16 12:01", "broken_version": "", "priority": "4", "subsystem": "Tests", "assigned_to": "bt", "derived_from": "#3594", "creator": "bt", "rust": "", "customer_group": "other", "visibility": "public", "notify": "johnkn@cs.wisc.edu, wenger@cs.wisc.edu", "due_date": ""}
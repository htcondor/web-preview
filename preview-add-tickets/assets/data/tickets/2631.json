{"id": 2631, "title": "Ticket #2631: Livelocked Metronome jobs.", "description": "<blockquote>\nWe got bit by this again at the tail end of last week.  (2011-11-06.)\n\n<p>Background: Metronome run consists of a minimum of four jobs.  Three of them are persistent: one DAGMan to watch the monitor, the monitor itself, and the DAGMan managing the worflow of the Metronome job proper.  All three of these run in the scheduler universe.  The submit-side jobs in the workflow all run in the local universe.  We split them up this way because we can't limit either type of job on what we'd like, which is I/O; instead, we limit by number, and have a much larger limit for scheduler universe jobs than local universe jobs, which tend to much heavier-weight.  Of course, the remote tasks run in the vanilla universe.  These last two types of job are (of course) transient, but are the actual work we're trying to get done.\n\n</p><p>The problem: we can end up in a state where no progress can be made.  This happens when the queue starts to get very large because machines are down.  What we see are a large number of idle scheduler universe jobs and (eventually) nothing but idle vanilla universe jobs.  With enough idle Metronome runs (at most one per idle vanilla universe jobs), there will be enough running jobs in the scheduler universe -- ones that aren't doing anything, because they're waiting for vanilla universe jobs that won't run until we bring the machine back up -- to prevent any new Metronome job from getting started.  (Its DAGMans will be submitted and stay idle.)\n\n</p><p>(One subtlety is that if the monitor and its DAG start, their presence as running scheduler jobs can prevent the corresponding run DAG from starting, leading to a more-formal kind of livelock.  We would like to be able to group these jobs to make sure that doesn't happen.)\n\n</p><p>We don't want to lose the jobs -- because once we get the machine back up, they can start running -- but we also don't want to waste time leaving execute nodes idle.\n\n</p><p>I'm just thinking out loud here, but I think what we want as a solution -- aside from limiting scheduler universe jobs by load -- would be some way to put the scheduler universe jobs corresponding to a given vanilla universe job on hold when the vanilla universe job has been idle for long enough (for say, eight hours).  That would give some other Metronome job \"room\" to start running and try to get some of its own vanilla universe jobs through.  With a <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=PeriodicRelease\" title=\"Periodic Release\">PeriodicRelease</a></span> expression, the held jobs could reenter the queue to check if the machine has been repaired.</p></blockquote>", "remarks": "<blockquote>\n<em>2011-Nov-16 18:24:15 by tlmiller:</em> <br/>\n\nToddT, ToddM, and BeckyG discussed this for a while.  We came up with two ideas that we like.\n\n<p>One idea daddresses the problem by allowing us to remove the numeric limit on scheduler universe jobs by directly limiting I/O.  We presently use DAGMan node classes to limit io-intensive parts of the run, but that only works withing the DAG.  The proposal is to leverage the token-dispensing code already present in the schedd for limiting Condor sandbox transfer, having DAGMan act on behalf of each node (including pre- and post- scripts) whose class is concurrency-limited.  (With some sort of declaration of which token each class tries to grab, if any.)  Doing this in DAGMan means that we can get it right once, instead of all of our users trying to reimplement in shell script.\n\n</p><p>Another option, that should be easy to implement, but could be very dangerous, would be to add <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=ClassAd\" title=\"Class Ad\">ClassAd</a></span> functions getJobAttribute( jobID, attribute ) and setJobAttribute( jobID, attribute, value ).  We could then use some periodic attribute in the vanilla universe jobs to place the corresponding scheduler universe jobs on hold, and give some other job a chance to run.\n\n</p><p>The first idea requires a little more work, but is much less dangerous and probably much easier to use.</p></blockquote>", "derived_tickets": "", "attachments": "", "check_ins": "", "type": "todo", "last_change": "2011-Nov-16 18:25", "status": "new", "created": "2011-Nov-10 17:55", "fixed_version": "2011-Nov-10 17:55", "broken_version": "", "priority": "4", "subsystem": "", "assigned_to": "", "derived_from": "", "creator": "tlmiller", "rust": "", "customer_group": "batlab", "visibility": "public", "notify": "", "due_date": ""}
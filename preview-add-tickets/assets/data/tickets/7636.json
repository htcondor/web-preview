{"id": 7636, "title": "Ticket #7636: condor_ssh_to_job: Have nsenter use env of container process", "description": "<blockquote>\nWhen using condor_ssh_to_job to a job running inside a container, the environment gotten is the one of the starter:\n\n<p><a class=\"external\" href=\"https://github.com/htcondor/htcondor/blob/422cf6cb36de091c7624d3b69a0834124efaaba6/src/condor_starter.V6.1/os_proc.cpp#L1203\">https://github.com/htcondor/htcondor/blob/422cf6cb36de091c7624d3b69a0834124efaaba6/src/condor_starter.V6.1/os_proc.cpp#L1203</a>\n\n</p><p>When singularity is invoked with the: --nv option, singularity setups a special environment for the container, to link a bunch of GPU libraries that come from the host. This feature is lost if condor just gets the starter environment.\n\n</p><p>Currently, a workaround is needed after entering the shell via condor_ssh_to_job in order to put PTY and the environment variables back (see below), but this is something the user would need to know and do. It would be good if condor could grab the for example, the environment (/proc/PID/environ) of the first child of the process spawned by the starter, rather than the starter itself for this use-case.\n\n</p><p></p><pre>     # Work around missing PTY\n     script /dev/null\n     # Re-set home directory (of course, this needs to be adapted):\n     export HOME=/jwd\n     # Re-source /etc/profile:\n     source /etc/profile\n     # Fixup TERM\n     export TERM=linux\n     # And here's the magic trick for CUDA:\n     export LD_LIBRARY_PATH=/.singularity.d/libs/</pre>\n</blockquote>", "remarks": "<blockquote>\n</blockquote>", "derived_tickets": "", "attachments": "", "check_ins": "", "type": "enhance", "last_change": "2020-May-08 08:51", "status": "new", "created": "2020-May-08 07:32", "fixed_version": "2020-May-08 07:32", "broken_version": "", "priority": "4", "subsystem": "", "assigned_to": "gthain", "derived_from": "", "creator": "khurtado", "rust": "", "customer_group": "other", "visibility": "public", "notify": "", "due_date": ""}
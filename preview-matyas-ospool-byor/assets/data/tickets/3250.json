{"id": 3250, "title": "Ticket #3250: Create script to preserve DAGs across 7.8.5 upgrade at LIGO", "description": "<blockquote>\nWhen LIGO upgrades to 7.8.5 their running DAGs will hit the problem noted in <span class=\"ticket\"><a class=\"defer\" href=\"/wiki-archive/tickets/?ticket=3207\" onclick=\"get_ticket_and_populate_wrapper('3207'); return false;\" title=\"Jobs in scheduler universe ignore OnExitRemove\">#3207</a></span>.  (Subsequent upgrades will be okay.)\n\n<p>We want to create a script to automate the re-starting of DAGs in recovery mode after the upgrade.</p></blockquote>", "remarks": "<blockquote>\n<em>2012-Oct-03 14:35:58 by wenger:</em> <br/>\n\nMy first thought on this is that they'll have to run a script before the upgrade to gather information on all of the running DAGs, and that script will create another script that they'll have to run after the upgrade to re-start things.\n\n<p>A few issues I've thought of:\n\n</p><p></p><ul>\n<li>What if a DAG finishes between the running of the first script and the Condor upgrade?  I guess the post-upgrade script could look for the lock files and not re-start a DAG if it doesn't find one.\n\n<p></p></li><li>Will each user who has DAGs run the script themselves, or do we have to set things up to just have the script run by one user?  In the latter case, it would have to be run as root so it can su to the right user to re-submit the DAGs.\n\n<p></p></li><li>Do the LIGO people use command-line flags like -usedagdir on the condor_submit_dag command line?  That might make things trickier.\n\n<p></p></li><li>Are all of their DAGs submitted on one machine, or do they submit from a bunch of machines?  The latter would obviously add complication...\n</li></ul>\n\n<p></p><hr/>\n<em>2012-Oct-03 14:41:29 by nwp:</em> <br/>\n\nHere is the plan according to Stuart.\n<div class=\"code\">\n<pre class=\"code\">Date: Wed, 26 Sep 2012 17:38:42 -0700\nFrom: Stuart Anderson\nTo: Nathan Panike\n\nNathan,\n\nWhen Condor 7.8.5 is released and we upgrade the LIGO Condor pools we will give\nour users the option of running condor_rm on any running DAGMan jobs prior to the\nupgrade (and submitting the rescue dag after the upgrade) versus running\ncondor_submit &lt;DAGFile&gt;.condor.sub after the upgrade and potentially needing to\ndo the manual cleanup as described by Duncan.\n\nThanks.\n</pre></div>\n\n\n<p></p><hr/>\n<em>2012-Oct-03 16:16:41 by wenger:</em> <br/>\n\nHmm -- the restart script should probably also make sure that the relevant job (the DAGMan job) is no longer actually in the queue before clearing out the lock file and re-submitting...\n\n<p></p><hr/>\n<em>2012-Oct-03 16:18:54 by nwp:</em> <br/>\n\nThe whole point is that it is known not to be in the queue.\n\n<p></p><hr/>\n<em>2012-Oct-03 16:26:38 by wenger:</em> <br/>\n\nWell, I guess I'm thinking about a scenario like the user accidentally running the re-start script before the Condor upgrade actually happens, or something like that...\n\n<p>But it should be easy to add a check to make sure that's not the case.\n\n</p><p></p><hr/>\n<em>2012-Oct-04 09:26:43 by nwp:</em> <br/>\n\nIndeed, here is a script that checks the history before resubmitting the DAG:\n<div class=\"code\">\n<pre class=\"code\">cat&lt;&lt;PRINT_FUNC\ncondor_history -const 'JobStatus=?=4 &amp;&amp; ExitStatus=?=0 &amp;&amp; isUndefined(ExitCode)' -format '%d.' ClusterId -format '%d\\n' ProcId &gt; suspicious-jobs\n\nresubmit_dag () {\n    if grep -q \\$1 suspicious-jobs; then \\\n                su - \\$5 -c \"echo -n &gt; \\$4; cd \\$2 &amp;&amp; condor_submit \\$3\"\n        else\n                echo\n                echo \"**************************************************\"\n                echo \"DID NOT FIND JOBID \\$1 IN CONDOR HISTORY\"\n                echo \"**************************************************\"\n                echo\n        fi\n}\n\nPRINT_FUNC\n\ncondor_q -const 'regexp(\"condor_dagman\",Cmd,\"i\")' | \\\nsed -n '/^\\s*[0-9]\\+\\./p' | \\\nawk '{print $1}' | \\\nwhile read jobid; do\n    dir=$(condor_q $jobid -f '%s' Iwd)\n    dag=$(condor_q $jobid -f '%s' UserLog | \\\n        sed 's/\\.dagman\\.log$//').condor.sub\n        lock=$(echo $dag | sed 's/\\.condor\\.sub/\\.lock/')\n        username=$(condor_q $jobid -f '%s' Owner)\n    echo \"resubmit_dag $jobid $dir $dag $lock $username\";\ndone\n</pre></div>\n\n\n<p></p><hr/>\n<em>2012-Oct-04 13:02:00 by wenger:</em> <br/>\n\nThis looks good except that the resubmit_dag script needs to clear out the lock file.  (That's the best thing to do, as opposed to trying to stick some different PID into it -- if you just truncate the lock file to zero size, the new DAGMan will definitely continue, as opposed to possibly deciding that the old DAGMan still exists.)\n\n<p>I was able to duplicate the problem by running a 7.8.4 personal Condor and doing a condor_restart -- I think that's very close to the upgrade scenario.\n\n</p><p>Nathan -- I'm going to assign this ticket back to you, since it looks like you're close to having something working.\n\n</p><p></p><hr/>\n<em>2012-Oct-04 13:18:37 by nwp:</em> <br/>\n\nThe script above clears out the lock file now.\n\n<p></p><hr/>\n<em>2012-Oct-04 15:22:13 by wenger:</em> <br/>\n\nAnother issue I just thought of -- what if some node jobs are scheduler universe?  (Although when I test by doing a condor_restart a scheduler universe node job stays in the queue, even though DAGMan is removed -- I'm not sure what's going on with that.)\n\n<p></p><hr/>\n<em>2012-Oct-04 15:53:00 by nwp:</em> <br/>\n\nPer the discussion on CondorLIGO, the script will now use su to invoke <code>condor_submit</code> as the user.  Also, fix it so that we do not run <code>condor_history</code> for every job. Complain loudly if we do not find the job\nin the history, and do not restart the job in that case.  Also we ran across a case that when only a few jobs have run, the sed script does not print out any job lines; this is fixed.\n\n<p></p><hr/>\n<em>2012-Oct-04 15:55:18 by nwp:</em> <br/>\n\nIf the scheduler job stays in the queue, DAGMan should be fine, right?  The log file will say it is running and that will be correct. If the scheduler job exits with nonzero status, DAGMan should keep going until it cannot make any forward progress, then dump a rescue dag and exit: the user will resubmit the rescue dag.\n\n<p></p><hr/>\n<em>2012-Oct-18 16:20:40 by wenger:</em> <br/>\n\nDid some more testing on this, here's a modified script that looks for the lock files rather than the <span class=\"wiki\"><a class=\"missing\" href=\"wiki?p=ExitCode\" title=\"Exit Code\">ExitCode</a></span> to decide whether a DAG should be restarted.\n\n<p>To use this script, do the following:\n</p><ol>\n<li>Put the code into a file, say fixit.sh.\n</li><li>Run this command: \"bash fixit.sh &gt; restart.sh\"\n</li><li>Do the Condor upgrade\n</li><li>Run this command: \"bash restart.sh\"\n</li></ol>\n\n<p></p><div class=\"code\">\n<pre class=\"code\">cat&lt;&lt;PRINT_FUNC\ncondor_history -const 'JobStatus=?=4' -format '%d.' ClusterId -format '%d\\n' ProcId &gt; suspicious-jobs\n\nresubmit_dag () {\n    if grep -q \\$1 suspicious-jobs;\n    then\n        if [ -e \\$4 ]\n        then\n            echo \"Restarting DAGMan job \\$1\"\n            su - \\$5 -c \"echo -n &gt; \\$4; cd \\$2 &amp;&amp; condor_submit \\$3\"\n        else\n            echo\n            echo \"**************************************************\"\n            echo \"Lock file \\$4 not found\"\n            echo \"DAGMan job \\$1 apparently finished normally -- not restarting\"\n            echo \"**************************************************\"\n            echo\n        fi\n    else\n        echo\n        echo \"**************************************************\"\n        echo \"DID NOT FIND JOBID \\$1 IN CONDOR HISTORY -- not restarting\"\n        echo \"**************************************************\"\n        echo\n    fi\n}\n\nPRINT_FUNC\n\ncondor_q -const 'regexp(\"condor_dagman\",Cmd,\"i\")' | \\\nsed -n '/^\\s*[0-9]/p' | \\\nawk '{print $1}' | \\\nwhile read jobid; do\n    dir=$(condor_q $jobid -f '%s' Iwd)\n    dag=$(condor_q $jobid -f '%s' UserLog | \\\n        sed 's/\\.dagman\\.log$//').condor.sub\n        lock=$(echo $dag | sed 's/\\.condor\\.sub/\\.lock/')\n        username=$(condor_q $jobid -f '%s' Owner)\n    echo \"resubmit_dag $jobid $dir $dag $lock $username\";\ndone\n\n</pre></div>\n\n\n<p></p><hr/>\n<em>2012-Oct-18 16:24:21 by wenger:</em> <br/>\n\nThere are at least a couple of things to watch out for with this:\n<ol>\n<li>Once you've re-submitted DAGs this way, doing a condor_rm of the DAGMan job won't remove the node jobs (see <span class=\"ticket\"><a class=\"new\" href=\"/wiki-archive/tickets/?ticket=3278\" onclick=\"get_ticket_and_populate_wrapper('3278'); return false;\" title=\"Condor_rm of recovered DAG sometimes leaves node jobs running\">#3278</a></span>).\n</li><li>You need to make sure that su'ing to a given use correctly sets up everything for that user to run Condor (e.g., they don't have to do any manual fixing up of their path or anything like that).\n</li></ol>\n\n<p>If someone wants to run this as a normal user instead of root, it will all work just by changing 'su - \\$5' to 'bash' in the script.\n\n</p><p></p><hr/>\n<em>2012-Oct-18 16:28:25 by wenger:</em> <br/>\n\nOh, yes -- as a further clarification, this script is designed to be run as root, one time on each machine that any running DAGs have been submitted from.  It should take care of DAGs started by any users on the machine on which it is run.\n\n<p></p><hr/>\n<em>2012-Oct-18 17:18:23 by wenger:</em> <br/>\n\nAnother thing to watch out for:\n\n<p>Make sure you don't run the restart.sh script more than once -- if you\ndo that, you'll end up with multiple instances of DAGMan running the same\nDAG, and you'll have problems.\n\n</p><p></p><hr/>\n<em>2012-Nov-13 14:08:28 by nwp:</em> <br/>\n\nWe confirmed in the LIGO conference call on 13 November 2012 that this can be resolved.</blockquote>", "derived_tickets": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"new\" href=\"/wiki-archive/tickets/?ticket=3278\" onclick=\"get_ticket_and_populate_wrapper('3278'); return false;\" title=\"Condor_rm of recovered DAG sometimes leaves node jobs running\">#3278</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nCondor_rm of recovered DAG sometimes leaves node jobs running</td></tr>\n</tbody></table>", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "todo", "last_change": "2012-Nov-13 14:08", "status": "defer", "created": "2012-Oct-03 14:28", "fixed_version": "2012-Oct-03 14:28", "broken_version": "v070705", "priority": "5", "subsystem": "Dag", "assigned_to": "nwp", "derived_from": "#3207", "creator": "wenger", "rust": "", "customer_group": "ligo", "visibility": "public", "notify": "wenger@cs.wisc.edu, tannenba@cs.wisc.edu dabrown@physics.syr.edu anderson@ligo.caltech.edu tstclair@redhat.com,pcouvare@caltech.edu", "due_date": ""}
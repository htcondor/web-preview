{"id": 2011, "title": "Ticket #2011: Investigate Condor / GPU integration for LIGO", "description": "<blockquote>\n<ul>\n<li><code>DONE....</code> Contact Carsten\n</li><li><code>DONE....</code> Review Carsten's RFC on condor-users\n</li><li><code>DONE....</code> Give feedback on immediate possible improvements\n</li><li><code>DONE....</code> Update this ticket with info.\n</li><li><code>DONE....</code> (Long deadline due to other deadlines pressing and Condor Week) - Have another chat with Carsten, update this ticket with info.\n</li><li><code>????????</code> Ongoing discussion; some basic experimentation with CUDA/OpenCL to try and answer a few technical questions.\n</li><li><code>????????</code> Identify functionality that we can provide immediately.\n</li><li><code>????????</code> Identify places where Condor is can't provide that functionality.\n</li></ul>\n\n<p><span class=\"section\"></span></p><h2>Random notes</h2>\n\n<p><span class=\"subsection\"></span></p><h3>General implementation ideas</h3>\n\n<p></p><ul>\n<li>Manage GPUs as part of the resources partionable with dynamic slots\n<ul>\n<li>Current \"one dynamic slot per machine per negotiation cycle\" limit applies, but we may be working on that separately.\n</li><li>Suffers slot fragmentation problems.\n</li></ul>\n</li><li>Manage GPUs similar to existing concurrency limits (typically used for software licenses).  \"I have 4 GPUs available. Oh, someone just claimed one, now I have 3.\" Simpler model, with attendant loss of nuance.\n<ul>\n<li>Notably messy if a machine has mixed GPUs.\n</li><li>Current concurrency limits are global; this would need to be per machine.\n</li></ul>\n</li></ul>\n\n<p><span class=\"subsection\"></span></p><h3>Usage open questions</h3>\n\n<p></p><ul>\n<li>Do users want/need coallocation?  \"I need 3 GPUs and 2 CPUs\"?\n<ul>\n<li>Carsten reports that developers are currently interested 1 GPU and N CPUs, but expected interest in M GPUs and N CPUs eventually, especially if the functionality is available.\n</li></ul>\n</li><li>What information should Condor automatically detect and advertise?  Probably start with Carsten's configuration.\n<ul>\n<li>Some benchmarks?  Is there anything good?\n<ul>\n<li>Make it easy to add more benchmarks with Condor cron support!\n</li></ul>\n</li><li>Do a minimal smoke test? (Specifically mentioned by Carsten, but probably not important.)\n</li></ul>\n</li><li>Are pools likely to have mixes of GPUs?\n</li><li>Is a given machine likely to have different GPUs? (Say, replacement of failed parts when the original is no longer available.)\n</li><li>Can Condor help make GPU programs more portable or easier to start up by providing a \"GPU\" universe (or more likely feature) akin to the \"Java\" universe\"?  We might recompile GPU programs (although isn't this usually done at run time via the normal API), or provide paths to local libraries/binaries.\n</li></ul>\n\n<p><span class=\"subsection\"></span></p><h3>Technical open questions</h3>\n\n<p></p><ul>\n<li>Is is possible to hard limit a job to a particular GPU, to prevent misuse of GPUs as a result of malice or error?\n<ul>\n<li>Slot user accounts and permissions on Linux?\n</li><li>Carsten expressed interest in this.\n</li></ul>\n</li><li>Can we make it easier for a job to use the correct GPU?  Currently limited to passing in the GPU identifier (0-n) on the command line or in the environment.\n</li><li>Do we need to do any cleanup of a GPU job to ensure that old GPU programs aren't left running and old data isn't left on the card?  Who handles the cleanup?  If it's a library, we probably need to clean up to recover abnormal terminations.  If it's the OS, we can probably trust it.\n</li><li>Hybrid CPU/GPU devices (AMD's Fusion, surely others) need consideration.\n</li><li>AMD and nVidia's implementations may not co-exist.  Is it enough to have suitable library on disk and (maybe) set LD_LIBRARY_PATH for the job?  Might we need to relink the job with the suitable library?\n</li></ul>\n\n<p><span class=\"section\"></span></p><h2>Random notes</h2>\n\n<p>ziliang has previously worked with CUDA/nVidia and OpenCL/AMD (and possibly OpenCL/nVidia?) and has offered advice and assistance.\n\n</p><p><span class=\"section\"></span></p><h2>Carsten Aulbert</h2>\n<a class=\"external\" href=\"https://lists.cs.wisc.edu/archive/condor-users/2011-March/msg00121.shtml\">Carsten's configuration</a> runs a program that emits a section of condor_config file that advertises GPU information. It is run on machine boot, so it should be up-to-date assuming no one invents hot-swappable GPUs on us.  The relevant information is:\n\n<p><table border=\"1\" cellspacing=\"0\">\n<tbody><tr>\n<td>\nAttribute/Setting     </td>\n<td>\nExample</td>\n<td>\nMeaning</td>\n</tr>\n\n<tr>\n<td>\nSLOT1_GPU_CUDA_DRV    </td>\n<td>\n=3.20  </td>\n<td>\nDriver version (Requires CUDA runtime 2020 or later) </td>\n</tr>\n\n<tr>\n<td>\nSLOT1_GPU_CUDA_RUN    </td>\n<td>\n=3.20  </td>\n<td>\nRuntime version (Requires CUDA runtime 2020 or later) </td>\n</tr>\n\n<tr>\n<td>\nSLOT1_GPU_DEV         </td>\n<td>\n=0     </td>\n<td>\nDevice number, unique per GPU, as known to the CUDA API </td>\n</tr>\n\n<tr>\n<td>\nSLOT1_GPU_NAME        </td>\n<td>\n=\"Tesla C2050\"</td>\n<td>\nDevice name</td>\n</tr>\n\n<tr>\n<td>\nSLOT1_GPU_CAPABILITY= </td>\n<td>\n2.0    </td>\n<td>\nDevice's compute capability major.minor</td>\n</tr>\n\n<tr>\n<td>\nSLOT1_GPU_GLOBALMEM_MB</td>\n<td>\n=2687  </td>\n<td>\nDevice memory in MB (CUDA API provides in bytes, this is X/(1024*1024))</td>\n</tr>\n\n<tr>\n<td>\nSLOT1_GPU_MULTIPROC   </td>\n<td>\n=14    </td>\n<td>\nNumber of multiprocessors (Requires CUDA runtime 2000 or later)</td>\n</tr>\n\n<tr>\n<td>\nSLOT1_GPU_NUMCORES    </td>\n<td>\n=32    </td>\n<td>\nNumber of cores <em>per multiprocessor</em> (Requires CUDA runtime 2000 or later) <em>Is directly based on GPU_CAPABILITY</em>. A given GPU_CAPABILITY has a given number of cores per multiprocessor. </td>\n</tr>\n\n<tr>\n<td>\nSLOT1_GPU_CLOCK_GHZ   </td>\n<td>\n=1.15  </td>\n<td>\nClock in GHz (CUDA API provides kHz, this is X*1e-6f)</td>\n</tr>\n\n</tbody></table>\n</p><p>Carsten is interested in using dynamic slots with GPUs.  They are planning on moving the machines in question to dynamic slots anyway, and it would be nice if GPUs were part of the partitionable resources.</p></blockquote>", "remarks": "<blockquote>\n<em>2011-Apr-11 15:24:18 by tannenba:</em> <br/>\n\nPlease add the wisdom discovered to date to this ticket, thanks.\n\n<p></p><hr/>\n<em>2011-May-24 08:20:47 by tannenba:</em> <br/>\n\nAlan, is there something I am missing re why this ticket was created private?\n\n<p></p><hr/>\n<em>2011-May-24 11:59:15 by adesmet:</em> <br/>\n\nThe ticket began as LIGO/Carsten specific outreach. As it's turning more into a general \"GPU in Condor\" perhaps it should be opened. I'll need to check if Carsten cares if his name is plastered all over it.\n\n<p></p><hr/>\n<em>2012-May-02 10:11:34 by adesmet:</em> <br/>\n\nRandom notes on managing GPUs using CUDA:\n<ul>\n<li>CUDA_VISIBLE_DEVICES - Environment variable, comma separated list of which GPU indices will be visible to the process.  If you say \"CUDA_VISIBLE_DEVICES=0,3 ./myjob\" on a machine with 4 GPUs, myjob will only see 2 of them, GPUs 0 and 3.  Down side: easy to evade, even accidentally by clobbering your environment.\n</li><li>nvidia-smi and NVML (Nvidia Management Library) - Can query various states. Can set various states, most notably can tell a GPU to not share, but only run one program at a time.\n</li></ul>\n\n<p></p><hr/>\n<em>2014-Jan-24 10:13:05 by johnkn:</em> <br/>\n\nThe OpenCL equivalent of <code>CUDA_VISIBLE_DEVICES</code> is <code>GPU_DEVICE_ORDINAL</code></blockquote>", "derived_tickets": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"defer\" href=\"/wiki-archive/tickets/?ticket=2905\" onclick=\"get_ticket_and_populate_wrapper('2905'); return false;\" title=\"RFE: adding local resource limits to partionable slots\">#2905</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nRFE: adding local resource limits to partionable slots</td></tr>\n</tbody></table>", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "todo", "last_change": "2014-Jan-24 10:13", "status": "stalled", "created": "2011-Mar-30 11:37", "fixed_version": "2011-Mar-30 11:37", "broken_version": "", "priority": "4", "subsystem": "", "assigned_to": "adesmet", "derived_from": "", "creator": "adesmet", "rust": "", "customer_group": "other", "visibility": "public", "notify": "tstclair@redhat.com tannenba@cs.wisc.edu", "due_date": ""}
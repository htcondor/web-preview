{"id": 572, "title": "Ticket #572: Be able to force DAG nodes to run on the same machine", "description": "<blockquote>\nUsers would often like to be able to force several consecutive DAG nodes to run on the same machine (data stage in/process/data stage out, for example).\n\n<p>Currently there's no good way to do this.\n\n</p><p>Todd T and I just talked about this -- we would need something on the Condor end to keep \"foreign\" jobs from coming in and grabbing the relevant machine between the DAG jobs.  Todd suggested the idea of being able to set a claim to not immediately time out after a job runs, and adding <em>claim requirements</em> and <em>claim timeouts</em>.  So then, when the first job runs, it could do something like setting the claim requirements to be that the next job's parent be the current job (since we stick parent node names into the job classads anyhow).  Then the last job in the sequence could unset the claim requirements and the claim would go away.\n\n</p><p>Maybe not for phase 1, we'd also propagate this from schedds to startds, so it can show up in condor_q -analyze, for instance.  We could also add something to the DAG language so that the user would just mark certain nodes as having to run on the same machine, and DAGMan would take care of the rest of the stuff automatically.</p></blockquote>", "remarks": "<blockquote>\n<em>2009-Jun-17 16:59:59 by matt:</em> <br/>\n\nThis sounds interesting, but seems strange. Does this aim to change the notion of the startd treating jobs as individual units to treating claims as such, e.g. the startd can/could/should cleanup after a job but now it's after a claim?\n\n<p></p><hr/>\n<em>2012-Feb-07 15:05:11 by nwp:</em> <br/>\n\nDAGman reuses claims now, from <span class=\"ticket\"><a class=\"resolved\" href=\"/wiki-archive/tickets/?ticket=2673\" onclick=\"get_ticket_and_populate_wrapper('2673'); return false;\" title='DAGman should use the \"hold claim\" functionality'>#2673</a></span>\n\n<p></p><hr/>\n<em>2016-May-18 16:42:18 by wenger:</em> <br/>\n\nHmm -- now that we have DAGMAN_HOLD_CLAIM_TIME, could we just do something like have DAGMan read the machine from the execute event of a parent job, and then add that to the requirements of a child job?  Maybe we could make a new command like SAME_MACHINE_PARENT ... CHILD ... that would cause DAGMan to do this?\n\n<p></p><hr/>\n<em>2016-May-18 16:48:06 by wenger:</em> <br/>\n\nAh, yeah -- here's a use case for this:  I just talked to a user who has jobs that start out by doing a large download from iPlant, and then a long computation phase.  They want to limit the number of jobs doing downloads at any one time to avoid overloading the iRODs, but then they want the computation part to not be throttled.  So the solution would be to break the download part and the computation part into separate jobs, and use category throttles on the download part.  The problem with that, of course, is that they need to have the two jobs run on the same machine; otherwise the download is wasted.</blockquote>", "derived_tickets": "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody><tr><td align=\"right\" valign=\"top\" width=\"160\">\n<span class=\"ticket\"><a class=\"resolved\" href=\"/wiki-archive/tickets/?ticket=4961\" onclick=\"get_ticket_and_populate_wrapper('4961'); return false;\" title=\"Document how to make DAG jobs run on the same machine\">#4961</a></span></td>\n<td align=\"center\" valign=\"center\" width=\"30\">\n<span class=\"icon ptr1\">\u00a0</span></td>\n<td align=\"left\" valign=\"top\">\nDocument how to make DAG jobs run on the same machine</td></tr>\n</tbody></table>", "attachments": "<html><head></head><body></body></html>", "check_ins": "", "type": "enhance", "last_change": "2016-May-18 16:48", "status": "new", "created": "2009-Jun-17 11:36", "fixed_version": "2009-Jun-17 11:36", "broken_version": "v070301", "priority": "3", "subsystem": "Dag", "assigned_to": "tannenba", "derived_from": "", "creator": "wenger", "rust": "", "customer_group": "other", "visibility": "public", "notify": "wenger@cs.wisc.edu, tannenba@cs.wisc.edu, tstclair@cs.wisc.edu", "due_date": ""}
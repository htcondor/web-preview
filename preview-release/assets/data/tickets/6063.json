{"id": 6063, "title": "Ticket #6063: Improve handling of job workflows with different memory requirements.", "description": "<blockquote>\nOver the next year, CMS will have an increasingly diverse set of memory requests due to two factors:\n<ul>\n<li>Increasing use of multicore.  Typically, memory usage in a multicore job is <code>M*(#cores) + B</code>.  For some workflows, <code>M=200</code>; for others, <code>M=500</code>.  Once we set <code>(# cores) = 8</code>, then we have a \"swing\" of 2.4GB even if <code>B</code> is similar for both workflows.\n</li><li>Concurrently running upgrade workflows (for future detector designs) that require significantly more memory than workflows that process the existing detector.\n</li></ul>\n\n<p>For many years, we've been able to assume 2GB RAM / core; in 2017, we'll\u00a0have workflows ranging from .6GB to 4GB / core.\n\n</p><p>Currently, we try to submit workflows with identical memory requests regardless of the actual workflow's need.  As an example, I attach a PNG that is a histogram of <em>actual</em> memory usage for jobs who request 5.9GB.\n\n</p><p>I'd like to get to the point where we could, in a scalable manner, actually set the the workflow's needed memory without doing 2GB of quantization.\n\n</p><p>Why?\n\n</p><p>Say we have:\n</p><ul>\n<li>One 8-core p-slot with 17GB RAM.\n</li><li>Assume all jobs are in the same schedd.  All jobs are from the same submitter but at different priorities.\n</li><li>An infinite number of \"backfill\" jobs that require 600MB RAM; 8 are currently running on our p-slot.\n</li><li>An infinite number of \"medium prio\" jobs that require 2GB RAM.\n</li><li>One \"high prio\" job that requires 3GB RAM.\n</li></ul>\n\n<p>Goal: Run the 'high prio' job as soon as possible.\n\n</p><p>Currently, in order to avoid creating work for the negotiator, we set the claim worklife to be relatively long (24hrs?).  To avoid massive priority inversion, we round up the <code>RequestMemory</code> of the backfill jobs to 2GB; that way, the schedd will at least switch to the medium prio job after a backfill job finishes.  We <em>do not</em> round everything to 3GB because then we would not utilize the available cores.  However, this means each d-slot is too small for the high prio job.\n\n</p><p>One <em>possible</em> mechanism to help the backfill job run is p-slot claiming (willing to consider other mechanisms in this ticket).  Then, it would be safe to submit the backfill jobs with <code>RequestMemory=600</code> because they won't starve medium prio jobs.  Further, the schedd has sufficient resources claimed to start the high prio job once a backfill job finishes.</p></blockquote>", "remarks": "<blockquote>\n</blockquote>", "derived_tickets": "", "attachments": "<html><head></head><body><blockquote>\n<ul>\n<li><a href=\"../files/975/CMS-Memory-Histogram.png\">CMS-Memory-Histogram.png</a>\n15998 bytes added by bbockelm on 2016-Dec-20 17:43:37 UTC.\n<br/>\nHistogram of <em>used</em> memory for all jobs submitted with <code>RequestMemory=5900</code>.  You can clearly see peaks around 2GB and 3GB, but almost no usage around the requested 5.9GB.<br/>\n</li><li><a href=\"../files/976/CMS-Memory-2300-Histogram.png\">CMS-Memory-2300-Histogram.png</a>\n23805 bytes added by bbockelm on 2016-Dec-20 17:53:16 UTC.\n<br/>\nHistogram of actual memory usage from jobs with <code>RequestMemory=2300</code> run in the last 30 days.  Not the peak is around 600MB; as current quantization rounds this to 2500MB, that means we are over-requesting by about 4x.<br/>\n</li></ul>\n</blockquote></body></html>", "check_ins": "", "type": "enhance", "last_change": "2016-Dec-20 11:40", "status": "new", "created": "2016-Dec-20 11:40", "fixed_version": "2016-Dec-20 11:40", "broken_version": "", "priority": "4", "subsystem": "DaemonsSubmitNode", "assigned_to": "tannenba", "derived_from": "", "creator": "bbockelm", "rust": "", "customer_group": "cms", "visibility": "public", "notify": "bbockelm@cse.unl.edu", "due_date": ""}